{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  CS6910: Fundamentals of Deep Learning\n",
    "#### Assignment 4- Team 3\n",
    "\n",
    "S Renganathan, CH16B058\t     \n",
    "S Nithya, CH16B113\t\t         \n",
    "Vasistha Singhal, CH16B119"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "colab_type": "code",
    "id": "1ttuirSPS8eP",
    "outputId": "fe487cea-f644-496d-b6f8-b0881eee66b8"
   },
   "outputs": [],
   "source": [
    "# # Run onluy the first time!\n",
    "# # from pyunpack import Archive\n",
    "# # Archive('Images.zip').extractall('Assignment4_Images')\n",
    "# !pip install torchtext==0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "lgMV2OMJS8eT",
    "outputId": "46e89a4a-f81b-44df-af06-1645ccc5c8da"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import pdb\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from io import StringIO\n",
    "from PIL import Image\n",
    "from torchtext.vocab import GloVe\n",
    "from torchtext.data import Field, get_tokenizer\n",
    "import torchvision.models as models\n",
    "from torchtext.data.metrics import bleu_score\n",
    "import torchtext\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kSKax7aBS8eX",
    "outputId": "56fa1f1c-aac4-460e-fb4a-7bdaa8806d39"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "beGr3RXYS8ec"
   },
   "outputs": [],
   "source": [
    "class DatasetClass(Dataset):\n",
    "    \n",
    "    def __init__(self, folder, image_list, captions):\n",
    "        \n",
    "        self.folder = folder\n",
    "        self.captions = captions\n",
    "        self.size = 5*len(image_list)\n",
    "        self.image_list = []\n",
    "        for image in image_list:\n",
    "            all_images = [image + '#' + str(i) for i in range(5)]\n",
    "            self.image_list.extend(all_images)\n",
    "        \n",
    "    def __getitem__(self, idx):     \n",
    "        \n",
    "        image_name = self.image_list[idx]\n",
    "        caption = self.captions.loc[image_name, 'Caption']\n",
    "        img = Image.open(self.folder + image_name[:-2])\n",
    "        trans = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor()])\n",
    "#         img = Image.open(self.folder + image_name[:-2]).resize((227, 227))\n",
    "#         trans = transforms.ToTensor()\n",
    "        return trans(img), caption\n",
    "      \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BZFjhoHoS8ef"
   },
   "outputs": [],
   "source": [
    "def train_test_loader(directory, image_list, captions, train_fraction=0.8, num_workers=0, batch_size=32):\n",
    "\n",
    "    dataset = DatasetClass(directory, image_list, captions)\n",
    "    \n",
    "    N = dataset.size\n",
    "    train_size = int(N*train_fraction)\n",
    "    test_size = N - train_size\n",
    "\n",
    "    train_data, test_data = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    trainloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    testloader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    \n",
    "    return trainloader, testloader, train_size, test_size\n",
    "\n",
    "def test_loader(directory, image_list, captions, num_workers=0, batch_size=32):\n",
    "\n",
    "    dataset = DatasetClass(directory, image_list, captions)\n",
    "    test_size = dataset.size\n",
    "    testloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    return testloader, test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J1SV7dwrS8ei"
   },
   "outputs": [],
   "source": [
    "# with open('/content/drive/My Drive/captions.txt') as f:\n",
    "#     captions = pd.read_csv(StringIO(f.read()), sep='\\t', header=None, names=['Image', 'Caption']).set_index('Image', drop=True)\n",
    "    \n",
    "# with open('/content/drive/My Drive/image_names.txt') as f:\n",
    "#     names = list(map(lambda x: x.rstrip(), f.readlines()))\n",
    "\n",
    "################### change link accordingly!\n",
    "\n",
    "with open('D:/_SEM8/DL/Assignment 4/captions.txt') as f:\n",
    "    captions = pd.read_csv(StringIO(f.read()), sep='\\t', header=None, names=['Image', 'Caption']).set_index('Image', drop=True)\n",
    "    \n",
    "with open('D:/_SEM8/DL/Assignment 4/image_names.txt') as f:\n",
    "    names = list(map(lambda x: x.rstrip(), f.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jf3Ual9sS8el"
   },
   "outputs": [],
   "source": [
    "# trainloader, testloader, train_size, test_size = train_test_loader(directory='/content/drive/My Drive/Assignment4_Data/Images/', image_list=names, captions=captions)\n",
    "\n",
    "testloader, test_size = test_loader(directory='D:/_SEM8/DL/Assignment 4/Assignment4_Images/Images/', image_list=names, captions=captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IbRgcoATS8eo"
   },
   "source": [
    "#### GloVe Representation\n",
    "\n",
    "Available GloVe Representations: \n",
    "\n",
    "1. glove.42B.300d \n",
    "2. glove.840B.300d \n",
    "3. glove.twitter.27B.25d \n",
    "4. glove.twitter.27B.50d \n",
    "5. glove.twitter.27B.100d \n",
    "6. glove.twitter.27B.200d \n",
    "7. glove.6B.50d \n",
    "8. glove.6B.100d \n",
    "9. glove.6B.200d \n",
    "10. glove.6B.300d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wcB9kNIWS8ep"
   },
   "outputs": [],
   "source": [
    "embedding_glove = GloVe(name='6B', dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HehTL29QS8et"
   },
   "outputs": [],
   "source": [
    "punct = [',','.','?','!',')','(',':',']','[','$','#','&','%','--']\n",
    "text_field = Field(tokenize='basic_english', lower=True, eos_token='eos', init_token='sos', stop_words=punct)\n",
    "preprocessed_text = captions['Caption'].apply(lambda x: text_field.preprocess(x))\n",
    "\n",
    "text_field.build_vocab(preprocessed_text, vectors=embedding_glove)\n",
    "embedding_trained = text_field.vocab.vectors[2:, :]\n",
    "vocab_tokens = np.array(text_field.vocab.itos)[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet = models.alexnet(pretrained=True)\n",
    "params_alex = list(alexnet.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 625, 2 / 625, 3 / 625, 4 / 625, 5 / 625, 6 / 625, 7 / 625, 8 / 625, 9 / 625, 10 / 625, 11 / 625, 12 / 625, 13 / 625, 14 / 625, 15 / 625, 16 / 625, 17 / 625, 18 / 625, 19 / 625, 20 / 625, 21 / 625, 22 / 625, 23 / 625, 24 / 625, 25 / 625, 26 / 625, 27 / 625, 28 / 625, 29 / 625, 30 / 625, 31 / 625, 32 / 625, 33 / 625, 34 / 625, 35 / 625, 36 / 625, 37 / 625, 38 / 625, 39 / 625, 40 / 625, 41 / 625, 42 / 625, 43 / 625, 44 / 625, 45 / 625, 46 / 625, 47 / 625, 48 / 625, 49 / 625, 50 / 625, 51 / 625, 52 / 625, 53 / 625, 54 / 625, 55 / 625, 56 / 625, 57 / 625, 58 / 625, 59 / 625, 60 / 625, 61 / 625, 62 / 625, 63 / 625, 64 / 625, 65 / 625, 66 / 625, 67 / 625, 68 / 625, 69 / 625, 70 / 625, 71 / 625, 72 / 625, 73 / 625, 74 / 625, 75 / 625, 76 / 625, 77 / 625, 78 / 625, 79 / 625, 80 / 625, 81 / 625, 82 / 625, 83 / 625, 84 / 625, 85 / 625, 86 / 625, 87 / 625, 88 / 625, 89 / 625, 90 / 625, 91 / 625, 92 / 625, 93 / 625, 94 / 625, 95 / 625, 96 / 625, 97 / 625, 98 / 625, 99 / 625, 100 / 625, 101 / 625, 102 / 625, 103 / 625, 104 / 625, 105 / 625, 106 / 625, 107 / 625, 108 / 625, 109 / 625, 110 / 625, 111 / 625, 112 / 625, 113 / 625, 114 / 625, 115 / 625, 116 / 625, 117 / 625, 118 / 625, 119 / 625, 120 / 625, 121 / 625, 122 / 625, 123 / 625, 124 / 625, 125 / 625, 126 / 625, 127 / 625, 128 / 625, 129 / 625, 130 / 625, 131 / 625, 132 / 625, 133 / 625, 134 / 625, 135 / 625, 136 / 625, 137 / 625, 138 / 625, 139 / 625, 140 / 625, 141 / 625, 142 / 625, 143 / 625, 144 / 625, 145 / 625, 146 / 625, 147 / 625, 148 / 625, 149 / 625, 150 / 625, 151 / 625, 152 / 625, 153 / 625, 154 / 625, 155 / 625, 156 / 625, 157 / 625, 158 / 625, 159 / 625, 160 / 625, 161 / 625, 162 / 625, 163 / 625, 164 / 625, 165 / 625, 166 / 625, 167 / 625, 168 / 625, 169 / 625, 170 / 625, 171 / 625, 172 / 625, 173 / 625, 174 / 625, 175 / 625, 176 / 625, 177 / 625, 178 / 625, 179 / 625, 180 / 625, 181 / 625, 182 / 625, 183 / 625, 184 / 625, 185 / 625, 186 / 625, 187 / 625, 188 / 625, 189 / 625, 190 / 625, 191 / 625, 192 / 625, 193 / 625, 194 / 625, 195 / 625, 196 / 625, 197 / 625, 198 / 625, 199 / 625, 200 / 625, 201 / 625, 202 / 625, 203 / 625, 204 / 625, 205 / 625, 206 / 625, 207 / 625, 208 / 625, 209 / 625, 210 / 625, 211 / 625, 212 / 625, 213 / 625, 214 / 625, 215 / 625, 216 / 625, 217 / 625, 218 / 625, 219 / 625, 220 / 625, 221 / 625, 222 / 625, 223 / 625, 224 / 625, 225 / 625, 226 / 625, 227 / 625, 228 / 625, 229 / 625, 230 / 625, 231 / 625, 232 / 625, 233 / 625, 234 / 625, 235 / 625, 236 / 625, 237 / 625, 238 / 625, 239 / 625, 240 / 625, 241 / 625, 242 / 625, 243 / 625, 244 / 625, 245 / 625, 246 / 625, 247 / 625, 248 / 625, 249 / 625, 250 / 625, 251 / 625, 252 / 625, 253 / 625, 254 / 625, 255 / 625, 256 / 625, 257 / 625, 258 / 625, 259 / 625, 260 / 625, 261 / 625, 262 / 625, 263 / 625, 264 / 625, 265 / 625, 266 / 625, 267 / 625, 268 / 625, 269 / 625, 270 / 625, 271 / 625, 272 / 625, 273 / 625, 274 / 625, 275 / 625, 276 / 625, 277 / 625, 278 / 625, 279 / 625, 280 / 625, 281 / 625, 282 / 625, 283 / 625, 284 / 625, 285 / 625, 286 / 625, 287 / 625, 288 / 625, 289 / 625, 290 / 625, 291 / 625, 292 / 625, 293 / 625, 294 / 625, 295 / 625, 296 / 625, 297 / 625, 298 / 625, 299 / 625, 300 / 625, 301 / 625, 302 / 625, 303 / 625, 304 / 625, 305 / 625, 306 / 625, 307 / 625, 308 / 625, 309 / 625, 310 / 625, 311 / 625, 312 / 625, 313 / 625, 314 / 625, 315 / 625, 316 / 625, 317 / 625, 318 / 625, 319 / 625, 320 / 625, 321 / 625, 322 / 625, 323 / 625, 324 / 625, 325 / 625, 326 / 625, 327 / 625, 328 / 625, 329 / 625, 330 / 625, 331 / 625, 332 / 625, 333 / 625, 334 / 625, 335 / 625, 336 / 625, 337 / 625, 338 / 625, 339 / 625, 340 / 625, 341 / 625, 342 / 625, 343 / 625, 344 / 625, 345 / 625, 346 / 625, 347 / 625, 348 / 625, 349 / 625, 350 / 625, 351 / 625, 352 / 625, 353 / 625, 354 / 625, 355 / 625, 356 / 625, 357 / 625, 358 / 625, 359 / 625, 360 / 625, 361 / 625, 362 / 625, 363 / 625, 364 / 625, 365 / 625, 366 / 625, 367 / 625, 368 / 625, 369 / 625, 370 / 625, 371 / 625, 372 / 625, 373 / 625, 374 / 625, 375 / 625, 376 / 625, 377 / 625, 378 / 625, 379 / 625, 380 / 625, 381 / 625, 382 / 625, 383 / 625, 384 / 625, 385 / 625, 386 / 625, 387 / 625, 388 / 625, 389 / 625, 390 / 625, 391 / 625, 392 / 625, 393 / 625, 394 / 625, 395 / 625, 396 / 625, 397 / 625, 398 / 625, 399 / 625, 400 / 625, 401 / 625, 402 / 625, 403 / 625, 404 / 625, 405 / 625, 406 / 625, 407 / 625, 408 / 625, 409 / 625, 410 / 625, 411 / 625, 412 / 625, 413 / 625, 414 / 625, 415 / 625, 416 / 625, 417 / 625, 418 / 625, 419 / 625, 420 / 625, 421 / 625, 422 / 625, 423 / 625, 424 / 625, 425 / 625, 426 / 625, 427 / 625, 428 / 625, 429 / 625, 430 / 625, 431 / 625, 432 / 625, 433 / 625, 434 / 625, 435 / 625, 436 / 625, 437 / 625, 438 / 625, 439 / 625, 440 / 625, 441 / 625, 442 / 625, 443 / 625, 444 / 625, 445 / 625, 446 / 625, 447 / 625, 448 / 625, 449 / 625, 450 / 625, 451 / 625, 452 / 625, 453 / 625, 454 / 625, 455 / 625, 456 / 625, 457 / 625, 458 / 625, 459 / 625, 460 / 625, 461 / 625, 462 / 625, 463 / 625, 464 / 625, 465 / 625, 466 / 625, 467 / 625, 468 / 625, 469 / 625, 470 / 625, 471 / 625, 472 / 625, 473 / 625, 474 / 625, 475 / 625, 476 / 625, 477 / 625, 478 / 625, 479 / 625, 480 / 625, 481 / 625, 482 / 625, 483 / 625, 484 / 625, 485 / 625, 486 / 625, 487 / 625, 488 / 625, 489 / 625, 490 / 625, 491 / 625, 492 / 625, 493 / 625, 494 / 625, 495 / 625, 496 / 625, 497 / 625, 498 / 625, 499 / 625, 500 / 625, 501 / 625, 502 / 625, 503 / 625, 504 / 625, 505 / 625, 506 / 625, 507 / 625, 508 / 625, 509 / 625, 510 / 625, 511 / 625, 512 / 625, 513 / 625, 514 / 625, 515 / 625, 516 / 625, 517 / 625, 518 / 625, 519 / 625, 520 / 625, 521 / 625, 522 / 625, 523 / 625, 524 / 625, 525 / 625, 526 / 625, 527 / 625, 528 / 625, 529 / 625, 530 / 625, 531 / 625, 532 / 625, 533 / 625, 534 / 625, 535 / 625, 536 / 625, 537 / 625, 538 / 625, 539 / 625, 540 / 625, 541 / 625, 542 / 625, 543 / 625, 544 / 625, 545 / 625, 546 / 625, 547 / 625, 548 / 625, 549 / 625, 550 / 625, 551 / 625, 552 / 625, 553 / 625, 554 / 625, 555 / 625, 556 / 625, 557 / 625, 558 / 625, 559 / 625, 560 / 625, 561 / 625, 562 / 625, 563 / 625, 564 / 625, 565 / 625, 566 / 625, 567 / 625, 568 / 625, 569 / 625, 570 / 625, 571 / 625, 572 / 625, 573 / 625, 574 / 625, 575 / 625, 576 / 625, 577 / 625, 578 / 625, 579 / 625, 580 / 625, 581 / 625, 582 / 625, 583 / 625, 584 / 625, 585 / 625, 586 / 625, 587 / 625, 588 / 625, 589 / 625, 590 / 625, 591 / 625, 592 / 625, 593 / 625, 594 / 625, 595 / 625, 596 / 625, 597 / 625, 598 / 625, 599 / 625, 600 / 625, 601 / 625, 602 / 625, 603 / 625, 604 / 625, 605 / 625, 606 / 625, 607 / 625, 608 / 625, 609 / 625, 610 / 625, 611 / 625, 612 / 625, 613 / 625, 614 / 625, 615 / 625, 616 / 625, 617 / 625, 618 / 625, 619 / 625, 620 / 625, 621 / 625, 622 / 625, 623 / 625, 624 / 625, 625 / 625, "
     ]
    }
   ],
   "source": [
    "RGB_mean = torch.zeros(3)\n",
    "i = 0\n",
    "for X, y in testloader:\n",
    "    i += 1\n",
    "    RGB_mean += (X.sum(0).sum(1).sum(1)/(X.shape[2]*X.shape[2]))/test_size\n",
    "    print(i, '/', len(testloader), end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "esNOLYw0S8e1"
   },
   "outputs": [],
   "source": [
    "class NetVLAD(nn.Module):\n",
    "    \n",
    "    def __init__(self, k, RGB_mean):\n",
    "        super(NetVLAD, self).__init__()\n",
    "        \n",
    "        self.RGB_mean = RGB_mean.to(device)\n",
    "        \n",
    "        # CNN\n",
    "        self.c1 = nn.Conv2d(3, 64, 11, stride=4, padding = (2,2))\n",
    "        self.mp1 = nn.MaxPool2d(3, stride=2)\n",
    "        self.c2 = nn.Conv2d(64, 192, 5, stride=1, padding=(2,2))\n",
    "        self.mp2 = nn.MaxPool2d(3, stride=2)\n",
    "        self.c3 = nn.Conv2d(192, 384, 3, stride=1, padding=(1,1))\n",
    "        self.c4 = nn.Conv2d(384, 256, 3, stride=1, padding=(1,1))\n",
    "        self.c5 = nn.Conv2d(256, 256, 3, stride=1, padding=(1,1))\n",
    "        self.mp3 = nn.MaxPool2d(3, stride=2)\n",
    "        \n",
    "        # NetVLAD\n",
    "        self.K = k\n",
    "        self.nv_conv = nn.Conv2d(256, k, 1)\n",
    "        self.nv_soft_ass = nn.Softmax2d()\n",
    "\n",
    "        # NetVLAD Parameter\n",
    "        self.c = nn.Parameter(torch.rand(self.K, 256))\n",
    "        \n",
    "        # Flatten to get h\n",
    "        self.flat = nn.Flatten(1, -1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # CNN\n",
    "        x = self.mp1(F.relu(self.c1(x)))\n",
    "        x = self.mp2(F.relu(self.c2(x)))\n",
    "        x = F.relu(self.c5(F.relu(self.c4(F.relu(self.c3(x))))))\n",
    "        x = self.mp3(x)\n",
    "\n",
    "        self.z_pre = x.flatten(2, 3)\n",
    "\n",
    "        # NetVLAD Step 1\n",
    "        a = self.nv_soft_ass(self.nv_conv(x))\n",
    "\n",
    "        # NetVLAD Step 2\n",
    "        for k in range(self.K):\n",
    "            a_k = a[:, k, :, :]\n",
    "            c_k = self.c[k, :]\n",
    "            temp = (x - c_k.reshape(1, -1, 1, 1))*a_k.unsqueeze(1)\n",
    "            z_k = torch.sum(temp, axis=(2, 3))\n",
    "            if k==0:\n",
    "                Z = z_k.unsqueeze(1)\n",
    "            else:\n",
    "                Z = torch.cat((Z, z_k.unsqueeze(1)), 1)\n",
    "        \n",
    "        # Flatten\n",
    "        Z = self.flat(Z)\n",
    "\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pCFgFv3JS8e5"
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, hidden_size, output_size, embedding_pre_trained):\n",
    "        \n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Pre-trained Word Embedding of all the words is used\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_pre_trained)\n",
    "        \n",
    "        # Input to the RNN is word embeddings of a word\n",
    "        self.rnn = nn.RNN(input_size=embed_size, hidden_size=hidden_size) \n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)  \n",
    "        \n",
    "    def forward(self, input_vec, hidden_vec):\n",
    "        '''\n",
    "        Parameters:\n",
    "        ------------\n",
    "        input_vec  - tensor of index of word/token. Example: torch.LongTensor([[0]]) for sos_token\n",
    "        hidden_vec - train_image or output from previous RNN cell\n",
    "        '''\n",
    "        embedded_input_vec = self.embedding(input_vec)\n",
    "        output_vec, hidden_vec = self.rnn(embedded_input_vec, hidden_vec)\n",
    "        output_vec = self.softmax(self.out(output_vec[0]))\n",
    "        return output_vec, hidden_vec      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WylTwrZ7S8e7"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, hidden_size, output_size, embedding_pre_trained):\n",
    "        \n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Pre-trained Word Embedding of all the words is used\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_pre_trained)\n",
    "        \n",
    "        # Input to the LSTM is word embeddings of a word\n",
    "        self.lstm = nn.LSTM(input_size=embed_size, hidden_size=hidden_size) \n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)  \n",
    "        \n",
    "    def forward(self, input_vec, hidden_vec, cell_state):\n",
    "        '''\n",
    "        Parameters:\n",
    "        ------------\n",
    "        input_vec  - tensor of index of word/token. Example: torch.LongTensor([[0]]) for sos_token\n",
    "        hidden_vec - train_image or output from previous RNN cell\n",
    "        '''\n",
    "        embedded_input_vec = self.embedding(input_vec)\n",
    "        output_vec, (hidden_vec, cell_state) = self.lstm(embedded_input_vec, (hidden_vec, cell_state))\n",
    "        output_vec = self.softmax(self.out(output_vec[0]))\n",
    "        return output_vec, (hidden_vec, cell_state)     \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_image(train_image, image_caption, encoder_obj, decoder_obj, encoder_optim, decoder_optim, loss_func, vocab_tokens, is_LSTM=False, print_pred = False):\n",
    "    '''\n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_image     - Images stored in batches using DataLoader\n",
    "    image_caption   - Caption of the image (in the same word embedding representation used in )\n",
    "    '''\n",
    "    # Start of sentence and End of Sentence token\n",
    "    try:\n",
    "        eos_token = np.argwhere(vocab_tokens=='eos').item()\n",
    "        sos_token = np.argwhere(vocab_tokens=='sos').item()\n",
    "    except:\n",
    "        pdb.set_trace()\n",
    "    \n",
    "    # Length of the image caption\n",
    "    caption_length = len(image_caption)\n",
    "\n",
    "    encoder_optim.zero_grad()\n",
    "    decoder_optim.zero_grad()\n",
    "\n",
    "    \n",
    "    encoder_output = encoder_obj(train_image).view(1, 1, -1)\n",
    "\n",
    "    decoder_input = torch.tensor([[sos_token]], device=device)  ## This is converted into an embedding withing the decoder class\n",
    "    decoder_hidden = encoder_output\n",
    "    decoder_hidden_size = decoder_hidden.shape[-1]\n",
    "    if is_LSTM:\n",
    "        decoder_cell_state = torch.zeros(decoder_hidden_size).view(1, 1, -1).to(device) \n",
    "\n",
    "    loss = 0\n",
    "    output_caption = []\n",
    "    for i in range(caption_length):\n",
    "        if is_LSTM:\n",
    "            decoder_output, (decoder_hidden, decoder_cell_state) = decoder_obj(decoder_input, decoder_hidden, decoder_cell_state)\n",
    "        else:    \n",
    "            decoder_output, decoder_hidden = decoder_obj(decoder_input, decoder_hidden) \n",
    "        max_val, max_ind = decoder_output.topk(1)  # Choosing the word with maximum probability \n",
    "        decoder_input = max_ind  #.squeeze().detach()  \n",
    "        output_caption.append(vocab_tokens[decoder_input.item()])\n",
    "        loss += loss_func(decoder_output, torch.tensor([np.argwhere(vocab_tokens == image_caption[i]).item()]).to(device))\n",
    "        if decoder_input.item() == eos_token:\n",
    "            break\n",
    "            \n",
    "    if print_pred:\n",
    "        print('\\n')\n",
    "        print('Target: ', image_caption)\n",
    "        print('Predicted :', output_caption)\n",
    "    \n",
    "    loss.backward()\n",
    "    encoder_optim.step()\n",
    "    decoder_optim.step()\n",
    "\n",
    "    return loss.item()/caption_length\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_checkpoint(ckp_state, ckp_path):\n",
    "    torch.save(ckp_state, ckp_path)\n",
    "    print('\\nCreated Checkpoint!')\n",
    "    \n",
    "def load_checkpoint(ckp_path, model, optimizer = None, load_optimizer_state = False):\n",
    "    ckp = torch.load(ckp_path)\n",
    "    model.load_state_dict(ckp['state_dict'])\n",
    "    if load_optimizer_state:\n",
    "        optimizer.load_state_dict(ckp['optimizer'])\n",
    "        return model, optimizer, ckp['epoch'], ckp['loss']\n",
    "    else:\n",
    "        return model, ckp['epoch'], ckp['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nJjH_4T6S8fA"
   },
   "outputs": [],
   "source": [
    "def train(lr, trainloader_images, train_images_size, encoder_obj, decoder_obj, vocab_tokens, params, is_LSTM=False):\n",
    "    \n",
    "    loss_func = nn.NLLLoss()\n",
    "\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    punct = [',','.','?','!',')','(',':',']','[','$','#','&','%','--']\n",
    "    \n",
    "    encoder_optim = optim.SGD(encoder_obj.parameters(), lr=lr)\n",
    "    decoder_optim = optim.SGD(decoder_obj.parameters(), lr=lr)\n",
    "    \n",
    "    old_loss = np.inf\n",
    "    epoch = 0\n",
    "    losses = []\n",
    "    print_pred = True\n",
    "    while True:\n",
    "        new_loss = 0\n",
    "        epoch += 1\n",
    "        batch = 0 \n",
    "        for data in trainloader_images:\n",
    "            batch += 1\n",
    "            images, captions = data[0].to(device), data[1]\n",
    "            bs = images.shape[0]\n",
    "\n",
    "            for i in range(bs):\n",
    "                train_image = images[i].view(1, images.shape[1], images.shape[2], images.shape[3])\n",
    "                image_caption = tokenizer(captions[i])\n",
    "                image_caption = list(filter(lambda x: x not in punct, image_caption))\n",
    "                image_caption.append('eos')\n",
    "                new_loss += train_one_image(train_image, image_caption, encoder_obj, decoder_obj, encoder_optim, decoder_optim, loss_func, vocab_tokens, is_LSTM, print_pred)\n",
    "                        \n",
    "        new_loss = new_loss/train_images_size\n",
    "        \n",
    "        print('\\n')\n",
    "        print('-----------------------------------------------------------------------------------------------')\n",
    "        print('Epoch {0}: Loss = {1}, Rel loss = {2}'.format(epoch, new_loss, (old_loss-new_loss)/new_loss))\n",
    "        \n",
    "        losses.append(new_loss)\n",
    "        \n",
    "        if epoch%5 == 0:\n",
    "            print_pred = True\n",
    "            pickle.dump(losses, open('checkpoints/losses.sav', 'wb'))\n",
    "            checkpoint_encoder = {'epoch': epoch, 'loss': new_loss, 'state_dict': encoder_obj.state_dict(), 'optimizer': encoder_optim.state_dict()}\n",
    "            checkpoint_decoder = {'epoch': epoch, 'loss': new_loss, 'state_dict': decoder_obj.state_dict(), 'optimizer': decoder_optim.state_dict()}\n",
    "            create_checkpoint(ckp_state=checkpoint_encoder, ckp_path='checkpoints/encoder{0}.pt'.format(epoch))\n",
    "            create_checkpoint(ckp_state=checkpoint_decoder, ckp_path='checkpoints/decoder{0}.pt'.format(epoch))          \n",
    "        else:\n",
    "            print_pred = False\n",
    "\n",
    "        if abs(new_loss-old_loss)/new_loss < 1e-5:\n",
    "            print('Converged')\n",
    "            return losses\n",
    "\n",
    "        old_loss = new_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_one_image(encoder_obj, decoder_obj, image, image_caption, vocab_tokens, is_LSTM=False):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Start of sentence and End of Sentence token\n",
    "        sos_token = np.argwhere(vocab_tokens=='sos').item()\n",
    "        eos_token = np.argwhere(vocab_tokens=='eos').item()\n",
    "\n",
    "        # Length of the image caption\n",
    "        caption_length = len(image_caption)\n",
    "\n",
    "        encoder_output = encoder_obj(image).view(1, 1, -1)  \n",
    "        \n",
    "        decoder_input = torch.tensor([[sos_token]], device=device)\n",
    "        decoder_hidden = encoder_output\n",
    "        decoder_hidden_size = decoder_hidden.shape[-1]\n",
    "        if is_LSTM:\n",
    "            decoder_cell_state = torch.zeros(decoder_hidden_size).view(1, 1, -1).to(device)\n",
    "            \n",
    "        output_caption = []\n",
    "            \n",
    "        for i in range(caption_length):\n",
    "            if is_LSTM:\n",
    "                decoder_output, (decoder_hidden, decoder_cell_state) = decoder_obj(decoder_input, decoder_hidden, decoder_cell_state)\n",
    "            else:    \n",
    "                decoder_output, decoder_hidden = decoder_obj(decoder_input, decoder_hidden)             \n",
    "            max_val, max_ind = decoder_output.topk(1)  # Choosing the word with maximum probability \n",
    "            decoder_input = max_ind \n",
    "            if decoder_input.item() == eos_token:\n",
    "                output_caption.append('eos')\n",
    "                break\n",
    "            else:\n",
    "                output_caption.append(vocab_tokens[decoder_input.item()])\n",
    "                \n",
    "        print('\\n')\n",
    "        print('Image caption: ', image_caption)\n",
    "        print('Predicted caption: ', output_caption)\n",
    "\n",
    "        return output_caption, [image_caption]\n",
    "    \n",
    "def evaluate(trainloader_images, train_images_size, encoder_obj, decoder_obj, is_LSTM = False):\n",
    "\n",
    "    \n",
    "    punct = [',','.','?','!',')','(',':',']','[','$','#','&','%','--']\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    \n",
    "    candidate_corpus = []\n",
    "    references_corpus = []\n",
    "    for data in trainloader_images:\n",
    "        images, captions = data[0].to(device), data[1]\n",
    "        bs = images.shape[0]\n",
    "        \n",
    "        for i in range(bs):\n",
    "            train_image = images[i].view(1, images.shape[1], images.shape[2], images.shape[3])\n",
    "            image_caption = tokenizer(captions[i])\n",
    "            image_caption = list(filter(lambda x: x not in punct, image_caption))\n",
    "            image_caption.append('eos')\n",
    "            oc, ic = evaluate_one_image(encoder_obj, decoder_obj, train_image, image_caption, vocab_tokens, is_LSTM)\n",
    "            candidate_corpus.append(oc)\n",
    "            references_corpus.append(ic)\n",
    "    \n",
    "    bleu1 = bleu_score(candidate_corpus, references_corpus, weights=(1.0, 0, 0, 0))\n",
    "    bleu2 = bleu_score(candidate_corpus, references_corpus, weights=(0.5, 0.5, 0, 0))\n",
    "    bleu3 = bleu_score(candidate_corpus, references_corpus, weights=(1/3, 1/3, 1/3, 0))\n",
    "    bleu4 = bleu_score(candidate_corpus, references_corpus)\n",
    "    \n",
    "    bleu_scores = [bleu1, bleu2, bleu3, bleu4]\n",
    "\n",
    "    return candidate_corpus, references_corpus, bleu_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCi7OWIyS8fI"
   },
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mH0ymSgALlZ_"
   },
   "outputs": [],
   "source": [
    "k = 3\n",
    "encoder_obj = NetVLAD(k, RGB_mean).to(device)\n",
    "# Initialization with pre-trained parameters\n",
    "with torch.no_grad():\n",
    "    encoder_obj.c1.weight = params_alex[0]\n",
    "    encoder_obj.c1.bias = params_alex[1]\n",
    "\n",
    "    encoder_obj.c2.weight = params_alex[2]\n",
    "    encoder_obj.c2.bias = params_alex[3]\n",
    "\n",
    "    encoder_obj.c3.weight = params_alex[4]\n",
    "    encoder_obj.c3.bias = params_alex[5]\n",
    "\n",
    "    encoder_obj.c4.weight = params_alex[6]\n",
    "    encoder_obj.c4.bias = params_alex[7]\n",
    "\n",
    "    encoder_obj.c5.weight = params_alex[8]\n",
    "    encoder_obj.c5.bias = params_alex[9]\n",
    "\n",
    "encoder_obj = encoder_obj.to(device)\n",
    "\n",
    "\n",
    "embed_size = embedding_trained.shape[1]\n",
    "decoder_output_size = embedding_trained.shape[0]\n",
    "decoder_hidden_size = k*256\n",
    "decoder_obj = RNN(embed_size, decoder_hidden_size, decoder_output_size, embedding_trained).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_trained1, e_epoch1, e_loss1 = load_checkpoint(\"q1_encoder_wts.pt\", encoder_obj)\n",
    "decoder_trained1, d_epoch1, d_loss1 = load_checkpoint(\"q1_decoder_wts.pt\", decoder_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_corpus1, references_corpus1, bleu_scores1 = evaluate(testloader, test_size, encoder_trained1, decoder_trained1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.40879117523956954,\n",
       " 0.23757306996097471,\n",
       " 0.14627992614805915,\n",
       " 0.09392114428659279]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YnrTPAEaS8fJ"
   },
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "encoder_obj = NetVLAD(k, RGB_mean).to(device)\n",
    "# Initialization with pre-trained parameters\n",
    "with torch.no_grad():\n",
    "    encoder_obj.c1.weight = params_alex[0]\n",
    "    encoder_obj.c1.bias = params_alex[1]\n",
    "\n",
    "    encoder_obj.c2.weight = params_alex[2]\n",
    "    encoder_obj.c2.bias = params_alex[3]\n",
    "\n",
    "    encoder_obj.c3.weight = params_alex[4]\n",
    "    encoder_obj.c3.bias = params_alex[5]\n",
    "\n",
    "    encoder_obj.c4.weight = params_alex[6]\n",
    "    encoder_obj.c4.bias = params_alex[7]\n",
    "\n",
    "    encoder_obj.c5.weight = params_alex[8]\n",
    "    encoder_obj.c5.bias = params_alex[9]\n",
    "\n",
    "encoder_obj = encoder_obj.to(device)\n",
    "\n",
    "embed_size = embedding_trained.shape[1]\n",
    "decoder_output_size = embedding_trained.shape[0]\n",
    "decoder_hidden_size = k*256\n",
    "decoder_obj = LSTM(embed_size, decoder_hidden_size, decoder_output_size, embedding_trained).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_trained2, e_epoch2, e_loss2 = load_checkpoint(\"q2_encoder_wts.pt\", encoder_obj)\n",
    "decoder_trained2, d_epoch2, d_loss2 = load_checkpoint(\"q2_decoder_wts.pt\", decoder_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_corpus2, references_corpus2, bleu_scores2 = evaluate(testloader, test_size, encoder_trained2, decoder_trained2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_scores2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Q1_Q2 (Ass4).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
