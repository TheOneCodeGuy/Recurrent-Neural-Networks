{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from io import StringIO\n",
    "from PIL import Image\n",
    "from torchtext.vocab import GloVe\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from torchtext.data.metrics import bleu_score\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DatasetClass(Dataset):\n",
    "    \n",
    "#     def __init__(self, source, target):\n",
    "        \n",
    "#         self.source = source\n",
    "#         self.target = target\n",
    "#         self.size = len(source)\n",
    "      \n",
    "#     def __len__(self):\n",
    "        \n",
    "#         return self.size\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "        \n",
    "#         return self.source[idx], self.target[idx]\n",
    "    \n",
    "# def train_test_loader(source_train, target_train, source_test, target_test, num_workers=0, batch_size=32):\n",
    "\n",
    "#     train_data = DatasetClass(source_train, target_train)\n",
    "#     test_data = DatasetClass(source_test, target_test)\n",
    "\n",
    "#     trainloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "#     testloader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    \n",
    "#     return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, SRC):\n",
    "        \n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(SRC.vocab.vectors)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        \n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "#         print(output.shape)\n",
    "#         print(hidden.shape)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        \n",
    "        return (torch.zeros(1, 1, self.hidden_size, device=device), torch.zeros(1, 1, self.hidden_size, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        \n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        \n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        \n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "    \n",
    "    def get_embedding(self, x):\n",
    "        return self.embedding(x).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=60):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(-1)\n",
    "    target_length = target_tensor.size(-1)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        \n",
    "        # this line calls forward() in the EncoderRNN\n",
    "        #print(encoder_hidden.shape)\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[2]], device=device)  #SOS token\n",
    "    \n",
    "    #the first hidden state of the decoder is the final hidden state of the encoder\n",
    "    decoder_hidden = encoder_hidden   \n",
    "\n",
    "    # Without teacher forcing: use its own predictions as the next input\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "        #print(decoder_output.shape) # tensor of size [1,18668] with logprobs\n",
    "        \n",
    "        # trg will be the index of the target word put in a long tensor of size 1\n",
    "        trg = torch.cuda.LongTensor([target_tensor[di].item()])\n",
    "        \n",
    "        loss += criterion(decoder_output, trg)\n",
    "        # 0 is the EOS token in the target language\n",
    "        if decoder_input.item() == 0:\n",
    "            break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, sources, targets, device, learning_rate=0.01):\n",
    "    \n",
    "    # sources and targets are lists of lists with each list containing the integer encodings of a sentence\n",
    "    \n",
    "    print_loss_total = 0  \n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for i in range(len(sources)):\n",
    "        input_tensor = torch.LongTensor(sources[i]).to(device)\n",
    "        target_tensor = torch.LongTensor(targets[i]).to(device)\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        \n",
    "        if i%2621==0:\n",
    "            print((i/2621)*10,'% done')\n",
    "            print(\"Current loss:\", print_loss_total)\n",
    "        \n",
    "    print(\"Loss = \",print_loss_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading files\n",
    "\n",
    "with open('trainen.txt', encoding='utf8') as f:\n",
    "    eng_train = list(map(lambda x: x.rstrip(), f.readlines()))\n",
    "    \n",
    "with open('trainta.txt', encoding='utf8') as f:\n",
    "    tamil_train = list(map(lambda x: x.rstrip(), f.readlines()))\n",
    "    \n",
    "with open('deven.txt', encoding='utf8') as f:\n",
    "    eng_test = list(map(lambda x: x.rstrip(), f.readlines()))\n",
    "    \n",
    "with open('devta.txt', encoding='utf8') as f:\n",
    "    tamil_test = list(map(lambda x: x.rstrip(), f.readlines()))\n",
    "    \n",
    "embedding_glove = GloVe(name='6B', dim=100)\n",
    "\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "SRC = Field(tokenize = tokenize_en, init_token='<sos>', eos_token = '<eos>', lower=True)\n",
    "processed_eng_train = list(map(lambda x: SRC.preprocess(x), eng_train))\n",
    "processed_eng_test = list(map(lambda x: SRC.preprocess(x), eng_test))\n",
    "\n",
    "SRC.build_vocab(processed_eng_train, vectors=embedding_glove)\n",
    "embedding_trained = nn.Embedding.from_pretrained(SRC.vocab.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(processed_eng):\n",
    "    \n",
    "    #function to return the numericalized version of the tokenized sentences\n",
    "    X = []\n",
    "    for tokenized_sentence in processed_eng:\n",
    "        int_sequence = [2]  #first element is the SOS token \n",
    "        for token in tokenized_sentence:\n",
    "            int_sequence.append(SRC.vocab.stoi[token])\n",
    "        int_sequence.append(3) #last element is the EOS token\n",
    "        X.append(int_sequence)\n",
    "    \n",
    "    return X\n",
    "    \n",
    "X_train = preprocess(processed_eng_train)\n",
    "X_test = preprocess(processed_eng_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Same thing for Tamil sentences\n",
    "tokenizer2 = Tokenizer()\n",
    "tokenizer2.fit_on_texts(tamil_train)\n",
    "y_tr_seq = tokenizer2.texts_to_sequences(tamil_train)\n",
    "y_val_seq = tokenizer2.texts_to_sequences(tamil_test)\n",
    "\n",
    "#adding EOS token\n",
    "_ = [y.append(0) for y in y_tr_seq]\n",
    "_ = [y.append(0) for y in y_val_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9737\n",
      "18669\n"
     ]
    }
   ],
   "source": [
    "source_vocab_size = len(SRC.vocab)\n",
    "target_vocab_size = len(tokenizer2.word_index)+1\n",
    "print(source_vocab_size)\n",
    "print(target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % done\n",
      "Current loss: 9.837830352783204\n",
      "10.0 % done\n",
      "Current loss: 10637.475831454185\n",
      "20.0 % done\n",
      "Current loss: 22052.417893069407\n",
      "30.0 % done\n",
      "Current loss: 33588.00305823722\n",
      "40.0 % done\n",
      "Current loss: 44821.36450597464\n",
      "50.0 % done\n",
      "Current loss: 56506.69794175826\n",
      "60.0 % done\n",
      "Current loss: 68169.20414662533\n",
      "70.0 % done\n",
      "Current loss: 79854.48099445802\n",
      "80.0 % done\n",
      "Current loss: 91522.4564530828\n",
      "90.0 % done\n",
      "Current loss: 103236.43000924337\n",
      "100.0 % done\n",
      "Current loss: 114782.68873067149\n",
      "Loss =  114809.6355010852\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 100\n",
    "encoder1 = EncoderRNN(source_vocab_size, hidden_size, SRC).to(device)\n",
    "decoder1 = DecoderRNN(hidden_size, target_vocab_size).to(device)\n",
    "trainIters(encoder1, decoder1, X_train, y_tr_seq, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_score([tem], [tem2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tem = [tokenizer2.index_word[x] for x in y_val_seq[0][:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tem2 = [tokenizer2.index_word[x] for x in y_val_seq[1][:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_score([['How', 'are', 'you']], [['How', 'are', 'you']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
