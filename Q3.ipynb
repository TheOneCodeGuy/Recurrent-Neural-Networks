{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torchtext.vocab import GloVe\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from torchtext.data.metrics import bleu_score\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading files\n",
    "\n",
    "with open('trainen.txt', encoding='utf8') as f:\n",
    "    eng_train = list(map(lambda x: x.rstrip(), f.readlines()))\n",
    "    \n",
    "with open('trainta.txt', encoding='utf8') as f:\n",
    "    tamil_train = list(map(lambda x: x.rstrip(), f.readlines()))\n",
    "    \n",
    "with open('deven.txt', encoding='utf8') as f:\n",
    "    eng_test = list(map(lambda x: x.rstrip(), f.readlines()))\n",
    "    \n",
    "with open('devta.txt', encoding='utf8') as f:\n",
    "    tamil_test = list(map(lambda x: x.rstrip(), f.readlines()))\n",
    "    \n",
    "embedding_glove = GloVe(name='6B', dim=100)\n",
    "\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "ENG = Field(tokenize = tokenize_en, init_token='sos', eos_token = 'eos', lower=True)\n",
    "processed_eng_train = list(map(lambda x: ENG.preprocess(x), eng_train))\n",
    "processed_eng_test = list(map(lambda x: ENG.preprocess(x), eng_test))\n",
    "\n",
    "ENG.build_vocab(processed_eng_train, vectors=embedding_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(processed_eng):\n",
    "    \n",
    "    #function to return the numericalized version of the tokenized sentences\n",
    "    X = []\n",
    "    for tokenized_sentence in processed_eng:\n",
    "        int_sequence = [2]  #first element is the SOS token \n",
    "        for token in tokenized_sentence:\n",
    "            int_sequence.append(ENG.vocab.stoi[token])\n",
    "        int_sequence.append(3) #last element is the EOS token\n",
    "        X.append(int_sequence)\n",
    "    \n",
    "    return X\n",
    "\n",
    "# X_train and X_test are lists of lists with the integer sequences for a given sentence\n",
    "X_train = preprocess(processed_eng_train)\n",
    "X_test = preprocess(processed_eng_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Same thing for Tamil sentences\n",
    "TAM = Tokenizer()\n",
    "TAM.fit_on_texts(tamil_train)\n",
    "Y_train = TAM.texts_to_sequences(tamil_train)\n",
    "Y_test = TAM.texts_to_sequences(tamil_test)\n",
    "\n",
    "#adding EOS token\n",
    "_ = [y.append(0) for y in Y_train]\n",
    "_ = [y.append(0) for y in Y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9736\n",
      "18669\n"
     ]
    }
   ],
   "source": [
    "source_vocab_size = len(ENG.vocab)\n",
    "target_vocab_size = len(TAM.word_index)+1\n",
    "print(source_vocab_size)\n",
    "print(target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_size, hidden_size, ENG):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed = nn.Embedding.from_pretrained(ENG.vocab.vectors)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size)\n",
    "        \n",
    "    def forward(self, x, enc_hidden):\n",
    "        \n",
    "        enc_output, enc_hidden = self.lstm(self.embed(x).view(1,1,-1), enc_hidden)\n",
    "        return enc_output, enc_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_size, hidden_size, n_classes):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed = nn.Embedding(n_classes, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, n_classes)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x, dec_hidden):\n",
    "        \n",
    "        dec_output, dec_hidden = self.lstm(self.embed(x).view(1,1,-1), dec_hidden)\n",
    "        dec_output = self.logsoftmax(self.fc(dec_output[0]))\n",
    "        return dec_output, dec_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_per_pair(source_sentence, target_sentence, enc_obj, dec_obj, enc_optimiser, dec_optimiser, loss_fn):\n",
    "    \n",
    "    Ts = source_sentence.size(-1)\n",
    "    Tt = target_sentence.size(-1)\n",
    "    enc_hidden = (torch.zeros(1, 1, enc_obj.hidden_size, device=device), torch.zeros(1, 1, enc_obj.hidden_size, device=device))\n",
    "    \n",
    "    enc_optimiser.zero_grad()\n",
    "    dec_optimiser.zero_grad()\n",
    "    \n",
    "    loss_val = 0\n",
    "    \n",
    "    for i in range(Ts):\n",
    "        enc_output, enc_hidden = enc_obj(source_sentence[i], enc_hidden)\n",
    "    \n",
    "    dec_input = torch.tensor([[2]], device=device)  #SOS token\n",
    "    \n",
    "    # first hidden state of decoder is made the final hidden state of the encoder and cell state is initialised with zeros\n",
    "    dec_hidden = (enc_hidden[0], torch.zeros(1, 1, dec_obj.hidden_size, device=device))\n",
    "    \n",
    "    for i in range(Tt):\n",
    "        \n",
    "        dec_output, dec_hidden = dec_obj(dec_input, dec_hidden)\n",
    "        _ , index = dec_output.topk(1)\n",
    "        dec_input = index.squeeze().detach()\n",
    "            \n",
    "        target_word = torch.cuda.LongTensor([target_sentence[i].item()])\n",
    "        \n",
    "        loss_val += loss_fn(dec_output, target_word)\n",
    "        \n",
    "        if dec_input.item() == 0:\n",
    "            break\n",
    "\n",
    "    loss_val.backward()\n",
    "    \n",
    "    nn.utils.clip_grad_norm_(enc_obj.parameters(), 0.5, 1)\n",
    "    nn.utils.clip_grad_norm_(dec_obj.parameters(), 0.5, 1)\n",
    "\n",
    "    enc_optimiser.step()\n",
    "    dec_optimiser.step()\n",
    "\n",
    "    return loss_val.item()/Tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(sources, targets, enc_obj, dec_obj):\n",
    "    \n",
    "    loss_fn = nn.NLLLoss()\n",
    "    enc_optimiser = optim.SGD(enc_obj.parameters(), lr=0.001, momentum=0.9)\n",
    "    dec_optimiser = optim.SGD(dec_obj.parameters(), lr=0.001, momentum=0.9)\n",
    "#     enc_optimiser = optim.Adagrad(enc_obj.parameters())\n",
    "#     dec_optimiser = optim.Adagrad(dec_obj.parameters())\n",
    "    \n",
    "    max_epochs = 100\n",
    "    old_loss = np.inf\n",
    "    indices = [i for i in range(len(sources))]\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        \n",
    "        # shuffling the input data manually at the start of every new epoch\n",
    "        np.random.shuffle(indices)\n",
    "        sources = list(np.array(sources)[indices])\n",
    "        targets = list(np.array(targets)[indices])\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        num_sentences = len(sources)\n",
    "        \n",
    "        for i in range(num_sentences):\n",
    "            source_sentence = torch.cuda.LongTensor(sources[i])\n",
    "            target_sentence = torch.cuda.LongTensor(targets[i])\n",
    "\n",
    "            loss = loss_per_pair(source_sentence, target_sentence, enc_obj, dec_obj, enc_optimiser, dec_optimiser, loss_fn)\n",
    "            running_loss += loss\n",
    "\n",
    "            if i%int(num_sentences*0.1)==0:\n",
    "                print(\"Epoch\", epoch+1, \":\", (i/int(num_sentences*0.1))*10,'% done')\n",
    "                print(\"Current loss:\", running_loss)\n",
    "                \n",
    "        print('\\nEpoch', epoch+1,\"\\n\")\n",
    "        print(\"Encoder obj lstm wts sum=\", torch.sum(enc_obj.lstm.weight_ih_l0))\n",
    "        print(\"Encoder obj lstm bias sum=\", torch.sum(enc_obj.lstm.bias_ih_l0))\n",
    "        print('Decoder obj emb wts sum=', torch.sum(dec_obj.embed.weight.data))\n",
    "        print('Decoder obj lstm wts sum=', torch.sum(dec_obj.lstm.weight_ih_l0.data))\n",
    "        print('Decoder obj lstm bias sum=', torch.sum(dec_obj.lstm.bias_ih_l0.data))\n",
    "        print('Decoder obj linear wts sum=', torch.sum(dec_obj.fc.weight.data))\n",
    "        print('Decoder obj linear bias sum=', torch.sum(dec_obj.fc.bias.data))\n",
    "        \n",
    "        \n",
    "        if abs(running_loss-old_loss)/running_loss < 1e-3:\n",
    "            print('Converged')\n",
    "            break\n",
    "    \n",
    "        old_loss = running_loss\n",
    "\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : 0.0 % done\n",
      "Current loss: 9.82754135131836\n",
      "Epoch 1 : 10.0 % done\n",
      "Current loss: 25755.683479615316\n",
      "Epoch 1 : 20.0 % done\n",
      "Current loss: 51502.66197272957\n",
      "Epoch 1 : 30.0 % done\n",
      "Current loss: 77215.21324324333\n",
      "Epoch 1 : 40.0 % done\n",
      "Current loss: 102899.94318143725\n",
      "Epoch 1 : 50.0 % done\n",
      "Current loss: 121611.55016383945\n",
      "Epoch 1 : 60.0 % done\n",
      "Current loss: 133938.8418479452\n",
      "Epoch 1 : 70.0 % done\n",
      "Current loss: 146497.32315648973\n",
      "Epoch 1 : 80.0 % done\n",
      "Current loss: 158783.32897929766\n",
      "Epoch 1 : 90.0 % done\n",
      "Current loss: 171805.28109355972\n",
      "Epoch 1 : 100.0 % done\n",
      "Current loss: 196143.57038203054\n",
      "\n",
      "Epoch 1 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(5.1913, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(0.8888, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8718, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-15.5397, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(-0.5211, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(48.2973, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(-7.1632, device='cuda:0')\n",
      "Epoch 2 : 0.0 % done\n",
      "Current loss: 9.77293872833252\n",
      "Epoch 2 : 10.0 % done\n",
      "Current loss: 25328.695364698582\n",
      "Epoch 2 : 20.0 % done\n",
      "Current loss: 38003.88452731085\n",
      "Epoch 2 : 30.0 % done\n",
      "Current loss: 50448.2269812234\n",
      "Epoch 2 : 40.0 % done\n",
      "Current loss: 62932.8023672409\n",
      "Epoch 2 : 50.0 % done\n",
      "Current loss: 75198.50337810998\n",
      "Epoch 2 : 60.0 % done\n",
      "Current loss: 87628.28012793465\n",
      "Epoch 2 : 70.0 % done\n",
      "Current loss: 100120.61551916825\n",
      "Epoch 2 : 80.0 % done\n",
      "Current loss: 112602.7321664275\n",
      "Epoch 2 : 90.0 % done\n",
      "Current loss: 124959.85396901566\n",
      "Epoch 2 : 100.0 % done\n",
      "Current loss: 137478.5182681971\n",
      "\n",
      "Epoch 2 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(5.2801, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(0.9290, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8635, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-15.4330, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(-0.4729, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(48.8144, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(-6.7683, device='cuda:0')\n",
      "Epoch 3 : 0.0 % done\n",
      "Current loss: 3.1919943491617837\n",
      "Epoch 3 : 10.0 % done\n",
      "Current loss: 12452.694303956767\n",
      "Epoch 3 : 20.0 % done\n",
      "Current loss: 24839.288777111844\n",
      "Epoch 3 : 30.0 % done\n",
      "Current loss: 37267.06411277523\n",
      "Epoch 3 : 40.0 % done\n",
      "Current loss: 49707.151002840656\n",
      "Epoch 3 : 50.0 % done\n",
      "Current loss: 61912.351433024\n",
      "Epoch 3 : 60.0 % done\n",
      "Current loss: 74259.51660880924\n",
      "Epoch 3 : 70.0 % done\n",
      "Current loss: 86531.62679990176\n",
      "Epoch 3 : 80.0 % done\n",
      "Current loss: 99127.75532054354\n",
      "Epoch 3 : 90.0 % done\n",
      "Current loss: 111438.1680501987\n",
      "Epoch 3 : 100.0 % done\n",
      "Current loss: 123831.74110614497\n",
      "\n",
      "Epoch 3 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(5.3922, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(0.9921, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8567, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-15.3182, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(-0.4107, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(49.4718, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(-6.3482, device='cuda:0')\n",
      "Epoch 4 : 0.0 % done\n",
      "Current loss: 1.8880661010742188\n",
      "Epoch 4 : 10.0 % done\n",
      "Current loss: 12288.776582563865\n",
      "Epoch 4 : 20.0 % done\n",
      "Current loss: 24565.762130270432\n",
      "Epoch 4 : 30.0 % done\n",
      "Current loss: 36806.0066148427\n",
      "Epoch 4 : 40.0 % done\n",
      "Current loss: 49246.87562463095\n",
      "Epoch 4 : 50.0 % done\n",
      "Current loss: 61492.75120489294\n",
      "Epoch 4 : 60.0 % done\n",
      "Current loss: 74005.79515447294\n",
      "Epoch 4 : 70.0 % done\n",
      "Current loss: 86261.79913384154\n",
      "Epoch 4 : 80.0 % done\n",
      "Current loss: 98717.84406413208\n",
      "Epoch 4 : 90.0 % done\n",
      "Current loss: 110854.87902861847\n",
      "Epoch 4 : 100.0 % done\n",
      "Current loss: 123153.36200751633\n",
      "\n",
      "Epoch 4 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(5.5260, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(1.0783, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8507, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-15.1709, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(-0.3309, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(50.2653, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(-5.9486, device='cuda:0')\n",
      "Epoch 5 : 0.0 % done\n",
      "Current loss: 3.7898441314697267\n",
      "Epoch 5 : 10.0 % done\n",
      "Current loss: 12221.795358007234\n",
      "Epoch 5 : 20.0 % done\n",
      "Current loss: 24465.52333121562\n",
      "Epoch 5 : 30.0 % done\n",
      "Current loss: 36750.555224294425\n",
      "Epoch 5 : 40.0 % done\n",
      "Current loss: 49003.71851250644\n",
      "Epoch 5 : 50.0 % done\n",
      "Current loss: 61229.96523461548\n",
      "Epoch 5 : 60.0 % done\n",
      "Current loss: 73467.93720768021\n",
      "Epoch 5 : 70.0 % done\n",
      "Current loss: 85757.65122302578\n",
      "Epoch 5 : 80.0 % done\n",
      "Current loss: 97875.93954470615\n",
      "Epoch 5 : 90.0 % done\n",
      "Current loss: 110062.48187827665\n",
      "Epoch 5 : 100.0 % done\n",
      "Current loss: 122419.48756226126\n",
      "\n",
      "Epoch 5 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(5.6799, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(1.1872, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8457, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-14.9947, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(-0.2350, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(51.1457, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(-5.5669, device='cuda:0')\n",
      "Epoch 6 : 0.0 % done\n",
      "Current loss: 4.896584987640381\n",
      "Epoch 6 : 10.0 % done\n",
      "Current loss: 12229.142691918509\n",
      "Epoch 6 : 20.0 % done\n",
      "Current loss: 24537.487149318313\n",
      "Epoch 6 : 30.0 % done\n",
      "Current loss: 36668.82759259996\n",
      "Epoch 6 : 40.0 % done\n",
      "Current loss: 48839.62259250806\n",
      "Epoch 6 : 50.0 % done\n",
      "Current loss: 60919.36738470982\n",
      "Epoch 6 : 60.0 % done\n",
      "Current loss: 73278.35266181138\n",
      "Epoch 6 : 70.0 % done\n",
      "Current loss: 85171.01645492658\n",
      "Epoch 6 : 80.0 % done\n",
      "Current loss: 97280.51034205021\n",
      "Epoch 6 : 90.0 % done\n",
      "Current loss: 109546.94632479125\n",
      "Epoch 6 : 100.0 % done\n",
      "Current loss: 121620.67721611365\n",
      "\n",
      "Epoch 6 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(5.8536, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(1.3192, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8416, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-14.7916, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(-0.1240, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(52.0934, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(-5.2049, device='cuda:0')\n",
      "Epoch 7 : 0.0 % done\n",
      "Current loss: 3.8763095855712892\n",
      "Epoch 7 : 10.0 % done\n",
      "Current loss: 12051.600051622992\n",
      "Epoch 7 : 20.0 % done\n",
      "Current loss: 24252.398720329013\n",
      "Epoch 7 : 30.0 % done\n",
      "Current loss: 36276.11996803636\n",
      "Epoch 7 : 40.0 % done\n",
      "Current loss: 48179.47603843347\n",
      "Epoch 7 : 50.0 % done\n",
      "Current loss: 60071.336091629084\n",
      "Epoch 7 : 60.0 % done\n",
      "Current loss: 72373.13569537851\n",
      "Epoch 7 : 70.0 % done\n",
      "Current loss: 84562.00078910975\n",
      "Epoch 7 : 80.0 % done\n",
      "Current loss: 96555.68640319181\n",
      "Epoch 7 : 90.0 % done\n",
      "Current loss: 108649.29657370849\n",
      "Epoch 7 : 100.0 % done\n",
      "Current loss: 120741.11661326465\n",
      "\n",
      "Epoch 7 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(6.0464, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(1.4752, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8381, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-14.5636, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(0.0009, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(53.1106, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(-4.8635, device='cuda:0')\n",
      "Epoch 8 : 0.0 % done\n",
      "Current loss: 6.577459971110026\n",
      "Epoch 8 : 10.0 % done\n",
      "Current loss: 11812.964878980803\n",
      "Epoch 8 : 20.0 % done\n",
      "Current loss: 23817.6374101592\n",
      "Epoch 8 : 30.0 % done\n",
      "Current loss: 35721.260484203085\n",
      "Epoch 8 : 40.0 % done\n",
      "Current loss: 47550.323554291215\n",
      "Epoch 8 : 50.0 % done\n",
      "Current loss: 59656.2108983413\n",
      "Epoch 8 : 60.0 % done\n",
      "Current loss: 71786.3050519904\n",
      "Epoch 8 : 70.0 % done\n",
      "Current loss: 83503.28925627162\n",
      "Epoch 8 : 80.0 % done\n",
      "Current loss: 95681.5246723352\n",
      "Epoch 8 : 90.0 % done\n",
      "Current loss: 107732.3323610055\n",
      "Epoch 8 : 100.0 % done\n",
      "Current loss: 119792.8746971414\n",
      "\n",
      "Epoch 8 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(6.2570, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(1.6558, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8359, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-14.3129, device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder obj lstm bias sum= tensor(0.1383, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(54.1911, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(-4.5419, device='cuda:0')\n",
      "Epoch 9 : 0.0 % done\n",
      "Current loss: 4.986844062805176\n",
      "Epoch 9 : 10.0 % done\n",
      "Current loss: 11941.203287185077\n",
      "Epoch 9 : 20.0 % done\n",
      "Current loss: 23822.639320536295\n",
      "Epoch 9 : 30.0 % done\n",
      "Current loss: 35845.762511963556\n",
      "Epoch 9 : 40.0 % done\n",
      "Current loss: 47726.2158771547\n",
      "Epoch 9 : 50.0 % done\n",
      "Current loss: 59526.074597624705\n",
      "Epoch 9 : 60.0 % done\n",
      "Current loss: 71294.32431022631\n",
      "Epoch 9 : 70.0 % done\n",
      "Current loss: 83189.1776628766\n",
      "Epoch 9 : 80.0 % done\n",
      "Current loss: 95074.94133035587\n",
      "Epoch 9 : 90.0 % done\n",
      "Current loss: 106861.63337565398\n",
      "Epoch 9 : 100.0 % done\n",
      "Current loss: 118753.40509349116\n",
      "\n",
      "Epoch 9 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(6.4848, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(1.8619, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8347, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-14.0418, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(0.2869, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(55.3360, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(-4.2366, device='cuda:0')\n",
      "Epoch 10 : 0.0 % done\n",
      "Current loss: 2.5620972769601003\n",
      "Epoch 10 : 10.0 % done\n",
      "Current loss: 11653.522291274972\n",
      "Epoch 10 : 20.0 % done\n",
      "Current loss: 23522.77213278673\n",
      "Epoch 10 : 30.0 % done\n",
      "Current loss: 35261.023121159684\n",
      "Epoch 10 : 40.0 % done\n",
      "Current loss: 46998.15129843277\n",
      "Epoch 10 : 50.0 % done\n",
      "Current loss: 58893.4830111586\n",
      "Epoch 10 : 60.0 % done\n",
      "Current loss: 70708.55664933352\n",
      "Epoch 10 : 70.0 % done\n",
      "Current loss: 82533.80874606845\n",
      "Epoch 10 : 80.0 % done\n",
      "Current loss: 94297.00579254709\n",
      "Epoch 10 : 90.0 % done\n",
      "Current loss: 105872.33002206236\n",
      "Epoch 10 : 100.0 % done\n",
      "Current loss: 117645.35993147649\n",
      "\n",
      "Epoch 10 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(6.7283, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(2.0942, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8342, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-13.7528, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(0.4451, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(56.5448, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(-3.9424, device='cuda:0')\n",
      "Epoch 11 : 0.0 % done\n",
      "Current loss: 8.371326446533203\n",
      "Epoch 11 : 10.0 % done\n",
      "Current loss: 11864.813257438145\n",
      "Epoch 11 : 20.0 % done\n",
      "Current loss: 23670.15926157069\n",
      "Epoch 11 : 30.0 % done\n",
      "Current loss: 35240.79615908723\n",
      "Epoch 11 : 40.0 % done\n",
      "Current loss: 46997.714877779734\n",
      "Epoch 11 : 50.0 % done\n",
      "Current loss: 58625.52128451639\n",
      "Epoch 11 : 60.0 % done\n",
      "Current loss: 70099.74416586617\n",
      "Epoch 11 : 70.0 % done\n",
      "Current loss: 81795.61485268864\n",
      "Epoch 11 : 80.0 % done\n",
      "Current loss: 93403.35768343361\n",
      "Epoch 11 : 90.0 % done\n",
      "Current loss: 104852.2692440494\n",
      "Epoch 11 : 100.0 % done\n",
      "Current loss: 116440.29937713189\n",
      "\n",
      "Epoch 11 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(6.9865, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(2.3528, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8340, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-13.4484, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(0.6115, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(57.8132, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(-3.6548, device='cuda:0')\n",
      "Epoch 12 : 0.0 % done\n",
      "Current loss: 2.4527318477630615\n",
      "Epoch 12 : 10.0 % done\n",
      "Current loss: 11593.449580744924\n",
      "Epoch 12 : 20.0 % done\n",
      "Current loss: 23088.697465211302\n",
      "Epoch 12 : 30.0 % done\n",
      "Current loss: 34626.13503185205\n",
      "Epoch 12 : 40.0 % done\n",
      "Current loss: 46201.568204408824\n",
      "Epoch 12 : 50.0 % done\n",
      "Current loss: 57703.73005735206\n",
      "Epoch 12 : 60.0 % done\n",
      "Current loss: 69327.2549950342\n",
      "Epoch 12 : 70.0 % done\n",
      "Current loss: 80747.88877745114\n",
      "Epoch 12 : 80.0 % done\n",
      "Current loss: 92204.51367164296\n",
      "Epoch 12 : 90.0 % done\n",
      "Current loss: 103607.27618239392\n",
      "Epoch 12 : 100.0 % done\n",
      "Current loss: 115178.3636918119\n",
      "\n",
      "Epoch 12 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(7.2569, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(2.6370, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8340, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-13.1319, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(0.7841, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(59.1310, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(-3.3692, device='cuda:0')\n",
      "Epoch 13 : 0.0 % done\n",
      "Current loss: 4.8353681564331055\n",
      "Epoch 13 : 10.0 % done\n",
      "Current loss: 11538.261210495364\n",
      "Epoch 13 : 20.0 % done\n",
      "Current loss: 23007.382901044948\n",
      "Epoch 13 : 30.0 % done\n",
      "Current loss: 34281.378788760594\n",
      "Epoch 13 : 40.0 % done\n",
      "Current loss: 45814.121043117644\n",
      "Epoch 13 : 50.0 % done\n",
      "Current loss: 57118.19573518521\n",
      "Epoch 13 : 60.0 % done\n",
      "Current loss: 68490.02056393407\n",
      "Epoch 13 : 70.0 % done\n",
      "Current loss: 79920.00232579133\n",
      "Epoch 13 : 80.0 % done\n",
      "Current loss: 91319.01639616671\n",
      "Epoch 13 : 90.0 % done\n",
      "Current loss: 102474.67862890728\n",
      "Epoch 13 : 100.0 % done\n",
      "Current loss: 113843.60873760538\n",
      "\n",
      "Epoch 13 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(7.5352, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(2.9443, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8341, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-12.8068, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(0.9609, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(60.4916, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(-3.0832, device='cuda:0')\n",
      "Epoch 14 : 0.0 % done\n",
      "Current loss: 2.775256565638951\n",
      "Epoch 14 : 10.0 % done\n",
      "Current loss: 11277.337711593802\n",
      "Epoch 14 : 20.0 % done\n",
      "Current loss: 22485.11475732899\n",
      "Epoch 14 : 30.0 % done\n",
      "Current loss: 33803.02540221519\n",
      "Epoch 14 : 40.0 % done\n",
      "Current loss: 44972.75665082315\n",
      "Epoch 14 : 50.0 % done\n",
      "Current loss: 56276.79135972258\n",
      "Epoch 14 : 60.0 % done\n",
      "Current loss: 67428.7222274338\n",
      "Epoch 14 : 70.0 % done\n",
      "Current loss: 78627.83975096491\n",
      "Epoch 14 : 80.0 % done\n",
      "Current loss: 90017.36660843894\n",
      "Epoch 14 : 90.0 % done\n",
      "Current loss: 101398.65181427317\n",
      "Epoch 14 : 100.0 % done\n",
      "Current loss: 112460.01019068337\n",
      "\n",
      "Epoch 14 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(7.8161, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(3.2706, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8342, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-12.4780, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(1.1393, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(61.8877, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(-2.7961, device='cuda:0')\n",
      "Epoch 15 : 0.0 % done\n",
      "Current loss: 7.5305705070495605\n",
      "Epoch 15 : 10.0 % done\n",
      "Current loss: 11063.066000280392\n",
      "Epoch 15 : 20.0 % done\n",
      "Current loss: 22244.810495753703\n",
      "Epoch 15 : 30.0 % done\n",
      "Current loss: 33381.91853249241\n",
      "Epoch 15 : 40.0 % done\n",
      "Current loss: 44586.68472797254\n",
      "Epoch 15 : 50.0 % done\n",
      "Current loss: 55653.412964284755\n",
      "Epoch 15 : 60.0 % done\n",
      "Current loss: 66768.70762798382\n",
      "Epoch 15 : 70.0 % done\n",
      "Current loss: 77734.33415183157\n",
      "Epoch 15 : 80.0 % done\n",
      "Current loss: 88852.54968077308\n",
      "Epoch 15 : 90.0 % done\n",
      "Current loss: 99836.35972456005\n",
      "Epoch 15 : 100.0 % done\n",
      "Current loss: 111070.88801819555\n",
      "\n",
      "Epoch 15 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(8.0917, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(3.6084, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8342, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-12.1517, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(1.3161, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(63.3128, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(-2.5086, device='cuda:0')\n",
      "Epoch 16 : 0.0 % done\n",
      "Current loss: 3.12555726369222\n",
      "Epoch 16 : 10.0 % done\n",
      "Current loss: 11135.001210585608\n",
      "Epoch 16 : 20.0 % done\n",
      "Current loss: 22058.137529505428\n",
      "Epoch 16 : 30.0 % done\n",
      "Current loss: 33102.697384226056\n",
      "Epoch 16 : 40.0 % done\n",
      "Current loss: 44272.218466196384\n",
      "Epoch 16 : 50.0 % done\n",
      "Current loss: 55198.26041891218\n",
      "Epoch 16 : 60.0 % done\n",
      "Current loss: 66077.48565520125\n",
      "Epoch 16 : 70.0 % done\n",
      "Current loss: 77031.80453658929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 : 80.0 % done\n",
      "Current loss: 88041.95402068774\n",
      "Epoch 16 : 90.0 % done\n",
      "Current loss: 98851.52716449038\n",
      "Epoch 16 : 100.0 % done\n",
      "Current loss: 109692.75820816717\n",
      "\n",
      "Epoch 16 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(8.3522, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(3.9472, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8342, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-11.8367, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(1.4868, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(64.7599, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(-2.2221, device='cuda:0')\n",
      "Epoch 17 : 0.0 % done\n",
      "Current loss: 3.5951175689697266\n",
      "Epoch 17 : 10.0 % done\n",
      "Current loss: 10998.101140331793\n",
      "Epoch 17 : 20.0 % done\n",
      "Current loss: 21806.48338822661\n",
      "Epoch 17 : 30.0 % done\n",
      "Current loss: 32704.532314967208\n",
      "Epoch 17 : 40.0 % done\n",
      "Current loss: 43651.725291980416\n",
      "Epoch 17 : 50.0 % done\n",
      "Current loss: 54519.81410588833\n",
      "Epoch 17 : 60.0 % done\n",
      "Current loss: 65331.5157923623\n",
      "Epoch 17 : 70.0 % done\n",
      "Current loss: 76043.46790051446\n",
      "Epoch 17 : 80.0 % done\n",
      "Current loss: 86887.10232964449\n",
      "Epoch 17 : 90.0 % done\n",
      "Current loss: 97625.45720733803\n",
      "Epoch 17 : 100.0 % done\n",
      "Current loss: 108423.40455617804\n",
      "\n",
      "Epoch 17 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(8.5858, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(4.2723, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8347, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-11.5440, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(1.6462, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(66.2229, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(-1.9377, device='cuda:0')\n",
      "Epoch 18 : 0.0 % done\n",
      "Current loss: 3.9505332946777343\n",
      "Epoch 18 : 10.0 % done\n",
      "Current loss: 10752.711121109518\n",
      "Epoch 18 : 20.0 % done\n",
      "Current loss: 21548.09714220875\n",
      "Epoch 18 : 30.0 % done\n",
      "Current loss: 32363.657762425166\n",
      "Epoch 18 : 40.0 % done\n",
      "Current loss: 43226.76403684681\n",
      "Epoch 18 : 50.0 % done\n",
      "Current loss: 53905.330359176995\n",
      "Epoch 18 : 60.0 % done\n",
      "Current loss: 64665.37239734126\n",
      "Epoch 18 : 70.0 % done\n",
      "Current loss: 75350.99412115583\n",
      "Epoch 18 : 80.0 % done\n",
      "Current loss: 86018.51046391205\n",
      "Epoch 18 : 90.0 % done\n",
      "Current loss: 96661.99229975809\n",
      "Epoch 18 : 100.0 % done\n",
      "Current loss: 107301.16382873052\n",
      "\n",
      "Epoch 18 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(8.7829, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(4.5697, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8353, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-11.2839, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(1.7898, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(67.6961, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(-1.6566, device='cuda:0')\n",
      "Epoch 19 : 0.0 % done\n",
      "Current loss: 1.1801237626509233\n",
      "Epoch 19 : 10.0 % done\n",
      "Current loss: 10650.791389740358\n",
      "Epoch 19 : 20.0 % done\n",
      "Current loss: 21361.165605203132\n",
      "Epoch 19 : 30.0 % done\n",
      "Current loss: 32105.481190942788\n",
      "Epoch 19 : 40.0 % done\n",
      "Current loss: 42870.20981360042\n",
      "Epoch 19 : 50.0 % done\n",
      "Current loss: 53418.54639693784\n",
      "Epoch 19 : 60.0 % done\n",
      "Current loss: 63915.31088317615\n",
      "Epoch 19 : 70.0 % done\n",
      "Current loss: 74534.37710418616\n",
      "Epoch 19 : 80.0 % done\n",
      "Current loss: 85208.57581727438\n",
      "Epoch 19 : 90.0 % done\n",
      "Current loss: 95781.48869650513\n",
      "Epoch 19 : 100.0 % done\n",
      "Current loss: 106369.83195461649\n",
      "\n",
      "Epoch 19 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(8.9387, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(4.8297, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8362, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-11.0631, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(1.9149, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(69.1734, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(-1.3799, device='cuda:0')\n",
      "Epoch 20 : 0.0 % done\n",
      "Current loss: 2.819746562412807\n",
      "Epoch 20 : 10.0 % done\n",
      "Current loss: 10632.104049789736\n",
      "Epoch 20 : 20.0 % done\n",
      "Current loss: 21148.987631767308\n",
      "Epoch 20 : 30.0 % done\n",
      "Current loss: 31757.425005688772\n",
      "Epoch 20 : 40.0 % done\n",
      "Current loss: 42307.86916928032\n",
      "Epoch 20 : 50.0 % done\n",
      "Current loss: 52849.80587781224\n",
      "Epoch 20 : 60.0 % done\n",
      "Current loss: 63411.82063164981\n",
      "Epoch 20 : 70.0 % done\n",
      "Current loss: 74139.619494592\n",
      "Epoch 20 : 80.0 % done\n",
      "Current loss: 84577.84826418325\n",
      "Epoch 20 : 90.0 % done\n",
      "Current loss: 95014.46993004282\n",
      "Epoch 20 : 100.0 % done\n",
      "Current loss: 105613.82518161164\n",
      "\n",
      "Epoch 20 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(9.0570, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(5.0536, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8374, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-10.8810, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(2.0228, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(70.6566, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(-1.1081, device='cuda:0')\n",
      "Epoch 21 : 0.0 % done\n",
      "Current loss: 2.7647794087727866\n",
      "Epoch 21 : 10.0 % done\n",
      "Current loss: 10527.7148612158\n",
      "Epoch 21 : 20.0 % done\n",
      "Current loss: 20827.29972461134\n",
      "Epoch 21 : 30.0 % done\n",
      "Current loss: 31398.494170863138\n",
      "Epoch 21 : 40.0 % done\n",
      "Current loss: 41934.63892702064\n",
      "Epoch 21 : 50.0 % done\n",
      "Current loss: 52433.53807743478\n",
      "Epoch 21 : 60.0 % done\n",
      "Current loss: 62890.361651075735\n",
      "Epoch 21 : 70.0 % done\n",
      "Current loss: 73550.46886982478\n",
      "Epoch 21 : 80.0 % done\n",
      "Current loss: 83914.60210840723\n",
      "Epoch 21 : 90.0 % done\n",
      "Current loss: 94477.93329100174\n",
      "Epoch 21 : 100.0 % done\n",
      "Current loss: 105011.71176650471\n",
      "\n",
      "Epoch 21 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(9.1446, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(5.2464, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8389, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-10.7305, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(2.1164, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(72.1465, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(-0.8411, device='cuda:0')\n",
      "Epoch 22 : 0.0 % done\n",
      "Current loss: 2.1041111416286893\n",
      "Epoch 22 : 10.0 % done\n",
      "Current loss: 10414.67200860815\n",
      "Epoch 22 : 20.0 % done\n",
      "Current loss: 21021.64691952993\n",
      "Epoch 22 : 30.0 % done\n",
      "Current loss: 31521.064430829083\n",
      "Epoch 22 : 40.0 % done\n",
      "Current loss: 42007.257125976976\n",
      "Epoch 22 : 50.0 % done\n",
      "Current loss: 52353.52261175357\n",
      "Epoch 22 : 60.0 % done\n",
      "Current loss: 62762.91884436473\n",
      "Epoch 22 : 70.0 % done\n",
      "Current loss: 73176.01021870352\n",
      "Epoch 22 : 80.0 % done\n",
      "Current loss: 83545.87662270629\n",
      "Epoch 22 : 90.0 % done\n",
      "Current loss: 94095.82528940233\n",
      "Epoch 22 : 100.0 % done\n",
      "Current loss: 104499.74955858986\n",
      "\n",
      "Epoch 22 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(9.2097, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(5.4171, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8402, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-10.6028, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(2.2001, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(73.6464, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(-0.5783, device='cuda:0')\n",
      "Epoch 23 : 0.0 % done\n",
      "Current loss: 4.873167991638184\n",
      "Epoch 23 : 10.0 % done\n",
      "Current loss: 10363.859563829643\n",
      "Epoch 23 : 20.0 % done\n",
      "Current loss: 20908.84837407577\n",
      "Epoch 23 : 30.0 % done\n",
      "Current loss: 31408.844118864272\n",
      "Epoch 23 : 40.0 % done\n",
      "Current loss: 41858.253567721855\n",
      "Epoch 23 : 50.0 % done\n",
      "Current loss: 52033.491331440375\n",
      "Epoch 23 : 60.0 % done\n",
      "Current loss: 62398.05948909328\n",
      "Epoch 23 : 70.0 % done\n",
      "Current loss: 72717.06659227487\n",
      "Epoch 23 : 80.0 % done\n",
      "Current loss: 83147.90701847989\n",
      "Epoch 23 : 90.0 % done\n",
      "Current loss: 93633.62458831475\n",
      "Epoch 23 : 100.0 % done\n",
      "Current loss: 104070.54534402897\n",
      "\n",
      "Epoch 23 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(9.2580, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(5.5715, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8418, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-10.4893, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(2.2769, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(75.1587, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(-0.3193, device='cuda:0')\n",
      "Epoch 24 : 0.0 % done\n",
      "Current loss: 1.434401512145996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 : 10.0 % done\n",
      "Current loss: 10452.018523840125\n",
      "Epoch 24 : 20.0 % done\n",
      "Current loss: 20727.797875684104\n",
      "Epoch 24 : 30.0 % done\n",
      "Current loss: 31158.921863480326\n",
      "Epoch 24 : 40.0 % done\n",
      "Current loss: 41505.39982411048\n",
      "Epoch 24 : 50.0 % done\n",
      "Current loss: 52075.93043320931\n",
      "Epoch 24 : 60.0 % done\n",
      "Current loss: 62503.8879984145\n",
      "Epoch 24 : 70.0 % done\n",
      "Current loss: 72763.03322129507\n",
      "Epoch 24 : 80.0 % done\n",
      "Current loss: 83040.14428902388\n",
      "Epoch 24 : 90.0 % done\n",
      "Current loss: 93421.43326177093\n",
      "Epoch 24 : 100.0 % done\n",
      "Current loss: 103683.25614448688\n",
      "\n",
      "Epoch 24 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(9.2946, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(5.7152, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8435, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-10.3851, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(2.3491, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(76.6861, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(-0.0635, device='cuda:0')\n",
      "Epoch 25 : 0.0 % done\n",
      "Current loss: 1.464758767022027\n",
      "Epoch 25 : 10.0 % done\n",
      "Current loss: 10337.758363881696\n",
      "Epoch 25 : 20.0 % done\n",
      "Current loss: 20770.71361957392\n",
      "Epoch 25 : 30.0 % done\n",
      "Current loss: 31122.346441414476\n",
      "Epoch 25 : 40.0 % done\n",
      "Current loss: 41652.083913382376\n",
      "Epoch 25 : 50.0 % done\n",
      "Current loss: 52049.42113929884\n",
      "Epoch 25 : 60.0 % done\n",
      "Current loss: 62350.31121593762\n",
      "Epoch 25 : 70.0 % done\n",
      "Current loss: 72527.63520464666\n",
      "Epoch 25 : 80.0 % done\n",
      "Current loss: 82734.55414485963\n",
      "Epoch 25 : 90.0 % done\n",
      "Current loss: 93044.92046816085\n",
      "Epoch 25 : 100.0 % done\n",
      "Current loss: 103333.92894250767\n",
      "\n",
      "Epoch 25 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(9.3231, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(5.8527, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8452, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-10.2838, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(2.4192, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(78.2302, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(0.1896, device='cuda:0')\n",
      "Epoch 26 : 0.0 % done\n",
      "Current loss: 4.763584136962891\n",
      "Epoch 26 : 10.0 % done\n",
      "Current loss: 10321.657909710633\n",
      "Epoch 26 : 20.0 % done\n",
      "Current loss: 20727.921475296538\n",
      "Epoch 26 : 30.0 % done\n",
      "Current loss: 30946.343464596703\n",
      "Epoch 26 : 40.0 % done\n",
      "Current loss: 41282.13853184487\n",
      "Epoch 26 : 50.0 % done\n",
      "Current loss: 51549.49295854054\n",
      "Epoch 26 : 60.0 % done\n",
      "Current loss: 61795.037517233286\n",
      "Epoch 26 : 70.0 % done\n",
      "Current loss: 72180.87370580657\n",
      "Epoch 26 : 80.0 % done\n",
      "Current loss: 82435.97084616378\n",
      "Epoch 26 : 90.0 % done\n",
      "Current loss: 92786.67694550486\n",
      "Epoch 26 : 100.0 % done\n",
      "Current loss: 103006.56203755864\n",
      "\n",
      "Epoch 26 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(9.3429, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(5.9833, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8470, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-10.1846, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(2.4871, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(79.7896, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(0.4401, device='cuda:0')\n",
      "Epoch 27 : 0.0 % done\n",
      "Current loss: 1.8609966550554549\n",
      "Epoch 27 : 10.0 % done\n",
      "Current loss: 10256.235940511739\n",
      "Epoch 27 : 20.0 % done\n",
      "Current loss: 20473.08207087193\n",
      "Epoch 27 : 30.0 % done\n",
      "Current loss: 30705.648918236326\n",
      "Epoch 27 : 40.0 % done\n",
      "Current loss: 41075.561720720805\n",
      "Epoch 27 : 50.0 % done\n",
      "Current loss: 51324.82513494936\n",
      "Epoch 27 : 60.0 % done\n",
      "Current loss: 61690.73865185212\n",
      "Epoch 27 : 70.0 % done\n",
      "Current loss: 71891.7121043364\n",
      "Epoch 27 : 80.0 % done\n",
      "Current loss: 82131.92498679519\n",
      "Epoch 27 : 90.0 % done\n",
      "Current loss: 92358.16107155605\n",
      "Epoch 27 : 100.0 % done\n",
      "Current loss: 102688.31871804099\n",
      "\n",
      "Epoch 27 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(9.3574, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(6.1105, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8488, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-10.0861, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(2.5539, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(81.3660, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(0.6884, device='cuda:0')\n",
      "Epoch 28 : 0.0 % done\n",
      "Current loss: 2.612278366088867\n",
      "Epoch 28 : 10.0 % done\n",
      "Current loss: 10130.41460117573\n",
      "Epoch 28 : 20.0 % done\n",
      "Current loss: 20196.23050598236\n",
      "Epoch 28 : 30.0 % done\n",
      "Current loss: 30352.13286881581\n",
      "Epoch 28 : 40.0 % done\n",
      "Current loss: 40724.904696362355\n",
      "Epoch 28 : 50.0 % done\n",
      "Current loss: 50908.03600940964\n",
      "Epoch 28 : 60.0 % done\n",
      "Current loss: 61106.07766569202\n",
      "Epoch 28 : 70.0 % done\n",
      "Current loss: 71346.89816649062\n",
      "Epoch 28 : 80.0 % done\n",
      "Current loss: 81771.3350993775\n",
      "Epoch 28 : 90.0 % done\n",
      "Current loss: 92074.93563110265\n",
      "Epoch 28 : 100.0 % done\n",
      "Current loss: 102374.1261054926\n",
      "\n",
      "Epoch 28 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(9.3685, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(6.2376, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8503, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-9.9843, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(2.6210, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(82.9612, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(0.9348, device='cuda:0')\n",
      "Epoch 29 : 0.0 % done\n",
      "Current loss: 1.9100086905739524\n",
      "Epoch 29 : 10.0 % done\n",
      "Current loss: 10206.895242869865\n",
      "Epoch 29 : 20.0 % done\n",
      "Current loss: 20492.123339561072\n",
      "Epoch 29 : 30.0 % done\n",
      "Current loss: 30750.63439234415\n",
      "Epoch 29 : 40.0 % done\n",
      "Current loss: 40982.12852493422\n",
      "Epoch 29 : 50.0 % done\n",
      "Current loss: 51230.99186225307\n",
      "Epoch 29 : 60.0 % done\n",
      "Current loss: 61427.56870802133\n",
      "Epoch 29 : 70.0 % done\n",
      "Current loss: 71605.85352715173\n",
      "Epoch 29 : 80.0 % done\n",
      "Current loss: 81696.08804299096\n",
      "Epoch 29 : 90.0 % done\n",
      "Current loss: 91743.58904241127\n",
      "Epoch 29 : 100.0 % done\n",
      "Current loss: 102059.32206657852\n",
      "\n",
      "Epoch 29 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(9.3777, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(6.3660, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8521, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-9.8816, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(2.6882, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(84.5729, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(1.1793, device='cuda:0')\n",
      "Epoch 30 : 0.0 % done\n",
      "Current loss: 3.7747562408447264\n",
      "Epoch 30 : 10.0 % done\n",
      "Current loss: 10148.607143396013\n",
      "Epoch 30 : 20.0 % done\n",
      "Current loss: 20142.38063107962\n",
      "Epoch 30 : 30.0 % done\n",
      "Current loss: 30354.196450693085\n",
      "Epoch 30 : 40.0 % done\n",
      "Current loss: 40653.04323763237\n",
      "Epoch 30 : 50.0 % done\n",
      "Current loss: 50827.06511244299\n",
      "Epoch 30 : 60.0 % done\n",
      "Current loss: 61032.011785932424\n",
      "Epoch 30 : 70.0 % done\n",
      "Current loss: 71186.06439085187\n",
      "Epoch 30 : 80.0 % done\n",
      "Current loss: 81494.8797187554\n",
      "Epoch 30 : 90.0 % done\n",
      "Current loss: 91459.43553724683\n",
      "Epoch 30 : 100.0 % done\n",
      "Current loss: 101755.84347341802\n",
      "\n",
      "Epoch 30 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(9.3842, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(6.4948, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8538, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-9.7765, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(2.7556, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(86.2004, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(1.4217, device='cuda:0')\n",
      "Epoch 31 : 0.0 % done\n",
      "Current loss: 6.281262397766113\n",
      "Epoch 31 : 10.0 % done\n",
      "Current loss: 10138.856958306178\n",
      "Epoch 31 : 20.0 % done\n",
      "Current loss: 20198.377050149833\n",
      "Epoch 31 : 30.0 % done\n",
      "Current loss: 30481.804968444874\n",
      "Epoch 31 : 40.0 % done\n",
      "Current loss: 40792.29257928745\n",
      "Epoch 31 : 50.0 % done\n",
      "Current loss: 50956.03827078533\n",
      "Epoch 31 : 60.0 % done\n",
      "Current loss: 60990.82440867107\n",
      "Epoch 31 : 70.0 % done\n",
      "Current loss: 71087.03440116927\n",
      "Epoch 31 : 80.0 % done\n",
      "Current loss: 81248.22086138302\n",
      "Epoch 31 : 90.0 % done\n",
      "Current loss: 91402.79079856802\n",
      "Epoch 31 : 100.0 % done\n",
      "Current loss: 101442.58255443837\n",
      "\n",
      "Epoch 31 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(9.3903, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(6.6270, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8553, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-9.6700, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(2.8236, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(87.8439, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(1.6622, device='cuda:0')\n",
      "Epoch 32 : 0.0 % done\n",
      "Current loss: 2.316950480143229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 : 10.0 % done\n",
      "Current loss: 10280.010339053297\n",
      "Epoch 32 : 20.0 % done\n",
      "Current loss: 20439.451908669424\n",
      "Epoch 32 : 30.0 % done\n",
      "Current loss: 30483.432123664734\n",
      "Epoch 32 : 40.0 % done\n",
      "Current loss: 40650.452304095714\n",
      "Epoch 32 : 50.0 % done\n",
      "Current loss: 50812.0108677405\n",
      "Epoch 32 : 60.0 % done\n",
      "Current loss: 60804.370842420314\n",
      "Epoch 32 : 70.0 % done\n",
      "Current loss: 70918.06112625563\n",
      "Epoch 32 : 80.0 % done\n",
      "Current loss: 80886.86160440858\n",
      "Epoch 32 : 90.0 % done\n",
      "Current loss: 90975.02440025925\n",
      "Epoch 32 : 100.0 % done\n",
      "Current loss: 101127.17700201955\n",
      "\n",
      "Epoch 32 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(9.3937, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(6.7599, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8569, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-9.5624, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(2.8916, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(89.5017, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(1.9009, device='cuda:0')\n",
      "Epoch 33 : 0.0 % done\n",
      "Current loss: 4.083780670166016\n",
      "Epoch 33 : 10.0 % done\n",
      "Current loss: 10068.896220317745\n",
      "Epoch 33 : 20.0 % done\n",
      "Current loss: 20107.218372930187\n",
      "Epoch 33 : 30.0 % done\n",
      "Current loss: 30037.195023989858\n",
      "Epoch 33 : 40.0 % done\n",
      "Current loss: 40232.255154653605\n",
      "Epoch 33 : 50.0 % done\n",
      "Current loss: 50383.4362585559\n",
      "Epoch 33 : 60.0 % done\n",
      "Current loss: 60542.180060042345\n",
      "Epoch 33 : 70.0 % done\n",
      "Current loss: 70598.06815094296\n",
      "Epoch 33 : 80.0 % done\n",
      "Current loss: 80641.21577998601\n",
      "Epoch 33 : 90.0 % done\n",
      "Current loss: 90803.10357028943\n",
      "Epoch 33 : 100.0 % done\n",
      "Current loss: 100835.8669231502\n",
      "\n",
      "Epoch 33 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(9.3948, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(6.8932, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8585, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-9.4533, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(2.9597, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(91.1742, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(2.1378, device='cuda:0')\n",
      "Epoch 34 : 0.0 % done\n",
      "Current loss: 1.1617073482937283\n",
      "Epoch 34 : 10.0 % done\n",
      "Current loss: 10028.856068205845\n",
      "Epoch 34 : 20.0 % done\n",
      "Current loss: 20111.46894576046\n",
      "Epoch 34 : 30.0 % done\n",
      "Current loss: 30275.53218868002\n",
      "Epoch 34 : 40.0 % done\n",
      "Current loss: 40599.552807825195\n",
      "Epoch 34 : 50.0 % done\n",
      "Current loss: 50696.07722661954\n",
      "Epoch 34 : 60.0 % done\n",
      "Current loss: 60804.46215110512\n",
      "Epoch 34 : 70.0 % done\n",
      "Current loss: 70833.23206541067\n",
      "Epoch 34 : 80.0 % done\n",
      "Current loss: 80765.09679604453\n",
      "Epoch 34 : 90.0 % done\n",
      "Current loss: 90677.3994822697\n",
      "Epoch 34 : 100.0 % done\n",
      "Current loss: 100530.150506265\n",
      "\n",
      "Epoch 34 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(9.3938, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(7.0275, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8600, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-9.3458, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.0273, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(92.8601, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(2.3727, device='cuda:0')\n",
      "Epoch 35 : 0.0 % done\n",
      "Current loss: 0.9423358223655007\n",
      "Epoch 35 : 10.0 % done\n",
      "Current loss: 9989.455585233338\n",
      "Epoch 35 : 20.0 % done\n",
      "Current loss: 20073.431294907343\n",
      "Epoch 35 : 30.0 % done\n",
      "Current loss: 30025.44554024518\n",
      "Epoch 35 : 40.0 % done\n",
      "Current loss: 40153.83726292032\n",
      "Epoch 35 : 50.0 % done\n",
      "Current loss: 50204.17139276463\n",
      "Epoch 35 : 60.0 % done\n",
      "Current loss: 60222.13903535526\n",
      "Epoch 35 : 70.0 % done\n",
      "Current loss: 70177.85952463238\n",
      "Epoch 35 : 80.0 % done\n",
      "Current loss: 80172.6329279153\n",
      "Epoch 35 : 90.0 % done\n",
      "Current loss: 90155.14871375597\n",
      "Epoch 35 : 100.0 % done\n",
      "Current loss: 100264.76332564847\n",
      "\n",
      "Epoch 35 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(9.3893, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(7.1598, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8615, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-9.2373, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.0945, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(94.5575, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(2.6057, device='cuda:0')\n",
      "Epoch 36 : 0.0 % done\n",
      "Current loss: 6.6872202555338545\n",
      "Epoch 36 : 10.0 % done\n",
      "Current loss: 10022.98120617366\n",
      "Epoch 36 : 20.0 % done\n",
      "Current loss: 20058.393858646155\n",
      "Epoch 36 : 30.0 % done\n",
      "Current loss: 29994.929964979998\n",
      "Epoch 36 : 40.0 % done\n",
      "Current loss: 39985.83119663653\n",
      "Epoch 36 : 50.0 % done\n",
      "Current loss: 49911.05659070038\n",
      "Epoch 36 : 60.0 % done\n",
      "Current loss: 59834.79150491765\n",
      "Epoch 36 : 70.0 % done\n",
      "Current loss: 69812.17267320445\n",
      "Epoch 36 : 80.0 % done\n",
      "Current loss: 79814.58380485004\n",
      "Epoch 36 : 90.0 % done\n",
      "Current loss: 89861.184526825\n",
      "Epoch 36 : 100.0 % done\n",
      "Current loss: 99981.90069317295\n",
      "\n",
      "Epoch 36 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(9.3809, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(7.2900, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8630, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-9.1307, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.1605, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(96.2637, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(2.8369, device='cuda:0')\n",
      "Epoch 37 : 0.0 % done\n",
      "Current loss: 6.687232971191406\n",
      "Epoch 37 : 10.0 % done\n",
      "Current loss: 9971.246128033752\n",
      "Epoch 37 : 20.0 % done\n",
      "Current loss: 19982.8487364747\n",
      "Epoch 37 : 30.0 % done\n",
      "Current loss: 30007.7117872411\n",
      "Epoch 37 : 40.0 % done\n",
      "Current loss: 39908.180407031316\n",
      "Epoch 37 : 50.0 % done\n",
      "Current loss: 49905.60205085245\n",
      "Epoch 37 : 60.0 % done\n",
      "Current loss: 59883.828120041835\n",
      "Epoch 37 : 70.0 % done\n",
      "Current loss: 69768.43837040674\n",
      "Epoch 37 : 80.0 % done\n",
      "Current loss: 79800.46617361813\n",
      "Epoch 37 : 90.0 % done\n",
      "Current loss: 89817.42936104204\n",
      "Epoch 37 : 100.0 % done\n",
      "Current loss: 99710.75440581239\n",
      "\n",
      "Epoch 37 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(9.3694, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(7.4186, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8644, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-9.0254, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.2257, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(97.9807, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(3.0664, device='cuda:0')\n",
      "Epoch 38 : 0.0 % done\n",
      "Current loss: 4.547421455383301\n",
      "Epoch 38 : 10.0 % done\n",
      "Current loss: 9905.65654828239\n",
      "Epoch 38 : 20.0 % done\n",
      "Current loss: 19753.315807337047\n",
      "Epoch 38 : 30.0 % done\n",
      "Current loss: 29685.311467462692\n",
      "Epoch 38 : 40.0 % done\n",
      "Current loss: 39638.2135691566\n",
      "Epoch 38 : 50.0 % done\n",
      "Current loss: 49496.80502192996\n",
      "Epoch 38 : 60.0 % done\n",
      "Current loss: 59449.29063111809\n",
      "Epoch 38 : 70.0 % done\n",
      "Current loss: 69556.16235354957\n",
      "Epoch 38 : 80.0 % done\n",
      "Current loss: 79550.42662791317\n",
      "Epoch 38 : 90.0 % done\n",
      "Current loss: 89374.81152226306\n",
      "Epoch 38 : 100.0 % done\n",
      "Current loss: 99469.85618056398\n",
      "\n",
      "Epoch 38 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(9.3533, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(7.5427, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8657, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-8.9226, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.2892, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(99.7064, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(3.2936, device='cuda:0')\n",
      "Epoch 39 : 0.0 % done\n",
      "Current loss: 5.983011881510417\n",
      "Epoch 39 : 10.0 % done\n",
      "Current loss: 9948.342892668938\n",
      "Epoch 39 : 20.0 % done\n",
      "Current loss: 19900.063134144646\n",
      "Epoch 39 : 30.0 % done\n",
      "Current loss: 29882.479229900222\n",
      "Epoch 39 : 40.0 % done\n",
      "Current loss: 39753.84464756669\n",
      "Epoch 39 : 50.0 % done\n",
      "Current loss: 49665.05141190244\n",
      "Epoch 39 : 60.0 % done\n",
      "Current loss: 59616.27630988084\n",
      "Epoch 39 : 70.0 % done\n",
      "Current loss: 69680.84630703765\n",
      "Epoch 39 : 80.0 % done\n",
      "Current loss: 79527.15820567202\n",
      "Epoch 39 : 90.0 % done\n",
      "Current loss: 89370.56628281833\n",
      "Epoch 39 : 100.0 % done\n",
      "Current loss: 99218.49899419976\n",
      "\n",
      "Epoch 39 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(9.3328, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(7.6625, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8671, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-8.8230, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.3510, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(101.4374, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(3.5187, device='cuda:0')\n",
      "Epoch 40 : 0.0 % done\n",
      "Current loss: 6.3563551902771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 : 10.0 % done\n",
      "Current loss: 10024.503948352292\n",
      "Epoch 40 : 20.0 % done\n",
      "Current loss: 19871.558647847476\n",
      "Epoch 40 : 30.0 % done\n",
      "Current loss: 29716.428979952598\n",
      "Epoch 40 : 40.0 % done\n",
      "Current loss: 39409.90665570761\n",
      "Epoch 40 : 50.0 % done\n",
      "Current loss: 49229.78928433847\n",
      "Epoch 40 : 60.0 % done\n",
      "Current loss: 59188.283806162515\n",
      "Epoch 40 : 70.0 % done\n",
      "Current loss: 69085.68222119872\n",
      "Epoch 40 : 80.0 % done\n",
      "Current loss: 79181.65546703547\n",
      "Epoch 40 : 90.0 % done\n",
      "Current loss: 89004.83447225345\n",
      "Epoch 40 : 100.0 % done\n",
      "Current loss: 98999.28758950782\n",
      "\n",
      "Epoch 40 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(9.3078, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(7.7769, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8683, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-8.7260, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.4110, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(103.1735, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(3.7423, device='cuda:0')\n",
      "Epoch 41 : 0.0 % done\n",
      "Current loss: 2.532990264892578\n",
      "Epoch 41 : 10.0 % done\n",
      "Current loss: 9706.276327135949\n",
      "Epoch 41 : 20.0 % done\n",
      "Current loss: 19743.43785627319\n",
      "Epoch 41 : 30.0 % done\n",
      "Current loss: 29753.70207100945\n",
      "Epoch 41 : 40.0 % done\n",
      "Current loss: 39611.361598664655\n",
      "Epoch 41 : 50.0 % done\n",
      "Current loss: 49604.222741393205\n",
      "Epoch 41 : 60.0 % done\n",
      "Current loss: 59439.51261885847\n",
      "Epoch 41 : 70.0 % done\n",
      "Current loss: 69135.13302833216\n",
      "Epoch 41 : 80.0 % done\n",
      "Current loss: 79009.15931211137\n",
      "Epoch 41 : 90.0 % done\n",
      "Current loss: 88848.96613982793\n",
      "Epoch 41 : 100.0 % done\n",
      "Current loss: 98782.12491815606\n",
      "\n",
      "Epoch 41 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(9.2783, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(7.8859, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8696, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-8.6327, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.4689, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(104.9148, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(3.9638, device='cuda:0')\n",
      "Epoch 42 : 0.0 % done\n",
      "Current loss: 6.63677978515625\n",
      "Epoch 42 : 10.0 % done\n",
      "Current loss: 9825.281959626973\n",
      "Epoch 42 : 20.0 % done\n",
      "Current loss: 19667.892558024498\n",
      "Epoch 42 : 30.0 % done\n",
      "Current loss: 29446.399381951964\n",
      "Epoch 42 : 40.0 % done\n",
      "Current loss: 39334.53568184575\n",
      "Epoch 42 : 50.0 % done\n",
      "Current loss: 49267.10949703678\n",
      "Epoch 42 : 60.0 % done\n",
      "Current loss: 59152.33801899557\n",
      "Epoch 42 : 70.0 % done\n",
      "Current loss: 69080.57812579253\n",
      "Epoch 42 : 80.0 % done\n",
      "Current loss: 78777.42374604612\n",
      "Epoch 42 : 90.0 % done\n",
      "Current loss: 88667.88853956688\n",
      "Epoch 42 : 100.0 % done\n",
      "Current loss: 98572.63414786893\n",
      "\n",
      "Epoch 42 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(9.2442, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(7.9886, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8707, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-8.5429, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.5247, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(106.6583, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(4.1832, device='cuda:0')\n",
      "Epoch 43 : 0.0 % done\n",
      "Current loss: 2.8368966238839284\n",
      "Epoch 43 : 10.0 % done\n",
      "Current loss: 9873.69496374789\n",
      "Epoch 43 : 20.0 % done\n",
      "Current loss: 19577.678936564847\n",
      "Epoch 43 : 30.0 % done\n",
      "Current loss: 29475.96204616992\n",
      "Epoch 43 : 40.0 % done\n",
      "Current loss: 39421.01359762759\n",
      "Epoch 43 : 50.0 % done\n",
      "Current loss: 49365.77460550727\n",
      "Epoch 43 : 60.0 % done\n",
      "Current loss: 59025.66389944669\n",
      "Epoch 43 : 70.0 % done\n",
      "Current loss: 68806.41531292058\n",
      "Epoch 43 : 80.0 % done\n",
      "Current loss: 78776.36868983653\n",
      "Epoch 43 : 90.0 % done\n",
      "Current loss: 88533.76544653592\n",
      "Epoch 43 : 100.0 % done\n",
      "Current loss: 98368.47600292337\n",
      "\n",
      "Epoch 43 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(9.2054, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(8.0842, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8718, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-8.4576, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.5780, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(108.4046, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(4.4006, device='cuda:0')\n",
      "Epoch 44 : 0.0 % done\n",
      "Current loss: 0.7347925359552557\n",
      "Epoch 44 : 10.0 % done\n",
      "Current loss: 9933.824192981208\n",
      "Epoch 44 : 20.0 % done\n",
      "Current loss: 19651.096293761613\n",
      "Epoch 44 : 30.0 % done\n",
      "Current loss: 29424.852249131738\n",
      "Epoch 44 : 40.0 % done\n",
      "Current loss: 39218.31942121576\n",
      "Epoch 44 : 50.0 % done\n",
      "Current loss: 49102.05059559662\n",
      "Epoch 44 : 60.0 % done\n",
      "Current loss: 58933.8979372837\n",
      "Epoch 44 : 70.0 % done\n",
      "Current loss: 68690.81130947512\n",
      "Epoch 44 : 80.0 % done\n",
      "Current loss: 78546.41196758559\n",
      "Epoch 44 : 90.0 % done\n",
      "Current loss: 88280.96383749036\n",
      "Epoch 44 : 100.0 % done\n",
      "Current loss: 98198.04295744795\n",
      "\n",
      "Epoch 44 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(9.1628, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(8.1741, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8730, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-8.3755, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.6292, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(110.1527, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(4.6164, device='cuda:0')\n",
      "Epoch 45 : 0.0 % done\n",
      "Current loss: 3.5681512355804443\n",
      "Epoch 45 : 10.0 % done\n",
      "Current loss: 9772.953578029228\n",
      "Epoch 45 : 20.0 % done\n",
      "Current loss: 19561.51887279991\n",
      "Epoch 45 : 30.0 % done\n",
      "Current loss: 29454.293383430766\n",
      "Epoch 45 : 40.0 % done\n",
      "Current loss: 39319.32606429404\n",
      "Epoch 45 : 50.0 % done\n",
      "Current loss: 49042.272412576465\n",
      "Epoch 45 : 60.0 % done\n",
      "Current loss: 58843.13145946925\n",
      "Epoch 45 : 70.0 % done\n",
      "Current loss: 68711.18567423397\n",
      "Epoch 45 : 80.0 % done\n",
      "Current loss: 78318.93801582309\n",
      "Epoch 45 : 90.0 % done\n",
      "Current loss: 88191.24081898388\n",
      "Epoch 45 : 100.0 % done\n",
      "Current loss: 98023.32459295042\n",
      "\n",
      "Epoch 45 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(9.1159, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(8.2568, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8743, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-8.2979, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.6780, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(111.9013, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(4.8301, device='cuda:0')\n",
      "Epoch 46 : 0.0 % done\n",
      "Current loss: 3.97440185546875\n",
      "Epoch 46 : 10.0 % done\n",
      "Current loss: 9789.167682903912\n",
      "Epoch 46 : 20.0 % done\n",
      "Current loss: 19636.355896682813\n",
      "Epoch 46 : 30.0 % done\n",
      "Current loss: 29410.048561102\n",
      "Epoch 46 : 40.0 % done\n",
      "Current loss: 39040.63450877992\n",
      "Epoch 46 : 50.0 % done\n",
      "Current loss: 48949.76049457038\n",
      "Epoch 46 : 60.0 % done\n",
      "Current loss: 58677.68371783683\n",
      "Epoch 46 : 70.0 % done\n",
      "Current loss: 68306.56698034915\n",
      "Epoch 46 : 80.0 % done\n",
      "Current loss: 78159.3239124212\n",
      "Epoch 46 : 90.0 % done\n",
      "Current loss: 87933.16931769837\n",
      "Epoch 46 : 100.0 % done\n",
      "Current loss: 97873.18875407721\n",
      "\n",
      "Epoch 46 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(9.0655, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(8.3334, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8752, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-8.2237, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.7246, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(113.6498, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(5.0418, device='cuda:0')\n",
      "Epoch 47 : 0.0 % done\n",
      "Current loss: 2.646869659423828\n",
      "Epoch 47 : 10.0 % done\n",
      "Current loss: 9939.012273205488\n",
      "Epoch 47 : 20.0 % done\n",
      "Current loss: 19733.745297924328\n",
      "Epoch 47 : 30.0 % done\n",
      "Current loss: 29527.97447889556\n",
      "Epoch 47 : 40.0 % done\n",
      "Current loss: 39303.682509273545\n",
      "Epoch 47 : 50.0 % done\n",
      "Current loss: 49051.025265168304\n",
      "Epoch 47 : 60.0 % done\n",
      "Current loss: 58856.906745150605\n",
      "Epoch 47 : 70.0 % done\n",
      "Current loss: 68518.70197615691\n",
      "Epoch 47 : 80.0 % done\n",
      "Current loss: 78400.49808577444\n",
      "Epoch 47 : 90.0 % done\n",
      "Current loss: 88107.39042877575\n",
      "Epoch 47 : 100.0 % done\n",
      "Current loss: 97714.562183792\n",
      "\n",
      "Epoch 47 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(9.0117, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(8.4039, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8763, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-8.1541, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.7688, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(115.3955, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(5.2515, device='cuda:0')\n",
      "Epoch 48 : 0.0 % done\n",
      "Current loss: 1.5984260241190593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 : 10.0 % done\n",
      "Current loss: 9825.608603630297\n",
      "Epoch 48 : 20.0 % done\n",
      "Current loss: 19547.126517378794\n",
      "Epoch 48 : 30.0 % done\n",
      "Current loss: 29392.31757745523\n",
      "Epoch 48 : 40.0 % done\n",
      "Current loss: 39088.53118128643\n",
      "Epoch 48 : 50.0 % done\n",
      "Current loss: 48790.79336672832\n",
      "Epoch 48 : 60.0 % done\n",
      "Current loss: 58455.28207784661\n",
      "Epoch 48 : 70.0 % done\n",
      "Current loss: 68193.387357955\n",
      "Epoch 48 : 80.0 % done\n",
      "Current loss: 77924.77637233684\n",
      "Epoch 48 : 90.0 % done\n",
      "Current loss: 87693.34763979711\n",
      "Epoch 48 : 100.0 % done\n",
      "Current loss: 97574.88418821339\n",
      "\n",
      "Epoch 48 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(8.9549, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(8.4696, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8773, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-8.0870, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.8113, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(117.1413, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(5.4600, device='cuda:0')\n",
      "Epoch 49 : 0.0 % done\n",
      "Current loss: 3.3821395874023437\n",
      "Epoch 49 : 10.0 % done\n",
      "Current loss: 9681.74231025\n",
      "Epoch 49 : 20.0 % done\n",
      "Current loss: 19405.121272684028\n",
      "Epoch 49 : 30.0 % done\n",
      "Current loss: 29162.99065991274\n",
      "Epoch 49 : 40.0 % done\n",
      "Current loss: 38771.73698532014\n",
      "Epoch 49 : 50.0 % done\n",
      "Current loss: 48585.41351725196\n",
      "Epoch 49 : 60.0 % done\n",
      "Current loss: 58340.792977922974\n",
      "Epoch 49 : 70.0 % done\n",
      "Current loss: 68089.38911463552\n",
      "Epoch 49 : 80.0 % done\n",
      "Current loss: 77876.54833340122\n",
      "Epoch 49 : 90.0 % done\n",
      "Current loss: 87599.32474231243\n",
      "Epoch 49 : 100.0 % done\n",
      "Current loss: 97438.04064401491\n",
      "\n",
      "Epoch 49 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(8.8953, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(8.5299, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8783, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-8.0235, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.8516, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(118.8853, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(5.6670, device='cuda:0')\n",
      "Epoch 50 : 0.0 % done\n",
      "Current loss: 6.152388254801433\n",
      "Epoch 50 : 10.0 % done\n",
      "Current loss: 9632.021742475781\n",
      "Epoch 50 : 20.0 % done\n",
      "Current loss: 19449.25542147067\n",
      "Epoch 50 : 30.0 % done\n",
      "Current loss: 29042.650435785985\n",
      "Epoch 50 : 40.0 % done\n",
      "Current loss: 38723.41489634227\n",
      "Epoch 50 : 50.0 % done\n",
      "Current loss: 48587.55410265891\n",
      "Epoch 50 : 60.0 % done\n",
      "Current loss: 58295.409462546275\n",
      "Epoch 50 : 70.0 % done\n",
      "Current loss: 68149.01216699103\n",
      "Epoch 50 : 80.0 % done\n",
      "Current loss: 77807.07289340289\n",
      "Epoch 50 : 90.0 % done\n",
      "Current loss: 87532.2736974922\n",
      "Epoch 50 : 100.0 % done\n",
      "Current loss: 97312.83476013094\n",
      "\n",
      "Epoch 50 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(8.8334, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(8.5868, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8794, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-7.9626, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.8904, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(120.6285, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(5.8724, device='cuda:0')\n",
      "Epoch 51 : 0.0 % done\n",
      "Current loss: 5.161776542663574\n",
      "Epoch 51 : 10.0 % done\n",
      "Current loss: 9555.103885545672\n",
      "Epoch 51 : 20.0 % done\n",
      "Current loss: 19245.628175460817\n",
      "Epoch 51 : 30.0 % done\n",
      "Current loss: 29098.585473808864\n",
      "Epoch 51 : 40.0 % done\n",
      "Current loss: 38871.37075492812\n",
      "Epoch 51 : 50.0 % done\n",
      "Current loss: 48728.43214400995\n",
      "Epoch 51 : 60.0 % done\n",
      "Current loss: 58349.733525745745\n",
      "Epoch 51 : 70.0 % done\n",
      "Current loss: 68132.57499125988\n",
      "Epoch 51 : 80.0 % done\n",
      "Current loss: 77801.29992171293\n",
      "Epoch 51 : 90.0 % done\n",
      "Current loss: 87501.416268642\n",
      "Epoch 51 : 100.0 % done\n",
      "Current loss: 97183.07438024337\n",
      "\n",
      "Epoch 51 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(8.7695, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(8.6404, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8801, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-7.9039, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.9277, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(122.3712, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(6.0767, device='cuda:0')\n",
      "Epoch 52 : 0.0 % done\n",
      "Current loss: 6.266612529754639\n",
      "Epoch 52 : 10.0 % done\n",
      "Current loss: 9691.010475964758\n",
      "Epoch 52 : 20.0 % done\n",
      "Current loss: 19494.047126653248\n",
      "Epoch 52 : 30.0 % done\n",
      "Current loss: 29267.899859768746\n",
      "Epoch 52 : 40.0 % done\n",
      "Current loss: 38792.52315512162\n",
      "Epoch 52 : 50.0 % done\n",
      "Current loss: 48474.16138849989\n",
      "Epoch 52 : 60.0 % done\n",
      "Current loss: 58159.44058531481\n",
      "Epoch 52 : 70.0 % done\n",
      "Current loss: 67900.53594071385\n",
      "Epoch 52 : 80.0 % done\n",
      "Current loss: 77690.82702835152\n",
      "Epoch 52 : 90.0 % done\n",
      "Current loss: 87468.21525908427\n",
      "Epoch 52 : 100.0 % done\n",
      "Current loss: 97069.0536902171\n",
      "\n",
      "Epoch 52 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(8.7036, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(8.6914, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8811, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-7.8479, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.9634, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(124.1136, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(6.2799, device='cuda:0')\n",
      "Epoch 53 : 0.0 % done\n",
      "Current loss: 3.1022462844848633\n",
      "Epoch 53 : 10.0 % done\n",
      "Current loss: 9721.964610234721\n",
      "Epoch 53 : 20.0 % done\n",
      "Current loss: 19455.714472099873\n",
      "Epoch 53 : 30.0 % done\n",
      "Current loss: 29238.49182264723\n",
      "Epoch 53 : 40.0 % done\n",
      "Current loss: 38910.261345830564\n",
      "Epoch 53 : 50.0 % done\n",
      "Current loss: 48649.031241393874\n",
      "Epoch 53 : 60.0 % done\n",
      "Current loss: 58268.20432290823\n",
      "Epoch 53 : 70.0 % done\n",
      "Current loss: 67968.60913320976\n",
      "Epoch 53 : 80.0 % done\n",
      "Current loss: 77610.46235930602\n",
      "Epoch 53 : 90.0 % done\n",
      "Current loss: 87240.41049382408\n",
      "Epoch 53 : 100.0 % done\n",
      "Current loss: 96949.1334830007\n",
      "\n",
      "Epoch 53 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(8.6357, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(8.7398, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8818, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-7.7944, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.9975, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(125.8536, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(6.4821, device='cuda:0')\n",
      "Epoch 54 : 0.0 % done\n",
      "Current loss: 3.4403394063313804\n",
      "Epoch 54 : 10.0 % done\n",
      "Current loss: 9666.501543564305\n",
      "Epoch 54 : 20.0 % done\n",
      "Current loss: 19430.833684436726\n",
      "Epoch 54 : 30.0 % done\n",
      "Current loss: 29051.557911945623\n",
      "Epoch 54 : 40.0 % done\n",
      "Current loss: 38712.253772824755\n",
      "Epoch 54 : 50.0 % done\n",
      "Current loss: 48383.81560797398\n",
      "Epoch 54 : 60.0 % done\n",
      "Current loss: 58011.15332515385\n",
      "Epoch 54 : 70.0 % done\n",
      "Current loss: 67705.05377929045\n",
      "Epoch 54 : 80.0 % done\n",
      "Current loss: 77529.37678992203\n",
      "Epoch 54 : 90.0 % done\n",
      "Current loss: 87183.2067547888\n",
      "Epoch 54 : 100.0 % done\n",
      "Current loss: 96835.70645319083\n",
      "\n",
      "Epoch 54 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(8.5659, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(8.7864, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8828, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-7.7437, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(4.0301, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(127.5917, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(6.6833, device='cuda:0')\n",
      "Epoch 55 : 0.0 % done\n",
      "Current loss: 6.0657548904418945\n",
      "Epoch 55 : 10.0 % done\n",
      "Current loss: 9555.380697633043\n",
      "Epoch 55 : 20.0 % done\n",
      "Current loss: 19243.259551066534\n",
      "Epoch 55 : 30.0 % done\n",
      "Current loss: 28913.282500834484\n",
      "Epoch 55 : 40.0 % done\n",
      "Current loss: 38627.48646001659\n",
      "Epoch 55 : 50.0 % done\n",
      "Current loss: 48294.50815113554\n",
      "Epoch 55 : 60.0 % done\n",
      "Current loss: 58086.61557308801\n",
      "Epoch 55 : 70.0 % done\n",
      "Current loss: 67660.78684968258\n",
      "Epoch 55 : 80.0 % done\n",
      "Current loss: 77313.76004974335\n",
      "Epoch 55 : 90.0 % done\n",
      "Current loss: 87034.19555101977\n",
      "Epoch 55 : 100.0 % done\n",
      "Current loss: 96737.02083746316\n",
      "\n",
      "Epoch 55 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(8.4949, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(8.8322, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8835, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-7.6937, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(4.0618, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(129.3300, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(6.8835, device='cuda:0')\n",
      "Epoch 56 : 0.0 % done\n",
      "Current loss: 0.7446707089742025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56 : 10.0 % done\n",
      "Current loss: 9628.952226901125\n",
      "Epoch 56 : 20.0 % done\n",
      "Current loss: 19317.222632047055\n",
      "Epoch 56 : 30.0 % done\n",
      "Current loss: 28875.21795204532\n",
      "Epoch 56 : 40.0 % done\n",
      "Current loss: 38621.163895386446\n",
      "Epoch 56 : 50.0 % done\n",
      "Current loss: 48293.821551196124\n",
      "Epoch 56 : 60.0 % done\n",
      "Current loss: 58066.45426771823\n",
      "Epoch 56 : 70.0 % done\n",
      "Current loss: 67710.18284539651\n",
      "Epoch 56 : 80.0 % done\n",
      "Current loss: 77299.56836222831\n",
      "Epoch 56 : 90.0 % done\n",
      "Current loss: 86900.13507563717\n",
      "Epoch 56 : 100.0 % done\n",
      "Current loss: 96620.65056345562\n",
      "\n",
      "Epoch 56 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(8.4228, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(8.8779, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8844, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-7.6459, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(4.0924, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(131.0668, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(7.0826, device='cuda:0')\n",
      "Epoch 57 : 0.0 % done\n",
      "Current loss: 3.349565267562866\n",
      "Epoch 57 : 10.0 % done\n",
      "Current loss: 9569.706300846417\n",
      "Epoch 57 : 20.0 % done\n",
      "Current loss: 19173.243168380002\n",
      "Epoch 57 : 30.0 % done\n",
      "Current loss: 28844.70582366135\n",
      "Epoch 57 : 40.0 % done\n",
      "Current loss: 38426.63215422745\n",
      "Epoch 57 : 50.0 % done\n",
      "Current loss: 48044.59457617137\n",
      "Epoch 57 : 60.0 % done\n",
      "Current loss: 57591.534369329805\n",
      "Epoch 57 : 70.0 % done\n",
      "Current loss: 67249.3314448017\n",
      "Epoch 57 : 80.0 % done\n",
      "Current loss: 77054.45789407288\n",
      "Epoch 57 : 90.0 % done\n",
      "Current loss: 86794.71909550745\n",
      "Epoch 57 : 100.0 % done\n",
      "Current loss: 96514.39085850505\n",
      "\n",
      "Epoch 57 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(8.3498, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(8.9242, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8853, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-7.5993, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(4.1222, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(132.8035, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(7.2811, device='cuda:0')\n",
      "Epoch 58 : 0.0 % done\n",
      "Current loss: 6.403554280598958\n",
      "Epoch 58 : 10.0 % done\n",
      "Current loss: 9655.612151061916\n",
      "Epoch 58 : 20.0 % done\n",
      "Current loss: 19330.22779902266\n",
      "Epoch 58 : 30.0 % done\n",
      "Current loss: 29071.69034088189\n",
      "Epoch 58 : 40.0 % done\n",
      "Current loss: 38596.98189160552\n",
      "Epoch 58 : 50.0 % done\n",
      "Current loss: 48189.78218877388\n",
      "Epoch 58 : 60.0 % done\n",
      "Current loss: 57909.56756969296\n",
      "Epoch 58 : 70.0 % done\n",
      "Current loss: 67522.05377781796\n",
      "Epoch 58 : 80.0 % done\n",
      "Current loss: 77295.38234018142\n",
      "Epoch 58 : 90.0 % done\n",
      "Current loss: 87003.94002603253\n",
      "Epoch 58 : 100.0 % done\n",
      "Current loss: 96411.24166934231\n",
      "\n",
      "Epoch 58 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(8.2763, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(8.9710, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8860, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-7.5539, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(4.1512, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(134.5381, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(7.4788, device='cuda:0')\n",
      "Epoch 59 : 0.0 % done\n",
      "Current loss: 2.161997159322103\n",
      "Epoch 59 : 10.0 % done\n",
      "Current loss: 9637.192634727879\n",
      "Epoch 59 : 20.0 % done\n",
      "Current loss: 19438.797859172624\n",
      "Epoch 59 : 30.0 % done\n",
      "Current loss: 29029.67124316075\n",
      "Epoch 59 : 40.0 % done\n",
      "Current loss: 38746.503666748795\n",
      "Epoch 59 : 50.0 % done\n",
      "Current loss: 48470.50996733673\n",
      "Epoch 59 : 60.0 % done\n",
      "Current loss: 58038.18970526531\n",
      "Epoch 59 : 70.0 % done\n",
      "Current loss: 67707.11676408365\n",
      "Epoch 59 : 80.0 % done\n",
      "Current loss: 77258.83613049511\n",
      "Epoch 59 : 90.0 % done\n",
      "Current loss: 86760.67245817685\n",
      "Epoch 59 : 100.0 % done\n",
      "Current loss: 96326.92685937104\n",
      "\n",
      "Epoch 59 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(8.2027, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(9.0192, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(1194.8867, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-7.5089, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(4.1795, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(136.2721, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(7.6760, device='cuda:0')\n",
      "Converged\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 100\n",
    "hidden_size = 150\n",
    "encoder = Encoder(embedding_size, hidden_size, ENG).to(device)\n",
    "decoder = Decoder(embedding_size, hidden_size, target_vocab_size).to(device)\n",
    "train_model(X_train, Y_train, encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), 'encoder_weights.pt')\n",
    "torch.save(decoder.state_dict(), 'decoder_weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loss(sources, targets, enc_obj, dec_obj):\n",
    "    \n",
    "    loss_fn = nn.NLLLoss()\n",
    "    num_sentences = len(sources)\n",
    "    total_loss = 0\n",
    "        \n",
    "    for i in range(num_sentences):\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            source_sentence = torch.cuda.LongTensor(sources[i])\n",
    "            target_sentence = torch.cuda.LongTensor(targets[i])\n",
    "            Ts = source_sentence.size(-1)\n",
    "            Tt = target_sentence.size(-1)\n",
    "            enc_hidden = (torch.zeros(1, 1, enc_obj.hidden_size, device=device), torch.zeros(1, 1, enc_obj.hidden_size, device=device))\n",
    "            \n",
    "            loss_val = 0\n",
    "            \n",
    "            for i in range(Ts):\n",
    "                enc_output, enc_hidden = enc_obj(source_sentence[i], enc_hidden)\n",
    "\n",
    "            dec_input = torch.tensor([[2]], device=device)  #SOS token\n",
    "\n",
    "            # first hidden state of decoder is made the final hidden state of the encoder and cell state is initialised with zeros\n",
    "            dec_hidden = (enc_hidden[0], torch.zeros(1, 1, dec_obj.hidden_size, device=device))\n",
    "\n",
    "            for i in range(60):\n",
    "\n",
    "                dec_output, dec_hidden = dec_obj(dec_input, dec_hidden)\n",
    "                _ , index = dec_output.topk(1)\n",
    "                dec_input = index.squeeze().detach()\n",
    "                if index.item() == 0:\n",
    "                    break\n",
    "\n",
    "                target_word = torch.cuda.LongTensor([target_sentence[i].item()])\n",
    "\n",
    "                loss_val += loss_fn(dec_output, target_word)\n",
    "            \n",
    "            total_loss += loss_val.item()/Tt\n",
    "    \n",
    "    return total_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262.6952312039345"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss(X_test, Y_test, encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu(enc_obj, dec_obj, source_sentence, target, target_vocab_dict):\n",
    "    \n",
    "    # function to return the BLEU score for a single sentence \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        Ts = source_sentence.size(-1)\n",
    "        enc_hidden = (torch.zeros(1, 1, enc_obj.hidden_size, device=device), torch.zeros(1, 1, enc_obj.hidden_size, device=device))\n",
    "\n",
    "        for i in range(Ts):\n",
    "            enc_output, enc_hidden = enc_obj(source_sentence[i], enc_hidden)\n",
    "\n",
    "        dec_input = torch.tensor([[2]], device=device)  # SOS\n",
    "\n",
    "        dec_hidden = (enc_hidden[0], torch.zeros(1, 1, dec_obj.hidden_size, device=device))\n",
    "\n",
    "        predicted = []\n",
    "\n",
    "        for i in range(60):\n",
    "            dec_output, dec_hidden = dec_obj(dec_input, dec_hidden)\n",
    "            _ , index = dec_output.data.topk(1)\n",
    "            if index.item() == 0:\n",
    "                #decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                predicted.append(target_vocab_dict[index.item()])\n",
    "\n",
    "            dec_input = index.squeeze().detach()\n",
    "\n",
    "    print(predicted)\n",
    "    print(target)\n",
    "    return bleu_score([predicted], [[target]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(enc_obj, dec_obj, source_test, target_test, target_vocab_dict):\n",
    "    \n",
    "    # returns the average bleu score for the model with the given test data\n",
    "    \n",
    "    total_bleu = 0\n",
    "    for i in range(len(source_test)):\n",
    "        source_sentence = torch.cuda.LongTensor(source_test[i])\n",
    "        target = [target_vocab_dict[x] for x in target_test[i][:-1]]\n",
    "        bleu = eval_bleu(enc_obj, dec_obj, source_sentence, target, target_vocab_dict)\n",
    "        total_bleu += bleu\n",
    "    \n",
    "    return total_bleu/len(source_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['']\n",
      "['', '', '', '', '', '', 'die', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '8', '00', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', 'stancroft', '', '', 'fucking', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '2', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['stan', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '60', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '10', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '15', '1989', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', \"'fuckin'\", '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '4', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', 'greely', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '1', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['um', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', 'tremo', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', \"'\", '', '', '', 'prayin', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', 'peoc', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', 'fucking', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', \"'\", '', '', '', '', '', '110', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', 'rom']\n",
      "['']\n",
      "['', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '11', '', '35', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', 'plan', 'a', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '100', '', '', '', '', '']\n",
      "['']\n",
      "['morel', '', 'greely', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', 'fucking', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', 'fuckin', \"'\", '', '', '', '', '', 'motherfucking', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', 'banir', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', 'fucking', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', 'stancroft', '', '', 'fucking', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', 'aka', '', '', '', '', '', '', '26', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', \"'\", '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '10', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', 'banir', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', \"'\", '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['saru', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '51', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '5', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '1948', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', 'donald', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', 'c', '4', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '7', '7', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', 'hlb', '', '', '', 'dr', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', 'greely', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', 'put', '', '', '', '', '', '', '', 'put', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '60', '000', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '25', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '60', '000', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', 'lt', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', \"'\", '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '4', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', \"'\", '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', \"'\"]\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', 'zod', '', '', '', '', '', '']\n",
      "['']\n",
      "['man', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', 'yabbering']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '50', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '25', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', 'fuckin', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '9', '2', '5', '', '', '', '', '', '8', '5', '6']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', 'fucker', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', 'motherfucking', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '90', '000', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', 'fellas', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '9', '00', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', 'peoc', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '13', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', 'outta']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '30', '000', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', \"there'sa\", '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['beech', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', 'dicks', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['cooper', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', 'morel', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '100', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['okay', '', '', '', '', 'wormhole', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '10', '', '', '', '', '', '', 'ls', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', 'tet', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '7', '00', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '15', '', '10', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '60', '000', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "[\"there'sa\", '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', 'homie', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', 'fucking', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', '', '', '', '', '', '', '']\n",
      "['']\n",
      "['', 'zod', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(encoder, decoder, X_test, Y_test, TAM.index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   .'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tamil_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's where we're going.\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 63, 359, 7, 0]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['that', \"'s\", 'where', 'we', \"'re\", 'going', '.']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_eng_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 15, 11, 102, 18, 32, 83, 4, 3]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
