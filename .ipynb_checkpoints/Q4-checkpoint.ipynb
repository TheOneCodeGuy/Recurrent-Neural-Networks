{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "id": "RSXTTHE1Tk7T",
    "outputId": "ade7052b-2020-4db3-fbdb-8d70af88da44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext==0.6\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/17/e7c588245aece7aa93f360894179374830daf60d7ed0bbb59332de3b3b61/torchtext-0.6.0-py3-none-any.whl (64kB)\n",
      "\r",
      "\u001b[K     |█████                           | 10kB 17.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▏                     | 20kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▎                | 30kB 4.6MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▍           | 40kB 3.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▌      | 51kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▋ | 61kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 71kB 3.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6) (1.18.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6) (1.15.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6) (1.5.1+cu101)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6) (4.41.1)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 10.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6) (2.23.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6) (0.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6) (1.24.3)\n",
      "Installing collected packages: sentencepiece, torchtext\n",
      "  Found existing installation: torchtext 0.3.1\n",
      "    Uninstalling torchtext-0.3.1:\n",
      "      Successfully uninstalled torchtext-0.3.1\n",
      "Successfully installed sentencepiece-0.1.91 torchtext-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext==0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oHgbMlRPTfxS"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "998uVqQvTfxX",
    "outputId": "3e785e48-fa8f-454c-d312-e57d79184fe4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torchtext.vocab import GloVe\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from torchtext.data.metrics import bleu_score\n",
    "import spacy\n",
    "import pickle\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "J41zovX7Tfxa",
    "outputId": "3ba40de7-7131-499a-899b-66b3c7158b4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "chHVSOtaTfxd"
   },
   "outputs": [],
   "source": [
    "#Reading files\n",
    "\n",
    "with open('/content/drive/My Drive/Assignment4_2/trainen.txt', encoding='utf8') as f:\n",
    "    eng_train = list(map(lambda x: x.rstrip(), f.readlines()))\n",
    "    \n",
    "with open('/content/drive/My Drive/Assignment4_2/trainta.txt', encoding='utf8') as f:\n",
    "    tamil_train = list(map(lambda x: x.rstrip(), f.readlines()))\n",
    "    \n",
    "with open('/content/drive/My Drive/Assignment4_2/deven.txt', encoding='utf8') as f:\n",
    "    eng_test = list(map(lambda x: x.rstrip(), f.readlines()))\n",
    "    \n",
    "with open('/content/drive/My Drive/Assignment4_2/devta.txt', encoding='utf8') as f:\n",
    "    tamil_test = list(map(lambda x: x.rstrip(), f.readlines()))\n",
    "    \n",
    "# with open('trainen.txt', encoding='utf8') as f:\n",
    "#     eng_train = list(map(lambda x: x.rstrip(), f.readlines()))\n",
    "    \n",
    "# with open('trainta.txt', encoding='utf8') as f:\n",
    "#     tamil_train = list(map(lambda x: x.rstrip(), f.readlines()))\n",
    "    \n",
    "# with open('deven.txt', encoding='utf8') as f:\n",
    "#     eng_test = list(map(lambda x: x.rstrip(), f.readlines()))\n",
    "    \n",
    "# with open('devta.txt', encoding='utf8') as f:\n",
    "#     tamil_test = list(map(lambda x: x.rstrip(), f.readlines()))\n",
    "    \n",
    "# embedding_glove = GloVe(name='6B', dim=100)\n",
    "embedding_glove = pickle.load(open('/content/drive/My Drive/Assignment4_2/glove.sav', 'rb'))\n",
    "\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "stop_words = [',','.','?','!',')','(',':',']','[','$','#','&','%','--']\n",
    "ENG = Field(tokenize = tokenize_en, init_token='sos', eos_token = 'eos', lower=True, stop_words=stop_words)\n",
    "processed_eng_train = list(map(lambda x: ENG.preprocess(x), eng_train))\n",
    "processed_eng_test = list(map(lambda x: ENG.preprocess(x), eng_test))\n",
    "\n",
    "ENG.build_vocab(processed_eng_train, vectors=embedding_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PMxDFuY3Tfxg"
   },
   "outputs": [],
   "source": [
    "def preprocess(processed_eng):\n",
    "    \n",
    "    #function to return the numericalized version of the tokenized sentences\n",
    "    X = []\n",
    "    for tokenized_sentence in processed_eng:\n",
    "        int_sequence = [2]  #first element is the SOS token \n",
    "        for token in tokenized_sentence:\n",
    "            int_sequence.append(ENG.vocab.stoi[token])\n",
    "        int_sequence.append(3) #last element is the EOS token\n",
    "        X.append(int_sequence)\n",
    "    \n",
    "    return X\n",
    "\n",
    "# X_train and X_test are lists of lists with the integer sequences for a given sentence\n",
    "X_train = preprocess(processed_eng_train)\n",
    "X_test = preprocess(processed_eng_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZetFBlWzTfxk",
    "outputId": "fc234ec1-c761-4001-e097-0879465c198c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Same thing for Tamil sentences\n",
    "TAM = Tokenizer()\n",
    "TAM.fit_on_texts(tamil_train)\n",
    "Y_train = TAM.texts_to_sequences(tamil_train)\n",
    "Y_test = TAM.texts_to_sequences(tamil_test)\n",
    "\n",
    "#adding EOS token\n",
    "_ = [y.append(0) for y in Y_train]\n",
    "_ = [y.append(0) for y in Y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "n1dRIuVKTfxn",
    "outputId": "b653fec1-bec3-4a9b-ae8a-d65f8e44bbc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9737\n",
      "18669\n"
     ]
    }
   ],
   "source": [
    "source_vocab_size = len(ENG.vocab)\n",
    "target_vocab_size = len(TAM.word_index)+2\n",
    "print(source_vocab_size)\n",
    "print(target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "feEUPvU_Tfxq"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_size, hidden_size, ENG):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed = nn.Embedding.from_pretrained(ENG.vocab.vectors)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size)\n",
    "        \n",
    "    def forward(self, x, enc_hidden):\n",
    "        \n",
    "        embedded_x = self.embed(x)  # Ts x 100\n",
    "        enc_output, enc_hidden = self.lstm(embedded_x.unsqueeze(1), enc_hidden)\n",
    "        return enc_output, enc_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "xXCGtxMzXq8h",
    "outputId": "31070487-78e1-4ac5-c298-7fbaec6a503f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6880, 0.5860, 0.7212, 0.2278],\n",
      "        [0.5532, 0.4187, 0.2648, 0.4213]])\n",
      "tensor([[[0.8132, 0.7761, 0.3467, 0.0980]]])\n"
     ]
    }
   ],
   "source": [
    "h1 = torch.rand(2, 4)\n",
    "h2 = torch.rand(1, 1, 4)\n",
    "\n",
    "print(h1)\n",
    "print(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TKz8J0UcYJXU",
    "outputId": "6f624dbd-e23d-431e-e239-8ac2b50900de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(h1, h2.squeeze().view(-1, 1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fq-fvnUETfxt"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-784996fcc74b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mAttnDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAttnDecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class AttnDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_size, hidden_size, n_classes):\n",
    "        \n",
    "        super(AttnDecoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed = nn.Embedding(n_classes, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size + hidden_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, n_classes)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden, enc_hidden_states):\n",
    "        \n",
    "        # hidden is a tuple with each element of size 1,1,self.hidden_size\n",
    "        # enc_hidden_states is of size Ts, self.hidden_size (Ts is the source length)\n",
    "        embedded = self.embed(input).view(1, 1, -1)   # size is 1,1,self.embed_size\n",
    "        \n",
    "        #attn_scores will be of shape Ts\n",
    "        attn_scores = torch.mm(enc_hidden_states, hidden[0].squeeze().view(-1,1)).squeeze()\n",
    "\n",
    "        attn_weights = F.softmax(attn_scores, dim=0)\n",
    "        \n",
    "        # context will be of shape 1,1,self.hidden_size\n",
    "        context = torch.bmm(attn_weights.view(1,1,-1), enc_hidden_states.unsqueeze(0))\n",
    "        \n",
    "        # new_input will be of shape 1,self.hidden_size + self.embed_size\n",
    "        new_input = torch.cat((embedded[0], context[0]), 1)\n",
    "        \n",
    "        output, hidden = self.lstm(new_input.unsqueeze(0), hidden)\n",
    "\n",
    "        output = self.logsoftmax(self.fc(output[0]))\n",
    "        \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rzXrfs_2Tfx0"
   },
   "outputs": [],
   "source": [
    "def loss_per_pair(source_sentence, target_sentence, enc_obj, dec_obj, enc_optimiser, dec_optimiser, loss_fn):\n",
    "    \n",
    "    Ts = source_sentence.size(-1)\n",
    "    Tt = target_sentence.size(-1)\n",
    "    enc_hidden = (torch.zeros(1, 1, enc_obj.hidden_size, device=device), torch.zeros(1, 1, enc_obj.hidden_size, device=device))\n",
    "    \n",
    "    enc_optimiser.zero_grad()\n",
    "    dec_optimiser.zero_grad()\n",
    "    \n",
    "    loss_val = 0\n",
    "    \n",
    "    enc_output, enc_hidden = enc_obj(source_sentence, enc_hidden) # enc_output will be Ts x 1 x hidden_size\n",
    "        \n",
    "    dec_input = torch.tensor([[target_vocab_size-1]], device=device)  #SOS token\n",
    "    \n",
    "    dec_hidden = enc_hidden\n",
    "    \n",
    "    for i in range(Tt):\n",
    "        dec_output, dec_hidden = dec_obj(dec_input, dec_hidden, enc_output.squeeze(1))\n",
    "        _ , index = dec_output.topk(1)\n",
    "        dec_input = index.squeeze().detach()\n",
    "        \n",
    "        target_word = torch.cuda.LongTensor([target_sentence[i].item()])\n",
    "        \n",
    "        loss_val += loss_fn(dec_output, target_word)\n",
    "        \n",
    "        if dec_input.item() == 0:\n",
    "            break\n",
    "\n",
    "    loss_val.backward()\n",
    "    \n",
    "    nn.utils.clip_grad_norm_(enc_obj.parameters(), 0.5, 1)\n",
    "    nn.utils.clip_grad_norm_(dec_obj.parameters(), 0.5, 1)\n",
    "\n",
    "    enc_optimiser.step()\n",
    "    dec_optimiser.step()\n",
    "\n",
    "    return loss_val.item()/Tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "96vEY6ayTfx3"
   },
   "outputs": [],
   "source": [
    "def train_model(sources, targets, enc_obj, dec_obj):\n",
    "    \n",
    "    loss_fn = nn.NLLLoss()\n",
    "    enc_optimiser = optim.SGD(enc_obj.parameters(), lr=0.001, momentum=0.9)\n",
    "    dec_optimiser = optim.SGD(dec_obj.parameters(), lr=0.001, momentum=0.9)\n",
    "    \n",
    "    max_epochs = 200\n",
    "    old_loss = np.inf\n",
    "    indices = [i for i in range(len(sources))]\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        \n",
    "        # shuffling the input data manually at the start of every new epoch\n",
    "        np.random.shuffle(indices)\n",
    "        sources = list(np.array(sources)[indices])\n",
    "        targets = list(np.array(targets)[indices])\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        num_sentences = len(sources)\n",
    "        \n",
    "        for i in range(num_sentences):\n",
    "            source_sentence = torch.cuda.LongTensor(sources[i])\n",
    "            target_sentence = torch.cuda.LongTensor(targets[i])\n",
    "\n",
    "            loss = loss_per_pair(source_sentence, target_sentence, enc_obj, dec_obj, enc_optimiser, dec_optimiser, loss_fn)\n",
    "            running_loss += loss\n",
    "\n",
    "            if i%int(num_sentences*0.1)==0:\n",
    "                print(\"Epoch\", epoch+1, \":\", (i/int(num_sentences*0.1))*10,'% done')\n",
    "                print(\"Current loss:\", running_loss)\n",
    "                \n",
    "        print('\\nEpoch', epoch+1,\"\\n\")\n",
    "        print(\"Encoder obj lstm wts sum=\", torch.sum(enc_obj.lstm.weight_ih_l0))\n",
    "        print(\"Encoder obj lstm bias sum=\", torch.sum(enc_obj.lstm.bias_ih_l0))\n",
    "        print('Decoder obj emb wts sum=', torch.sum(dec_obj.embed.weight.data))\n",
    "        print('Decoder obj lstm wts sum=', torch.sum(dec_obj.lstm.weight_ih_l0.data))\n",
    "        print('Decoder obj lstm bias sum=', torch.sum(dec_obj.lstm.bias_ih_l0.data))\n",
    "        print('Decoder obj linear wts sum=', torch.sum(dec_obj.fc.weight.data))\n",
    "        print('Decoder obj linear bias sum=', torch.sum(dec_obj.fc.bias.data))\n",
    "        \n",
    "        if abs(running_loss-old_loss)/running_loss < 1e-4:\n",
    "            print('Converged')\n",
    "            break\n",
    "    \n",
    "        old_loss = running_loss\n",
    "\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RQ9-V70ZTfx5",
    "outputId": "f2df1ae6-f08c-4826-809d-cdfbe76d4d77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : 0.0 % done\n",
      "Current loss: 9.761347770690918\n",
      "Epoch 1 : 10.0 % done\n",
      "Current loss: 25722.622101867826\n",
      "Epoch 1 : 20.0 % done\n",
      "Current loss: 51419.551730597865\n",
      "Epoch 1 : 30.0 % done\n",
      "Current loss: 76504.73394887363\n",
      "Epoch 1 : 40.0 % done\n",
      "Current loss: 100120.44813214155\n",
      "Epoch 1 : 50.0 % done\n",
      "Current loss: 114823.96439688418\n",
      "Epoch 1 : 60.0 % done\n",
      "Current loss: 123280.46197410204\n",
      "Epoch 1 : 70.0 % done\n",
      "Current loss: 130402.98479227147\n",
      "Epoch 1 : 80.0 % done\n",
      "Current loss: 137191.065316288\n",
      "Epoch 1 : 90.0 % done\n",
      "Current loss: 143567.85100204308\n",
      "Epoch 1 : 100.0 % done\n",
      "Current loss: 150183.98504863324\n",
      "\n",
      "Epoch 1 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(0.1122, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(-1.9066, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.1962, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(12.5563, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(0.3138, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(41.6170, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(0.5181, device='cuda:0')\n",
      "Epoch 2 : 0.0 % done\n",
      "Current loss: 0.8212630748748779\n",
      "Epoch 2 : 10.0 % done\n",
      "Current loss: 6426.418796382673\n",
      "Epoch 2 : 20.0 % done\n",
      "Current loss: 15368.23938760777\n",
      "Epoch 2 : 30.0 % done\n",
      "Current loss: 29173.319071397455\n",
      "Epoch 2 : 40.0 % done\n",
      "Current loss: 43764.992727830715\n",
      "Epoch 2 : 50.0 % done\n",
      "Current loss: 57797.447752739536\n",
      "Epoch 2 : 60.0 % done\n",
      "Current loss: 70999.28252260441\n",
      "Epoch 2 : 70.0 % done\n",
      "Current loss: 83393.42639426852\n",
      "Epoch 2 : 80.0 % done\n",
      "Current loss: 95924.90169211915\n",
      "Epoch 2 : 90.0 % done\n",
      "Current loss: 108394.82923093195\n",
      "Epoch 2 : 100.0 % done\n",
      "Current loss: 120618.81094935794\n",
      "\n",
      "Epoch 2 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(0.0518, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(-1.8677, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.1964, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(12.5039, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(0.3460, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(42.2444, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(0.9017, device='cuda:0')\n",
      "Epoch 3 : 0.0 % done\n",
      "Current loss: 2.1893463134765625\n",
      "Epoch 3 : 10.0 % done\n",
      "Current loss: 12463.319967630523\n",
      "Epoch 3 : 20.0 % done\n",
      "Current loss: 25022.61187415608\n",
      "Epoch 3 : 30.0 % done\n",
      "Current loss: 37498.03353964569\n",
      "Epoch 3 : 40.0 % done\n",
      "Current loss: 50021.98265156324\n",
      "Epoch 3 : 50.0 % done\n",
      "Current loss: 62400.968544316594\n",
      "Epoch 3 : 60.0 % done\n",
      "Current loss: 74747.77439224265\n",
      "Epoch 3 : 70.0 % done\n",
      "Current loss: 86860.68079647353\n",
      "Epoch 3 : 80.0 % done\n",
      "Current loss: 99186.64927209697\n",
      "Epoch 3 : 90.0 % done\n",
      "Current loss: 111561.8110040116\n",
      "Epoch 3 : 100.0 % done\n",
      "Current loss: 123888.98707729929\n",
      "\n",
      "Epoch 3 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(0.0126, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(-1.8210, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.1952, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(12.3720, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(0.3938, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(42.8888, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(1.2767, device='cuda:0')\n",
      "Epoch 4 : 0.0 % done\n",
      "Current loss: 3.9455352783203126\n",
      "Epoch 4 : 10.0 % done\n",
      "Current loss: 12435.702331261477\n",
      "Epoch 4 : 20.0 % done\n",
      "Current loss: 24867.6556995682\n",
      "Epoch 4 : 30.0 % done\n",
      "Current loss: 37011.96367636295\n",
      "Epoch 4 : 40.0 % done\n",
      "Current loss: 49553.33533645634\n",
      "Epoch 4 : 50.0 % done\n",
      "Current loss: 61932.959003434786\n",
      "Epoch 4 : 60.0 % done\n",
      "Current loss: 74246.26434254656\n",
      "Epoch 4 : 70.0 % done\n",
      "Current loss: 86331.64983096498\n",
      "Epoch 4 : 80.0 % done\n",
      "Current loss: 98586.66260163886\n",
      "Epoch 4 : 90.0 % done\n",
      "Current loss: 110681.38184765061\n",
      "Epoch 4 : 100.0 % done\n",
      "Current loss: 123229.19269524526\n",
      "\n",
      "Epoch 4 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(-0.0235, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(-1.7450, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.1927, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(12.1586, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(0.4590, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(43.6093, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(1.6342, device='cuda:0')\n",
      "Epoch 5 : 0.0 % done\n",
      "Current loss: 4.916939735412598\n",
      "Epoch 5 : 10.0 % done\n",
      "Current loss: 12336.394564141992\n",
      "Epoch 5 : 20.0 % done\n",
      "Current loss: 24602.761854668537\n",
      "Epoch 5 : 30.0 % done\n",
      "Current loss: 36836.928556551\n",
      "Epoch 5 : 40.0 % done\n",
      "Current loss: 49145.15289935245\n",
      "Epoch 5 : 50.0 % done\n",
      "Current loss: 61487.33368194389\n",
      "Epoch 5 : 60.0 % done\n",
      "Current loss: 73920.00824296579\n",
      "Epoch 5 : 70.0 % done\n",
      "Current loss: 86177.14950853825\n",
      "Epoch 5 : 80.0 % done\n",
      "Current loss: 98286.54704386198\n",
      "Epoch 5 : 90.0 % done\n",
      "Current loss: 110412.79144362242\n",
      "Epoch 5 : 100.0 % done\n",
      "Current loss: 122518.93211807082\n",
      "\n",
      "Epoch 5 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(-0.0614, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(-1.6349, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.1894, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(11.8614, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(0.5398, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(44.4244, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(1.9696, device='cuda:0')\n",
      "Epoch 6 : 0.0 % done\n",
      "Current loss: 9.27048110961914\n",
      "Epoch 6 : 10.0 % done\n",
      "Current loss: 11993.85596416311\n",
      "Epoch 6 : 20.0 % done\n",
      "Current loss: 24214.24469148238\n",
      "Epoch 6 : 30.0 % done\n",
      "Current loss: 36566.687394711815\n",
      "Epoch 6 : 40.0 % done\n",
      "Current loss: 48878.018149598545\n",
      "Epoch 6 : 50.0 % done\n",
      "Current loss: 60960.60922093468\n",
      "Epoch 6 : 60.0 % done\n",
      "Current loss: 73259.45377500112\n",
      "Epoch 6 : 70.0 % done\n",
      "Current loss: 85366.75316939126\n",
      "Epoch 6 : 80.0 % done\n",
      "Current loss: 97269.58761502369\n",
      "Epoch 6 : 90.0 % done\n",
      "Current loss: 109588.34242963811\n",
      "Epoch 6 : 100.0 % done\n",
      "Current loss: 121751.54617615274\n",
      "\n",
      "Epoch 6 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(-0.1035, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(-1.4872, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.1852, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(11.4829, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(0.6341, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(45.3235, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(2.2805, device='cuda:0')\n",
      "Epoch 7 : 0.0 % done\n",
      "Current loss: 3.9424957275390624\n",
      "Epoch 7 : 10.0 % done\n",
      "Current loss: 12236.73146979967\n",
      "Epoch 7 : 20.0 % done\n",
      "Current loss: 24133.892189775535\n",
      "Epoch 7 : 30.0 % done\n",
      "Current loss: 36247.02978969657\n",
      "Epoch 7 : 40.0 % done\n",
      "Current loss: 48477.329754000486\n",
      "Epoch 7 : 50.0 % done\n",
      "Current loss: 60407.18017002344\n",
      "Epoch 7 : 60.0 % done\n",
      "Current loss: 72592.46508633478\n",
      "Epoch 7 : 70.0 % done\n",
      "Current loss: 84507.24799968355\n",
      "Epoch 7 : 80.0 % done\n",
      "Current loss: 96717.87157545079\n",
      "Epoch 7 : 90.0 % done\n",
      "Current loss: 108821.14463304383\n",
      "Epoch 7 : 100.0 % done\n",
      "Current loss: 120921.62778388211\n",
      "\n",
      "Epoch 7 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(-0.1515, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(-1.2997, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.1802, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(11.0275, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(0.7402, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(46.2980, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(2.5695, device='cuda:0')\n",
      "Epoch 8 : 0.0 % done\n",
      "Current loss: 3.0860137939453125\n",
      "Epoch 8 : 10.0 % done\n",
      "Current loss: 12231.928018260229\n",
      "Epoch 8 : 20.0 % done\n",
      "Current loss: 24167.850802519373\n",
      "Epoch 8 : 30.0 % done\n",
      "Current loss: 36005.218193623325\n",
      "Epoch 8 : 40.0 % done\n",
      "Current loss: 47914.38294394292\n",
      "Epoch 8 : 50.0 % done\n",
      "Current loss: 59988.55177753061\n",
      "Epoch 8 : 60.0 % done\n",
      "Current loss: 71975.3696230533\n",
      "Epoch 8 : 70.0 % done\n",
      "Current loss: 84079.29820108655\n",
      "Epoch 8 : 80.0 % done\n",
      "Current loss: 96081.20734442105\n",
      "Epoch 8 : 90.0 % done\n",
      "Current loss: 108058.13743235661\n",
      "Epoch 8 : 100.0 % done\n",
      "Current loss: 120017.26249544778\n",
      "\n",
      "Epoch 8 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(-0.2056, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(-1.0722, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.1743, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(10.5001, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(0.8563, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(47.3322, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(2.8459, device='cuda:0')\n",
      "Epoch 9 : 0.0 % done\n",
      "Current loss: 4.965341567993164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 : 10.0 % done\n",
      "Current loss: 12142.30834804117\n",
      "Epoch 9 : 20.0 % done\n",
      "Current loss: 24133.47028765388\n",
      "Epoch 9 : 30.0 % done\n",
      "Current loss: 35996.588912490406\n",
      "Epoch 9 : 40.0 % done\n",
      "Current loss: 47747.265418903255\n",
      "Epoch 9 : 50.0 % done\n",
      "Current loss: 59557.925468834364\n",
      "Epoch 9 : 60.0 % done\n",
      "Current loss: 71594.53875926104\n",
      "Epoch 9 : 70.0 % done\n",
      "Current loss: 83378.30471483692\n",
      "Epoch 9 : 80.0 % done\n",
      "Current loss: 95175.98767341355\n",
      "Epoch 9 : 90.0 % done\n",
      "Current loss: 106954.5158094883\n",
      "Epoch 9 : 100.0 % done\n",
      "Current loss: 119049.8598388047\n",
      "\n",
      "Epoch 9 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(-0.2642, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(-0.8055, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.1676, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(9.9023, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(0.9805, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(48.4120, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(3.1208, device='cuda:0')\n",
      "Epoch 10 : 0.0 % done\n",
      "Current loss: 4.569540023803711\n",
      "Epoch 10 : 10.0 % done\n",
      "Current loss: 11920.666698785064\n",
      "Epoch 10 : 20.0 % done\n",
      "Current loss: 23755.022345441066\n",
      "Epoch 10 : 30.0 % done\n",
      "Current loss: 35588.47575694841\n",
      "Epoch 10 : 40.0 % done\n",
      "Current loss: 47464.5109277681\n",
      "Epoch 10 : 50.0 % done\n",
      "Current loss: 59046.62689303876\n",
      "Epoch 10 : 60.0 % done\n",
      "Current loss: 70997.78687542689\n",
      "Epoch 10 : 70.0 % done\n",
      "Current loss: 82889.62898801788\n",
      "Epoch 10 : 80.0 % done\n",
      "Current loss: 94771.46369875454\n",
      "Epoch 10 : 90.0 % done\n",
      "Current loss: 106506.27347699489\n",
      "Epoch 10 : 100.0 % done\n",
      "Current loss: 118008.36097317442\n",
      "\n",
      "Epoch 10 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(-0.3238, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(-0.5020, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.1601, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(9.2367, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(1.1113, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(49.5239, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(3.4002, device='cuda:0')\n",
      "Epoch 11 : 0.0 % done\n",
      "Current loss: 2.9159367879231772\n",
      "Epoch 11 : 10.0 % done\n",
      "Current loss: 11498.637359697237\n",
      "Epoch 11 : 20.0 % done\n",
      "Current loss: 23402.999496078348\n",
      "Epoch 11 : 30.0 % done\n",
      "Current loss: 35318.01422505143\n",
      "Epoch 11 : 40.0 % done\n",
      "Current loss: 47030.98991871018\n",
      "Epoch 11 : 50.0 % done\n",
      "Current loss: 58733.44949443592\n",
      "Epoch 11 : 60.0 % done\n",
      "Current loss: 70367.80001588393\n",
      "Epoch 11 : 70.0 % done\n",
      "Current loss: 82173.56068486952\n",
      "Epoch 11 : 80.0 % done\n",
      "Current loss: 93671.04833505437\n",
      "Epoch 11 : 90.0 % done\n",
      "Current loss: 105335.03295126962\n",
      "Epoch 11 : 100.0 % done\n",
      "Current loss: 116895.64125098217\n",
      "\n",
      "Epoch 11 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(-0.3804, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(-0.1646, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.1520, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(8.5093, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(1.2471, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(50.6596, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(3.6851, device='cuda:0')\n",
      "Epoch 12 : 0.0 % done\n",
      "Current loss: 4.706386566162109\n",
      "Epoch 12 : 10.0 % done\n",
      "Current loss: 11690.490385955321\n",
      "Epoch 12 : 20.0 % done\n",
      "Current loss: 23207.354638520395\n",
      "Epoch 12 : 30.0 % done\n",
      "Current loss: 34789.04996861743\n",
      "Epoch 12 : 40.0 % done\n",
      "Current loss: 46255.64973880832\n",
      "Epoch 12 : 50.0 % done\n",
      "Current loss: 57813.40061949041\n",
      "Epoch 12 : 60.0 % done\n",
      "Current loss: 69497.9273434849\n",
      "Epoch 12 : 70.0 % done\n",
      "Current loss: 81016.7768492847\n",
      "Epoch 12 : 80.0 % done\n",
      "Current loss: 92575.0574458483\n",
      "Epoch 12 : 90.0 % done\n",
      "Current loss: 104163.44596906642\n",
      "Epoch 12 : 100.0 % done\n",
      "Current loss: 115729.71108234249\n",
      "\n",
      "Epoch 12 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(-0.4299, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(0.2022, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.1435, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(7.7280, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(1.3863, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(51.8083, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(3.9717, device='cuda:0')\n",
      "Epoch 13 : 0.0 % done\n",
      "Current loss: 4.001513671875\n",
      "Epoch 13 : 10.0 % done\n",
      "Current loss: 11590.52556161111\n",
      "Epoch 13 : 20.0 % done\n",
      "Current loss: 22959.141574791473\n",
      "Epoch 13 : 30.0 % done\n",
      "Current loss: 34363.49106934718\n",
      "Epoch 13 : 40.0 % done\n",
      "Current loss: 45773.29697964133\n",
      "Epoch 13 : 50.0 % done\n",
      "Current loss: 57208.61298996595\n",
      "Epoch 13 : 60.0 % done\n",
      "Current loss: 68640.01824629425\n",
      "Epoch 13 : 70.0 % done\n",
      "Current loss: 80121.96033426368\n",
      "Epoch 13 : 80.0 % done\n",
      "Current loss: 91615.63924697191\n",
      "Epoch 13 : 90.0 % done\n",
      "Current loss: 103085.92922162527\n",
      "Epoch 13 : 100.0 % done\n",
      "Current loss: 114504.10689957088\n",
      "\n",
      "Epoch 13 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(-0.4678, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(0.5938, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.1347, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(6.9025, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(1.5275, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(52.9668, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(4.2562, device='cuda:0')\n",
      "Epoch 14 : 0.0 % done\n",
      "Current loss: 4.711423873901367\n",
      "Epoch 14 : 10.0 % done\n",
      "Current loss: 11346.790858125536\n",
      "Epoch 14 : 20.0 % done\n",
      "Current loss: 22761.204065132635\n",
      "Epoch 14 : 30.0 % done\n",
      "Current loss: 34217.907189277816\n",
      "Epoch 14 : 40.0 % done\n",
      "Current loss: 45380.4424285938\n",
      "Epoch 14 : 50.0 % done\n",
      "Current loss: 56865.561239697934\n",
      "Epoch 14 : 60.0 % done\n",
      "Current loss: 68176.36805055979\n",
      "Epoch 14 : 70.0 % done\n",
      "Current loss: 79510.68244896407\n",
      "Epoch 14 : 80.0 % done\n",
      "Current loss: 90722.09365575633\n",
      "Epoch 14 : 90.0 % done\n",
      "Current loss: 101866.92210533968\n",
      "Epoch 14 : 100.0 % done\n",
      "Current loss: 113234.14954290155\n",
      "\n",
      "Epoch 14 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(-0.4896, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(1.0036, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.1261, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(6.0454, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(1.6688, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(54.1286, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(4.5356, device='cuda:0')\n",
      "Epoch 15 : 0.0 % done\n",
      "Current loss: 2.91150016784668\n",
      "Epoch 15 : 10.0 % done\n",
      "Current loss: 11166.81909990642\n",
      "Epoch 15 : 20.0 % done\n",
      "Current loss: 22375.51056520203\n",
      "Epoch 15 : 30.0 % done\n",
      "Current loss: 33503.53191203481\n",
      "Epoch 15 : 40.0 % done\n",
      "Current loss: 44732.643053767664\n",
      "Epoch 15 : 50.0 % done\n",
      "Current loss: 55981.138221146684\n",
      "Epoch 15 : 60.0 % done\n",
      "Current loss: 67086.61948512516\n",
      "Epoch 15 : 70.0 % done\n",
      "Current loss: 78291.85979110647\n",
      "Epoch 15 : 80.0 % done\n",
      "Current loss: 89492.26356121467\n",
      "Epoch 15 : 90.0 % done\n",
      "Current loss: 100713.25095916391\n",
      "Epoch 15 : 100.0 % done\n",
      "Current loss: 111914.39243994471\n",
      "\n",
      "Epoch 15 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(-0.4914, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(1.4231, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.1176, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(5.1723, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(1.8080, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(55.2901, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(4.8084, device='cuda:0')\n",
      "Epoch 16 : 0.0 % done\n",
      "Current loss: 4.892477035522461\n",
      "Epoch 16 : 10.0 % done\n",
      "Current loss: 11095.721997322362\n",
      "Epoch 16 : 20.0 % done\n",
      "Current loss: 22045.95988856732\n",
      "Epoch 16 : 30.0 % done\n",
      "Current loss: 33177.92687857447\n",
      "Epoch 16 : 40.0 % done\n",
      "Current loss: 44299.55553046867\n",
      "Epoch 16 : 50.0 % done\n",
      "Current loss: 55363.15880387423\n",
      "Epoch 16 : 60.0 % done\n",
      "Current loss: 66380.99605165151\n",
      "Epoch 16 : 70.0 % done\n",
      "Current loss: 77456.38447491832\n",
      "Epoch 16 : 80.0 % done\n",
      "Current loss: 88520.22745897836\n",
      "Epoch 16 : 90.0 % done\n",
      "Current loss: 99672.91214004051\n",
      "Epoch 16 : 100.0 % done\n",
      "Current loss: 110624.6977093701\n",
      "\n",
      "Epoch 16 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(-0.4707, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(1.8411, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.1098, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(4.3041, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(1.9426, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(56.4492, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(5.0730, device='cuda:0')\n",
      "Epoch 17 : 0.0 % done\n",
      "Current loss: 5.277897834777832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 : 10.0 % done\n",
      "Current loss: 10935.059991337153\n",
      "Epoch 17 : 20.0 % done\n",
      "Current loss: 22029.63481463565\n",
      "Epoch 17 : 30.0 % done\n",
      "Current loss: 33127.05674032132\n",
      "Epoch 17 : 40.0 % done\n",
      "Current loss: 44073.95222317476\n",
      "Epoch 17 : 50.0 % done\n",
      "Current loss: 54945.04811559472\n",
      "Epoch 17 : 60.0 % done\n",
      "Current loss: 65858.66695008826\n",
      "Epoch 17 : 70.0 % done\n",
      "Current loss: 76726.07772954898\n",
      "Epoch 17 : 80.0 % done\n",
      "Current loss: 87619.16218864774\n",
      "Epoch 17 : 90.0 % done\n",
      "Current loss: 98576.9920263841\n",
      "Epoch 17 : 100.0 % done\n",
      "Current loss: 109373.49738540538\n",
      "\n",
      "Epoch 17 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(-0.4278, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(2.2430, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.1029, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(3.4732, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(2.0692, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(57.6017, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(5.3288, device='cuda:0')\n",
      "Epoch 18 : 0.0 % done\n",
      "Current loss: 3.55496826171875\n",
      "Epoch 18 : 10.0 % done\n",
      "Current loss: 10801.309627588571\n",
      "Epoch 18 : 20.0 % done\n",
      "Current loss: 21666.788918332313\n",
      "Epoch 18 : 30.0 % done\n",
      "Current loss: 32489.80197095571\n",
      "Epoch 18 : 40.0 % done\n",
      "Current loss: 43348.06977917562\n",
      "Epoch 18 : 50.0 % done\n",
      "Current loss: 53932.11566638699\n",
      "Epoch 18 : 60.0 % done\n",
      "Current loss: 64896.43946689583\n",
      "Epoch 18 : 70.0 % done\n",
      "Current loss: 75671.86420395515\n",
      "Epoch 18 : 80.0 % done\n",
      "Current loss: 86487.90196993784\n",
      "Epoch 18 : 90.0 % done\n",
      "Current loss: 97384.80554435748\n",
      "Epoch 18 : 100.0 % done\n",
      "Current loss: 108215.97925926403\n",
      "\n",
      "Epoch 18 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(-0.3657, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(2.6135, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0976, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(2.7114, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(2.1844, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(58.7436, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(5.5759, device='cuda:0')\n",
      "Epoch 19 : 0.0 % done\n",
      "Current loss: 4.867886543273926\n",
      "Epoch 19 : 10.0 % done\n",
      "Current loss: 10846.531804609816\n",
      "Epoch 19 : 20.0 % done\n",
      "Current loss: 21650.217451189103\n",
      "Epoch 19 : 30.0 % done\n",
      "Current loss: 32458.951767800198\n",
      "Epoch 19 : 40.0 % done\n",
      "Current loss: 43207.51758055458\n",
      "Epoch 19 : 50.0 % done\n",
      "Current loss: 53950.70318546388\n",
      "Epoch 19 : 60.0 % done\n",
      "Current loss: 64536.08276794093\n",
      "Epoch 19 : 70.0 % done\n",
      "Current loss: 75310.79896586055\n",
      "Epoch 19 : 80.0 % done\n",
      "Current loss: 85966.91889317278\n",
      "Epoch 19 : 90.0 % done\n",
      "Current loss: 96547.28176394697\n",
      "Epoch 19 : 100.0 % done\n",
      "Current loss: 107214.3189120247\n",
      "\n",
      "Epoch 19 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(-0.2885, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(2.9372, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0935, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(2.0539, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(2.2852, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(59.8705, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(5.8143, device='cuda:0')\n",
      "Epoch 20 : 0.0 % done\n",
      "Current loss: 6.671709696451823\n",
      "Epoch 20 : 10.0 % done\n",
      "Current loss: 10557.87834096298\n",
      "Epoch 20 : 20.0 % done\n",
      "Current loss: 21214.520648974605\n",
      "Epoch 20 : 30.0 % done\n",
      "Current loss: 31812.22112224404\n",
      "Epoch 20 : 40.0 % done\n",
      "Current loss: 42454.29826270009\n",
      "Epoch 20 : 50.0 % done\n",
      "Current loss: 53018.05752323884\n",
      "Epoch 20 : 60.0 % done\n",
      "Current loss: 63896.94168291742\n",
      "Epoch 20 : 70.0 % done\n",
      "Current loss: 74430.18422949153\n",
      "Epoch 20 : 80.0 % done\n",
      "Current loss: 85183.36975447471\n",
      "Epoch 20 : 90.0 % done\n",
      "Current loss: 95795.0863492626\n",
      "Epoch 20 : 100.0 % done\n",
      "Current loss: 106393.7710889232\n",
      "\n",
      "Epoch 20 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(-0.2026, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(3.2091, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0907, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(1.5098, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(2.3709, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(60.9802, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(6.0448, device='cuda:0')\n",
      "Epoch 21 : 0.0 % done\n",
      "Current loss: 2.7870559692382812\n",
      "Epoch 21 : 10.0 % done\n",
      "Current loss: 10726.125294373236\n",
      "Epoch 21 : 20.0 % done\n",
      "Current loss: 21298.88348758182\n",
      "Epoch 21 : 30.0 % done\n",
      "Current loss: 31851.519060391485\n",
      "Epoch 21 : 40.0 % done\n",
      "Current loss: 42409.60018895497\n",
      "Epoch 21 : 50.0 % done\n",
      "Current loss: 52934.41228638184\n",
      "Epoch 21 : 60.0 % done\n",
      "Current loss: 63545.89570544086\n",
      "Epoch 21 : 70.0 % done\n",
      "Current loss: 74076.59019514188\n",
      "Epoch 21 : 80.0 % done\n",
      "Current loss: 84678.32738766879\n",
      "Epoch 21 : 90.0 % done\n",
      "Current loss: 95193.2330890129\n",
      "Epoch 21 : 100.0 % done\n",
      "Current loss: 105738.88208435812\n",
      "\n",
      "Epoch 21 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(-0.1114, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(3.4339, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0887, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(1.0698, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(2.4431, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(62.0699, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(6.2683, device='cuda:0')\n",
      "Epoch 22 : 0.0 % done\n",
      "Current loss: 2.038461923599243\n",
      "Epoch 22 : 10.0 % done\n",
      "Current loss: 10593.206590989883\n",
      "Epoch 22 : 20.0 % done\n",
      "Current loss: 21158.983823849798\n",
      "Epoch 22 : 30.0 % done\n",
      "Current loss: 31729.63629752698\n",
      "Epoch 22 : 40.0 % done\n",
      "Current loss: 42288.36136538169\n",
      "Epoch 22 : 50.0 % done\n",
      "Current loss: 52761.77293959315\n",
      "Epoch 22 : 60.0 % done\n",
      "Current loss: 63237.84453961044\n",
      "Epoch 22 : 70.0 % done\n",
      "Current loss: 73620.61787976639\n",
      "Epoch 22 : 80.0 % done\n",
      "Current loss: 84207.87596745335\n",
      "Epoch 22 : 90.0 % done\n",
      "Current loss: 94714.41333638945\n",
      "Epoch 22 : 100.0 % done\n",
      "Current loss: 105209.96352575098\n",
      "\n",
      "Epoch 22 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(-0.0184, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(3.6231, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0874, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(0.7085, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(2.5048, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(63.1417, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(6.4863, device='cuda:0')\n",
      "Epoch 23 : 0.0 % done\n",
      "Current loss: 2.846142360142299\n",
      "Epoch 23 : 10.0 % done\n",
      "Current loss: 10563.279485601499\n",
      "Epoch 23 : 20.0 % done\n",
      "Current loss: 20986.934623854984\n",
      "Epoch 23 : 30.0 % done\n",
      "Current loss: 31464.99262049957\n",
      "Epoch 23 : 40.0 % done\n",
      "Current loss: 41925.702383293305\n",
      "Epoch 23 : 50.0 % done\n",
      "Current loss: 52309.1036611897\n",
      "Epoch 23 : 60.0 % done\n",
      "Current loss: 62670.23791472236\n",
      "Epoch 23 : 70.0 % done\n",
      "Current loss: 73279.71146986814\n",
      "Epoch 23 : 80.0 % done\n",
      "Current loss: 83845.12964725426\n",
      "Epoch 23 : 90.0 % done\n",
      "Current loss: 94390.68627526313\n",
      "Epoch 23 : 100.0 % done\n",
      "Current loss: 104767.38405125545\n",
      "\n",
      "Epoch 23 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(0.0745, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(3.7881, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0866, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(0.4021, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(2.5590, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(64.1975, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(6.7000, device='cuda:0')\n",
      "Epoch 24 : 0.0 % done\n",
      "Current loss: 6.868798573811849\n",
      "Epoch 24 : 10.0 % done\n",
      "Current loss: 10422.856970648483\n",
      "Epoch 24 : 20.0 % done\n",
      "Current loss: 20810.929083560695\n",
      "Epoch 24 : 30.0 % done\n",
      "Current loss: 31346.18360133287\n",
      "Epoch 24 : 40.0 % done\n",
      "Current loss: 41558.8901821425\n",
      "Epoch 24 : 50.0 % done\n",
      "Current loss: 52191.92544026542\n",
      "Epoch 24 : 60.0 % done\n",
      "Current loss: 62669.53918827194\n",
      "Epoch 24 : 70.0 % done\n",
      "Current loss: 73199.90765136766\n",
      "Epoch 24 : 80.0 % done\n",
      "Current loss: 83547.19528280728\n",
      "Epoch 24 : 90.0 % done\n",
      "Current loss: 93888.6172151582\n",
      "Epoch 24 : 100.0 % done\n",
      "Current loss: 104382.76234762395\n",
      "\n",
      "Epoch 24 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(0.1665, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(3.9363, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0860, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(0.1343, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(2.6077, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(65.2388, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(6.9101, device='cuda:0')\n",
      "Epoch 25 : 0.0 % done\n",
      "Current loss: 2.2944992065429686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 : 10.0 % done\n",
      "Current loss: 10392.798434645842\n",
      "Epoch 25 : 20.0 % done\n",
      "Current loss: 20720.68125502302\n",
      "Epoch 25 : 30.0 % done\n",
      "Current loss: 31287.80938479594\n",
      "Epoch 25 : 40.0 % done\n",
      "Current loss: 41573.5407337252\n",
      "Epoch 25 : 50.0 % done\n",
      "Current loss: 51874.72294775939\n",
      "Epoch 25 : 60.0 % done\n",
      "Current loss: 62250.09809569443\n",
      "Epoch 25 : 70.0 % done\n",
      "Current loss: 72712.62505074195\n",
      "Epoch 25 : 80.0 % done\n",
      "Current loss: 83299.57021420595\n",
      "Epoch 25 : 90.0 % done\n",
      "Current loss: 93669.35398864654\n",
      "Epoch 25 : 100.0 % done\n",
      "Current loss: 104034.88737335507\n",
      "\n",
      "Epoch 25 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(0.2564, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(4.0768, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0854, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-0.1163, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(2.6534, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(66.2671, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(7.1175, device='cuda:0')\n",
      "Epoch 26 : 0.0 % done\n",
      "Current loss: 1.2736515045166015\n",
      "Epoch 26 : 10.0 % done\n",
      "Current loss: 10287.33161295534\n",
      "Epoch 26 : 20.0 % done\n",
      "Current loss: 20704.876389646666\n",
      "Epoch 26 : 30.0 % done\n",
      "Current loss: 31088.582521213313\n",
      "Epoch 26 : 40.0 % done\n",
      "Current loss: 41431.49800671249\n",
      "Epoch 26 : 50.0 % done\n",
      "Current loss: 51765.62196813644\n",
      "Epoch 26 : 60.0 % done\n",
      "Current loss: 62195.35135181331\n",
      "Epoch 26 : 70.0 % done\n",
      "Current loss: 72572.28218379729\n",
      "Epoch 26 : 80.0 % done\n",
      "Current loss: 83049.541693021\n",
      "Epoch 26 : 90.0 % done\n",
      "Current loss: 93498.68572090124\n",
      "Epoch 26 : 100.0 % done\n",
      "Current loss: 103714.18766896265\n",
      "\n",
      "Epoch 26 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(0.3448, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(4.2127, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0849, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-0.3565, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(2.6968, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(67.2834, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(7.3225, device='cuda:0')\n",
      "Epoch 27 : 0.0 % done\n",
      "Current loss: 4.037234497070313\n",
      "Epoch 27 : 10.0 % done\n",
      "Current loss: 10473.43500023341\n",
      "Epoch 27 : 20.0 % done\n",
      "Current loss: 20913.383116392703\n",
      "Epoch 27 : 30.0 % done\n",
      "Current loss: 31116.778203260685\n",
      "Epoch 27 : 40.0 % done\n",
      "Current loss: 41430.209778225544\n",
      "Epoch 27 : 50.0 % done\n",
      "Current loss: 51832.40978865015\n",
      "Epoch 27 : 60.0 % done\n",
      "Current loss: 62072.25406793813\n",
      "Epoch 27 : 70.0 % done\n",
      "Current loss: 72534.59626157949\n",
      "Epoch 27 : 80.0 % done\n",
      "Current loss: 82895.39335955396\n",
      "Epoch 27 : 90.0 % done\n",
      "Current loss: 93086.51041510215\n",
      "Epoch 27 : 100.0 % done\n",
      "Current loss: 103404.35125325306\n",
      "\n",
      "Epoch 27 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(0.4310, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(4.3468, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0844, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-0.5917, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(2.7387, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(68.2891, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(7.5255, device='cuda:0')\n",
      "Epoch 28 : 0.0 % done\n",
      "Current loss: 6.683493296305339\n",
      "Epoch 28 : 10.0 % done\n",
      "Current loss: 10279.805826142518\n",
      "Epoch 28 : 20.0 % done\n",
      "Current loss: 20567.061314464907\n",
      "Epoch 28 : 30.0 % done\n",
      "Current loss: 30947.442062510232\n",
      "Epoch 28 : 40.0 % done\n",
      "Current loss: 41232.42917802283\n",
      "Epoch 28 : 50.0 % done\n",
      "Current loss: 51647.867408645536\n",
      "Epoch 28 : 60.0 % done\n",
      "Current loss: 61995.96561043623\n",
      "Epoch 28 : 70.0 % done\n",
      "Current loss: 72224.24454704103\n",
      "Epoch 28 : 80.0 % done\n",
      "Current loss: 82456.46222575165\n",
      "Epoch 28 : 90.0 % done\n",
      "Current loss: 92749.81320252482\n",
      "Epoch 28 : 100.0 % done\n",
      "Current loss: 103104.01285479346\n",
      "\n",
      "Epoch 28 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(0.5146, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(4.4810, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0840, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-0.8262, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(2.7796, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(69.2842, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(7.7265, device='cuda:0')\n",
      "Epoch 29 : 0.0 % done\n",
      "Current loss: 4.990804672241211\n",
      "Epoch 29 : 10.0 % done\n",
      "Current loss: 10354.839998505902\n",
      "Epoch 29 : 20.0 % done\n",
      "Current loss: 20597.576192337245\n",
      "Epoch 29 : 30.0 % done\n",
      "Current loss: 30757.184993116327\n",
      "Epoch 29 : 40.0 % done\n",
      "Current loss: 40851.69591170034\n",
      "Epoch 29 : 50.0 % done\n",
      "Current loss: 51151.30590740328\n",
      "Epoch 29 : 60.0 % done\n",
      "Current loss: 61407.53480112045\n",
      "Epoch 29 : 70.0 % done\n",
      "Current loss: 71869.5456061388\n",
      "Epoch 29 : 80.0 % done\n",
      "Current loss: 82269.12794996015\n",
      "Epoch 29 : 90.0 % done\n",
      "Current loss: 92501.3119923836\n",
      "Epoch 29 : 100.0 % done\n",
      "Current loss: 102813.11446273314\n",
      "\n",
      "Epoch 29 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(0.5965, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(4.6183, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0835, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-1.0649, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(2.8203, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(70.2691, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(7.9257, device='cuda:0')\n",
      "Epoch 30 : 0.0 % done\n",
      "Current loss: 2.221742630004883\n",
      "Epoch 30 : 10.0 % done\n",
      "Current loss: 10308.663536256743\n",
      "Epoch 30 : 20.0 % done\n",
      "Current loss: 20532.273202111573\n",
      "Epoch 30 : 30.0 % done\n",
      "Current loss: 30795.052417291205\n",
      "Epoch 30 : 40.0 % done\n",
      "Current loss: 41093.70554779454\n",
      "Epoch 30 : 50.0 % done\n",
      "Current loss: 51258.598629556895\n",
      "Epoch 30 : 60.0 % done\n",
      "Current loss: 61630.6731667606\n",
      "Epoch 30 : 70.0 % done\n",
      "Current loss: 71832.11169191416\n",
      "Epoch 30 : 80.0 % done\n",
      "Current loss: 82019.04931828231\n",
      "Epoch 30 : 90.0 % done\n",
      "Current loss: 92309.58154431202\n",
      "Epoch 30 : 100.0 % done\n",
      "Current loss: 102524.56673270564\n",
      "\n",
      "Epoch 30 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(0.6768, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(4.7573, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0831, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-1.3046, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(2.8604, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(71.2438, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(8.1230, device='cuda:0')\n",
      "Epoch 31 : 0.0 % done\n",
      "Current loss: 3.0304012298583984\n",
      "Epoch 31 : 10.0 % done\n",
      "Current loss: 10275.441848277922\n",
      "Epoch 31 : 20.0 % done\n",
      "Current loss: 20479.89184831971\n",
      "Epoch 31 : 30.0 % done\n",
      "Current loss: 30581.400238126487\n",
      "Epoch 31 : 40.0 % done\n",
      "Current loss: 40764.41005374288\n",
      "Epoch 31 : 50.0 % done\n",
      "Current loss: 51040.867643068006\n",
      "Epoch 31 : 60.0 % done\n",
      "Current loss: 61334.37850676395\n",
      "Epoch 31 : 70.0 % done\n",
      "Current loss: 71466.73732554793\n",
      "Epoch 31 : 80.0 % done\n",
      "Current loss: 81659.63609999378\n",
      "Epoch 31 : 90.0 % done\n",
      "Current loss: 91904.7242199542\n",
      "Epoch 31 : 100.0 % done\n",
      "Current loss: 102239.0108390146\n",
      "\n",
      "Epoch 31 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(0.7556, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(4.8993, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0827, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-1.5481, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(2.9004, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(72.2087, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(8.3184, device='cuda:0')\n",
      "Epoch 32 : 0.0 % done\n",
      "Current loss: 6.309471130371094\n",
      "Epoch 32 : 10.0 % done\n",
      "Current loss: 10184.457160729768\n",
      "Epoch 32 : 20.0 % done\n",
      "Current loss: 20452.51216915134\n",
      "Epoch 32 : 30.0 % done\n",
      "Current loss: 30560.998837215804\n",
      "Epoch 32 : 40.0 % done\n",
      "Current loss: 40747.944153367374\n",
      "Epoch 32 : 50.0 % done\n",
      "Current loss: 50849.58567100778\n",
      "Epoch 32 : 60.0 % done\n",
      "Current loss: 60982.43915912948\n",
      "Epoch 32 : 70.0 % done\n",
      "Current loss: 71247.07354295529\n",
      "Epoch 32 : 80.0 % done\n",
      "Current loss: 81465.58192391669\n",
      "Epoch 32 : 90.0 % done\n",
      "Current loss: 91670.77889366793\n",
      "Epoch 32 : 100.0 % done\n",
      "Current loss: 101941.58214526881\n",
      "\n",
      "Epoch 32 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(0.8334, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(5.0444, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0822, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-1.7958, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(2.9404, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(73.1651, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(8.5121, device='cuda:0')\n",
      "Epoch 33 : 0.0 % done\n",
      "Current loss: 6.419131596883138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 : 10.0 % done\n",
      "Current loss: 10118.96367260125\n",
      "Epoch 33 : 20.0 % done\n",
      "Current loss: 20298.969199589497\n",
      "Epoch 33 : 30.0 % done\n",
      "Current loss: 30432.7217712188\n",
      "Epoch 33 : 40.0 % done\n",
      "Current loss: 40678.409640711165\n",
      "Epoch 33 : 50.0 % done\n",
      "Current loss: 50929.10334422874\n",
      "Epoch 33 : 60.0 % done\n",
      "Current loss: 61060.10765547092\n",
      "Epoch 33 : 70.0 % done\n",
      "Current loss: 71370.98267678246\n",
      "Epoch 33 : 80.0 % done\n",
      "Current loss: 81350.78070839854\n",
      "Epoch 33 : 90.0 % done\n",
      "Current loss: 91527.43659107918\n",
      "Epoch 33 : 100.0 % done\n",
      "Current loss: 101666.82966476776\n",
      "\n",
      "Epoch 33 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(0.9108, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(5.1941, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0818, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-2.0522, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(2.9807, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(74.1144, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(8.7043, device='cuda:0')\n",
      "Epoch 34 : 0.0 % done\n",
      "Current loss: 1.718306268964495\n",
      "Epoch 34 : 10.0 % done\n",
      "Current loss: 10115.61962545572\n",
      "Epoch 34 : 20.0 % done\n",
      "Current loss: 20263.81412027864\n",
      "Epoch 34 : 30.0 % done\n",
      "Current loss: 30392.214064914497\n",
      "Epoch 34 : 40.0 % done\n",
      "Current loss: 40577.37008221864\n",
      "Epoch 34 : 50.0 % done\n",
      "Current loss: 50688.38470594994\n",
      "Epoch 34 : 60.0 % done\n",
      "Current loss: 60879.54986026193\n",
      "Epoch 34 : 70.0 % done\n",
      "Current loss: 71086.76278304782\n",
      "Epoch 34 : 80.0 % done\n",
      "Current loss: 81123.6417494993\n",
      "Epoch 34 : 90.0 % done\n",
      "Current loss: 91274.94052204379\n",
      "Epoch 34 : 100.0 % done\n",
      "Current loss: 101382.3000779493\n",
      "\n",
      "Epoch 34 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(0.9873, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(5.3465, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0813, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-2.3139, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.0210, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(75.0565, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(8.8952, device='cuda:0')\n",
      "Epoch 35 : 0.0 % done\n",
      "Current loss: 1.9428743362426757\n",
      "Epoch 35 : 10.0 % done\n",
      "Current loss: 10061.864281031567\n",
      "Epoch 35 : 20.0 % done\n",
      "Current loss: 20244.905466750446\n",
      "Epoch 35 : 30.0 % done\n",
      "Current loss: 30490.655628053104\n",
      "Epoch 35 : 40.0 % done\n",
      "Current loss: 40567.71381390556\n",
      "Epoch 35 : 50.0 % done\n",
      "Current loss: 50614.79736814049\n",
      "Epoch 35 : 60.0 % done\n",
      "Current loss: 60810.46956528524\n",
      "Epoch 35 : 70.0 % done\n",
      "Current loss: 70773.0019142542\n",
      "Epoch 35 : 80.0 % done\n",
      "Current loss: 80817.19334023821\n",
      "Epoch 35 : 90.0 % done\n",
      "Current loss: 90908.32060546882\n",
      "Epoch 35 : 100.0 % done\n",
      "Current loss: 101088.81650273911\n",
      "\n",
      "Epoch 35 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(1.0642, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(5.4998, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0810, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-2.5742, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.0609, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(75.9917, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(9.0845, device='cuda:0')\n",
      "Epoch 36 : 0.0 % done\n",
      "Current loss: 3.170962651570638\n",
      "Epoch 36 : 10.0 % done\n",
      "Current loss: 10130.213779530503\n",
      "Epoch 36 : 20.0 % done\n",
      "Current loss: 20145.24554899726\n",
      "Epoch 36 : 30.0 % done\n",
      "Current loss: 30234.03201296798\n",
      "Epoch 36 : 40.0 % done\n",
      "Current loss: 40212.7112241669\n",
      "Epoch 36 : 50.0 % done\n",
      "Current loss: 50255.45394324654\n",
      "Epoch 36 : 60.0 % done\n",
      "Current loss: 60294.84630439204\n",
      "Epoch 36 : 70.0 % done\n",
      "Current loss: 70405.63604053526\n",
      "Epoch 36 : 80.0 % done\n",
      "Current loss: 80640.62957020215\n",
      "Epoch 36 : 90.0 % done\n",
      "Current loss: 90733.03300769704\n",
      "Epoch 36 : 100.0 % done\n",
      "Current loss: 100821.72711870159\n",
      "\n",
      "Epoch 36 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(1.1405, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(5.6549, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0805, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-2.8394, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.1007, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(76.9199, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(9.2723, device='cuda:0')\n",
      "Epoch 37 : 0.0 % done\n",
      "Current loss: 2.2959611075265065\n",
      "Epoch 37 : 10.0 % done\n",
      "Current loss: 9952.739239354896\n",
      "Epoch 37 : 20.0 % done\n",
      "Current loss: 19868.096928433777\n",
      "Epoch 37 : 30.0 % done\n",
      "Current loss: 30041.17192721465\n",
      "Epoch 37 : 40.0 % done\n",
      "Current loss: 40166.21978062838\n",
      "Epoch 37 : 50.0 % done\n",
      "Current loss: 50163.80172778986\n",
      "Epoch 37 : 60.0 % done\n",
      "Current loss: 60145.102498303204\n",
      "Epoch 37 : 70.0 % done\n",
      "Current loss: 70302.66296320024\n",
      "Epoch 37 : 80.0 % done\n",
      "Current loss: 80444.01538547786\n",
      "Epoch 37 : 90.0 % done\n",
      "Current loss: 90426.75255881358\n",
      "Epoch 37 : 100.0 % done\n",
      "Current loss: 100556.82699586518\n",
      "\n",
      "Epoch 37 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(1.2172, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(5.8099, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0801, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-3.1048, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.1401, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(77.8413, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(9.4586, device='cuda:0')\n",
      "Epoch 38 : 0.0 % done\n",
      "Current loss: 4.412306308746338\n",
      "Epoch 38 : 10.0 % done\n",
      "Current loss: 10040.033833378699\n",
      "Epoch 38 : 20.0 % done\n",
      "Current loss: 20039.531419696214\n",
      "Epoch 38 : 30.0 % done\n",
      "Current loss: 29989.966431434\n",
      "Epoch 38 : 40.0 % done\n",
      "Current loss: 39915.43912953796\n",
      "Epoch 38 : 50.0 % done\n",
      "Current loss: 50011.46518845789\n",
      "Epoch 38 : 60.0 % done\n",
      "Current loss: 60173.18948494545\n",
      "Epoch 38 : 70.0 % done\n",
      "Current loss: 70104.47740048623\n",
      "Epoch 38 : 80.0 % done\n",
      "Current loss: 80052.93084335489\n",
      "Epoch 38 : 90.0 % done\n",
      "Current loss: 90152.25419071029\n",
      "Epoch 38 : 100.0 % done\n",
      "Current loss: 100293.54418334298\n",
      "\n",
      "Epoch 38 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(1.2941, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(5.9652, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0797, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-3.3703, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.1793, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(78.7556, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(9.6434, device='cuda:0')\n",
      "Epoch 39 : 0.0 % done\n",
      "Current loss: 4.969699859619141\n",
      "Epoch 39 : 10.0 % done\n",
      "Current loss: 10023.73945045862\n",
      "Epoch 39 : 20.0 % done\n",
      "Current loss: 19995.354409540832\n",
      "Epoch 39 : 30.0 % done\n",
      "Current loss: 29964.539422134727\n",
      "Epoch 39 : 40.0 % done\n",
      "Current loss: 39987.602373638256\n",
      "Epoch 39 : 50.0 % done\n",
      "Current loss: 50019.10555912659\n",
      "Epoch 39 : 60.0 % done\n",
      "Current loss: 60084.04179598988\n",
      "Epoch 39 : 70.0 % done\n",
      "Current loss: 69952.75539501029\n",
      "Epoch 39 : 80.0 % done\n",
      "Current loss: 79933.9792769957\n",
      "Epoch 39 : 90.0 % done\n",
      "Current loss: 89950.40399347809\n",
      "Epoch 39 : 100.0 % done\n",
      "Current loss: 100027.58240573142\n",
      "\n",
      "Epoch 39 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(1.3717, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(6.1179, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0794, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-3.6303, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.2176, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(79.6627, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(9.8266, device='cuda:0')\n",
      "Epoch 40 : 0.0 % done\n",
      "Current loss: 1.5934160672701323\n",
      "Epoch 40 : 10.0 % done\n",
      "Current loss: 10003.318139458155\n",
      "Epoch 40 : 20.0 % done\n",
      "Current loss: 19900.687202296955\n",
      "Epoch 40 : 30.0 % done\n",
      "Current loss: 29824.50306518313\n",
      "Epoch 40 : 40.0 % done\n",
      "Current loss: 39705.360534800886\n",
      "Epoch 40 : 50.0 % done\n",
      "Current loss: 49665.15643539548\n",
      "Epoch 40 : 60.0 % done\n",
      "Current loss: 59634.22973159198\n",
      "Epoch 40 : 70.0 % done\n",
      "Current loss: 69744.34825772923\n",
      "Epoch 40 : 80.0 % done\n",
      "Current loss: 79801.02151013864\n",
      "Epoch 40 : 90.0 % done\n",
      "Current loss: 89791.50227013502\n",
      "Epoch 40 : 100.0 % done\n",
      "Current loss: 99785.67281090518\n",
      "\n",
      "Epoch 40 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(1.4495, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(6.2687, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0792, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-3.8881, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.2553, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(80.5634, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(10.0083, device='cuda:0')\n",
      "Epoch 41 : 0.0 % done\n",
      "Current loss: 4.560582160949707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 : 10.0 % done\n",
      "Current loss: 10020.255986145476\n",
      "Epoch 41 : 20.0 % done\n",
      "Current loss: 20096.131479103577\n",
      "Epoch 41 : 30.0 % done\n",
      "Current loss: 29915.821240456466\n",
      "Epoch 41 : 40.0 % done\n",
      "Current loss: 39737.10901501328\n",
      "Epoch 41 : 50.0 % done\n",
      "Current loss: 49689.214455925605\n",
      "Epoch 41 : 60.0 % done\n",
      "Current loss: 59678.25657573501\n",
      "Epoch 41 : 70.0 % done\n",
      "Current loss: 69692.10418735711\n",
      "Epoch 41 : 80.0 % done\n",
      "Current loss: 79646.37814248222\n",
      "Epoch 41 : 90.0 % done\n",
      "Current loss: 89563.37193411554\n",
      "Epoch 41 : 100.0 % done\n",
      "Current loss: 99546.25603091357\n",
      "\n",
      "Epoch 41 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(1.5275, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(6.4156, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0791, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-4.1376, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.2920, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(81.4566, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(10.1884, device='cuda:0')\n",
      "Epoch 42 : 0.0 % done\n",
      "Current loss: 5.174472332000732\n",
      "Epoch 42 : 10.0 % done\n",
      "Current loss: 9981.338053501675\n",
      "Epoch 42 : 20.0 % done\n",
      "Current loss: 20108.678880016585\n",
      "Epoch 42 : 30.0 % done\n",
      "Current loss: 29908.347849693742\n",
      "Epoch 42 : 40.0 % done\n",
      "Current loss: 39822.3281064135\n",
      "Epoch 42 : 50.0 % done\n",
      "Current loss: 49850.59391120258\n",
      "Epoch 42 : 60.0 % done\n",
      "Current loss: 59672.48913126833\n",
      "Epoch 42 : 70.0 % done\n",
      "Current loss: 69416.28114586647\n",
      "Epoch 42 : 80.0 % done\n",
      "Current loss: 79361.33795219613\n",
      "Epoch 42 : 90.0 % done\n",
      "Current loss: 89332.63177358624\n",
      "Epoch 42 : 100.0 % done\n",
      "Current loss: 99314.85496105607\n",
      "\n",
      "Epoch 42 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(1.6058, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(6.5586, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0789, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-4.3805, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.3278, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(82.3445, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(10.3671, device='cuda:0')\n",
      "Epoch 43 : 0.0 % done\n",
      "Current loss: 4.9945597648620605\n",
      "Epoch 43 : 10.0 % done\n",
      "Current loss: 10090.158464326068\n",
      "Epoch 43 : 20.0 % done\n",
      "Current loss: 20037.382245173274\n",
      "Epoch 43 : 30.0 % done\n",
      "Current loss: 29865.324280592926\n",
      "Epoch 43 : 40.0 % done\n",
      "Current loss: 39627.21029318113\n",
      "Epoch 43 : 50.0 % done\n",
      "Current loss: 49503.28930228726\n",
      "Epoch 43 : 60.0 % done\n",
      "Current loss: 59379.11679596897\n",
      "Epoch 43 : 70.0 % done\n",
      "Current loss: 69312.06814164347\n",
      "Epoch 43 : 80.0 % done\n",
      "Current loss: 79160.67062785139\n",
      "Epoch 43 : 90.0 % done\n",
      "Current loss: 89122.50825735703\n",
      "Epoch 43 : 100.0 % done\n",
      "Current loss: 99104.17753089586\n",
      "\n",
      "Epoch 43 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(1.6841, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(6.6970, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0788, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-4.6156, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.3625, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(83.2248, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(10.5443, device='cuda:0')\n",
      "Epoch 44 : 0.0 % done\n",
      "Current loss: 1.7863756815592449\n",
      "Epoch 44 : 10.0 % done\n",
      "Current loss: 9860.779756543774\n",
      "Epoch 44 : 20.0 % done\n",
      "Current loss: 19831.26679016519\n",
      "Epoch 44 : 30.0 % done\n",
      "Current loss: 29784.912108067743\n",
      "Epoch 44 : 40.0 % done\n",
      "Current loss: 39858.60534234181\n",
      "Epoch 44 : 50.0 % done\n",
      "Current loss: 49723.33537911005\n",
      "Epoch 44 : 60.0 % done\n",
      "Current loss: 59620.91819663438\n",
      "Epoch 44 : 70.0 % done\n",
      "Current loss: 69469.25370405849\n",
      "Epoch 44 : 80.0 % done\n",
      "Current loss: 79210.75684582278\n",
      "Epoch 44 : 90.0 % done\n",
      "Current loss: 89102.06948531474\n",
      "Epoch 44 : 100.0 % done\n",
      "Current loss: 98896.06097306841\n",
      "\n",
      "Epoch 44 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(1.7623, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(6.8294, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0786, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-4.8398, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.3960, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(84.0980, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(10.7199, device='cuda:0')\n",
      "Epoch 45 : 0.0 % done\n",
      "Current loss: 2.0362588337489536\n",
      "Epoch 45 : 10.0 % done\n",
      "Current loss: 9864.521897794104\n",
      "Epoch 45 : 20.0 % done\n",
      "Current loss: 19580.400022252812\n",
      "Epoch 45 : 30.0 % done\n",
      "Current loss: 29657.444960232326\n",
      "Epoch 45 : 40.0 % done\n",
      "Current loss: 39506.884385908386\n",
      "Epoch 45 : 50.0 % done\n",
      "Current loss: 49475.139218980716\n",
      "Epoch 45 : 60.0 % done\n",
      "Current loss: 59366.4927042632\n",
      "Epoch 45 : 70.0 % done\n",
      "Current loss: 69092.46352897077\n",
      "Epoch 45 : 80.0 % done\n",
      "Current loss: 78963.80418135841\n",
      "Epoch 45 : 90.0 % done\n",
      "Current loss: 88939.55269094436\n",
      "Epoch 45 : 100.0 % done\n",
      "Current loss: 98689.71002329375\n",
      "\n",
      "Epoch 45 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(1.8403, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(6.9558, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0784, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-5.0533, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.4282, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(84.9640, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(10.8941, device='cuda:0')\n",
      "Epoch 46 : 0.0 % done\n",
      "Current loss: 6.754425684611003\n",
      "Epoch 46 : 10.0 % done\n",
      "Current loss: 9774.740535053745\n",
      "Epoch 46 : 20.0 % done\n",
      "Current loss: 19789.648479089243\n",
      "Epoch 46 : 30.0 % done\n",
      "Current loss: 29700.50056918143\n",
      "Epoch 46 : 40.0 % done\n",
      "Current loss: 39340.25736058995\n",
      "Epoch 46 : 50.0 % done\n",
      "Current loss: 49084.594169638534\n",
      "Epoch 46 : 60.0 % done\n",
      "Current loss: 58714.41103348354\n",
      "Epoch 46 : 70.0 % done\n",
      "Current loss: 68744.09934957356\n",
      "Epoch 46 : 80.0 % done\n",
      "Current loss: 78589.13375717122\n",
      "Epoch 46 : 90.0 % done\n",
      "Current loss: 88487.58184549939\n",
      "Epoch 46 : 100.0 % done\n",
      "Current loss: 98510.70147565767\n",
      "\n",
      "Epoch 46 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(1.9171, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(7.0755, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0782, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-5.2565, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.4592, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(85.8231, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(11.0667, device='cuda:0')\n",
      "Epoch 47 : 0.0 % done\n",
      "Current loss: 5.57768440246582\n",
      "Epoch 47 : 10.0 % done\n",
      "Current loss: 9874.355086814485\n",
      "Epoch 47 : 20.0 % done\n",
      "Current loss: 19680.224744820604\n",
      "Epoch 47 : 30.0 % done\n",
      "Current loss: 29640.80013523667\n",
      "Epoch 47 : 40.0 % done\n",
      "Current loss: 39569.50920529211\n",
      "Epoch 47 : 50.0 % done\n",
      "Current loss: 49285.28143039531\n",
      "Epoch 47 : 60.0 % done\n",
      "Current loss: 59088.05498734993\n",
      "Epoch 47 : 70.0 % done\n",
      "Current loss: 69042.70920173753\n",
      "Epoch 47 : 80.0 % done\n",
      "Current loss: 78837.3485224206\n",
      "Epoch 47 : 90.0 % done\n",
      "Current loss: 88613.3911216017\n",
      "Epoch 47 : 100.0 % done\n",
      "Current loss: 98332.95923709053\n",
      "\n",
      "Epoch 47 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(1.9931, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(7.1890, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0780, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-5.4492, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.4889, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(86.6755, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(11.2379, device='cuda:0')\n",
      "Epoch 48 : 0.0 % done\n",
      "Current loss: 2.262383143107096\n",
      "Epoch 48 : 10.0 % done\n",
      "Current loss: 9877.51730473047\n",
      "Epoch 48 : 20.0 % done\n",
      "Current loss: 19696.477299128506\n",
      "Epoch 48 : 30.0 % done\n",
      "Current loss: 29492.893600265546\n",
      "Epoch 48 : 40.0 % done\n",
      "Current loss: 39191.010865257704\n",
      "Epoch 48 : 50.0 % done\n",
      "Current loss: 48989.58768964311\n",
      "Epoch 48 : 60.0 % done\n",
      "Current loss: 58929.422203003705\n",
      "Epoch 48 : 70.0 % done\n",
      "Current loss: 68773.8816314933\n",
      "Epoch 48 : 80.0 % done\n",
      "Current loss: 78422.29606125691\n",
      "Epoch 48 : 90.0 % done\n",
      "Current loss: 88265.79533399639\n",
      "Epoch 48 : 100.0 % done\n",
      "Current loss: 98158.84404554186\n",
      "\n",
      "Epoch 48 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(2.0676, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(7.2958, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0779, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-5.6310, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.5173, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(87.5210, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(11.4078, device='cuda:0')\n",
      "Epoch 49 : 0.0 % done\n",
      "Current loss: 6.5695749918619795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 : 10.0 % done\n",
      "Current loss: 9801.248745864199\n",
      "Epoch 49 : 20.0 % done\n",
      "Current loss: 19642.74043678557\n",
      "Epoch 49 : 30.0 % done\n",
      "Current loss: 29376.211955497416\n",
      "Epoch 49 : 40.0 % done\n",
      "Current loss: 39176.82955294443\n",
      "Epoch 49 : 50.0 % done\n",
      "Current loss: 49080.86017850145\n",
      "Epoch 49 : 60.0 % done\n",
      "Current loss: 58718.12984761932\n",
      "Epoch 49 : 70.0 % done\n",
      "Current loss: 68503.33173622322\n",
      "Epoch 49 : 80.0 % done\n",
      "Current loss: 78176.02594586581\n",
      "Epoch 49 : 90.0 % done\n",
      "Current loss: 88111.64188876806\n",
      "Epoch 49 : 100.0 % done\n",
      "Current loss: 97997.75762222495\n",
      "\n",
      "Epoch 49 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(2.1411, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(7.3959, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0777, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-5.8027, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.5445, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(88.3593, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(11.5762, device='cuda:0')\n",
      "Epoch 50 : 0.0 % done\n",
      "Current loss: 2.254009928022112\n",
      "Epoch 50 : 10.0 % done\n",
      "Current loss: 9776.548446989855\n",
      "Epoch 50 : 20.0 % done\n",
      "Current loss: 19495.52694620105\n",
      "Epoch 50 : 30.0 % done\n",
      "Current loss: 29254.453547827467\n",
      "Epoch 50 : 40.0 % done\n",
      "Current loss: 39153.887845647936\n",
      "Epoch 50 : 50.0 % done\n",
      "Current loss: 48967.831529360876\n",
      "Epoch 50 : 60.0 % done\n",
      "Current loss: 58780.52097737803\n",
      "Epoch 50 : 70.0 % done\n",
      "Current loss: 68425.55793165448\n",
      "Epoch 50 : 80.0 % done\n",
      "Current loss: 78276.41204209282\n",
      "Epoch 50 : 90.0 % done\n",
      "Current loss: 88112.77280881988\n",
      "Epoch 50 : 100.0 % done\n",
      "Current loss: 97858.65205746554\n",
      "\n",
      "Epoch 50 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(2.2123, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(7.4895, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0775, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-5.9643, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.5704, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(89.1903, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(11.7434, device='cuda:0')\n",
      "Epoch 51 : 0.0 % done\n",
      "Current loss: 0.648135781288147\n",
      "Epoch 51 : 10.0 % done\n",
      "Current loss: 9804.92538027592\n",
      "Epoch 51 : 20.0 % done\n",
      "Current loss: 19517.150849599562\n",
      "Epoch 51 : 30.0 % done\n",
      "Current loss: 29244.959014562988\n",
      "Epoch 51 : 40.0 % done\n",
      "Current loss: 39062.01215408534\n",
      "Epoch 51 : 50.0 % done\n",
      "Current loss: 48835.524707172815\n",
      "Epoch 51 : 60.0 % done\n",
      "Current loss: 58634.885906112104\n",
      "Epoch 51 : 70.0 % done\n",
      "Current loss: 68435.41024101616\n",
      "Epoch 51 : 80.0 % done\n",
      "Current loss: 78030.47879478565\n",
      "Epoch 51 : 90.0 % done\n",
      "Current loss: 87818.24511076299\n",
      "Epoch 51 : 100.0 % done\n",
      "Current loss: 97710.50627568466\n",
      "\n",
      "Epoch 51 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(2.2813, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(7.5762, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0772, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-6.1148, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.5950, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(90.0144, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(11.9091, device='cuda:0')\n",
      "Epoch 52 : 0.0 % done\n",
      "Current loss: 4.264013290405273\n",
      "Epoch 52 : 10.0 % done\n",
      "Current loss: 9791.472007667506\n",
      "Epoch 52 : 20.0 % done\n",
      "Current loss: 19440.685367159833\n",
      "Epoch 52 : 30.0 % done\n",
      "Current loss: 29224.802637861692\n",
      "Epoch 52 : 40.0 % done\n",
      "Current loss: 39130.18205155218\n",
      "Epoch 52 : 50.0 % done\n",
      "Current loss: 48752.66844309401\n",
      "Epoch 52 : 60.0 % done\n",
      "Current loss: 58488.17067887038\n",
      "Epoch 52 : 70.0 % done\n",
      "Current loss: 68286.44193819242\n",
      "Epoch 52 : 80.0 % done\n",
      "Current loss: 78057.11501534407\n",
      "Epoch 52 : 90.0 % done\n",
      "Current loss: 87906.64475333964\n",
      "Epoch 52 : 100.0 % done\n",
      "Current loss: 97582.68702064689\n",
      "\n",
      "Epoch 52 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(2.3482, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(7.6571, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0771, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-6.2554, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.6185, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(90.8308, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(12.0732, device='cuda:0')\n",
      "Epoch 53 : 0.0 % done\n",
      "Current loss: 4.81282377243042\n",
      "Epoch 53 : 10.0 % done\n",
      "Current loss: 9749.752181852617\n",
      "Epoch 53 : 20.0 % done\n",
      "Current loss: 19441.445212338796\n",
      "Epoch 53 : 30.0 % done\n",
      "Current loss: 29252.699188044528\n",
      "Epoch 53 : 40.0 % done\n",
      "Current loss: 38992.793632772955\n",
      "Epoch 53 : 50.0 % done\n",
      "Current loss: 48746.91065848011\n",
      "Epoch 53 : 60.0 % done\n",
      "Current loss: 58528.43419083533\n",
      "Epoch 53 : 70.0 % done\n",
      "Current loss: 68416.22202659296\n",
      "Epoch 53 : 80.0 % done\n",
      "Current loss: 77992.79031939604\n",
      "Epoch 53 : 90.0 % done\n",
      "Current loss: 87605.33193177398\n",
      "Epoch 53 : 100.0 % done\n",
      "Current loss: 97457.45301577989\n",
      "\n",
      "Epoch 53 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(2.4123, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(7.7327, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0768, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-6.3894, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.6409, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(91.6396, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(12.2361, device='cuda:0')\n",
      "Epoch 54 : 0.0 % done\n",
      "Current loss: 2.430964152018229\n",
      "Epoch 54 : 10.0 % done\n",
      "Current loss: 9782.410162750146\n",
      "Epoch 54 : 20.0 % done\n",
      "Current loss: 19493.973590876\n",
      "Epoch 54 : 30.0 % done\n",
      "Current loss: 29102.27474057137\n",
      "Epoch 54 : 40.0 % done\n",
      "Current loss: 39061.44194035413\n",
      "Epoch 54 : 50.0 % done\n",
      "Current loss: 48774.01170694229\n",
      "Epoch 54 : 60.0 % done\n",
      "Current loss: 58552.16017247625\n",
      "Epoch 54 : 70.0 % done\n",
      "Current loss: 68145.23252097813\n",
      "Epoch 54 : 80.0 % done\n",
      "Current loss: 77909.34813816696\n",
      "Epoch 54 : 90.0 % done\n",
      "Current loss: 87656.98993872893\n",
      "Epoch 54 : 100.0 % done\n",
      "Current loss: 97335.05811478158\n",
      "\n",
      "Epoch 54 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(2.4737, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(7.8026, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0766, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-6.5127, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.6621, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(92.4426, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(12.3979, device='cuda:0')\n",
      "Epoch 55 : 0.0 % done\n",
      "Current loss: 3.630634069442749\n",
      "Epoch 55 : 10.0 % done\n",
      "Current loss: 9583.442006435596\n",
      "Epoch 55 : 20.0 % done\n",
      "Current loss: 19325.140929571608\n",
      "Epoch 55 : 30.0 % done\n",
      "Current loss: 29211.034271773275\n",
      "Epoch 55 : 40.0 % done\n",
      "Current loss: 39083.84661396\n",
      "Epoch 55 : 50.0 % done\n",
      "Current loss: 48828.66368981967\n",
      "Epoch 55 : 60.0 % done\n",
      "Current loss: 58647.45713800334\n",
      "Epoch 55 : 70.0 % done\n",
      "Current loss: 68299.03132381037\n",
      "Epoch 55 : 80.0 % done\n",
      "Current loss: 77898.83925909566\n",
      "Epoch 55 : 90.0 % done\n",
      "Current loss: 87577.47721642246\n",
      "Epoch 55 : 100.0 % done\n",
      "Current loss: 97229.42687113017\n",
      "\n",
      "Epoch 55 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(2.5320, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(7.8686, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0763, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-6.6303, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.6825, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(93.2394, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(12.5585, device='cuda:0')\n",
      "Epoch 56 : 0.0 % done\n",
      "Current loss: 6.674896876017253\n",
      "Epoch 56 : 10.0 % done\n",
      "Current loss: 9677.096565698734\n",
      "Epoch 56 : 20.0 % done\n",
      "Current loss: 19505.12740897167\n",
      "Epoch 56 : 30.0 % done\n",
      "Current loss: 29357.674344950457\n",
      "Epoch 56 : 40.0 % done\n",
      "Current loss: 39050.8171678142\n",
      "Epoch 56 : 50.0 % done\n",
      "Current loss: 48922.33746457004\n",
      "Epoch 56 : 60.0 % done\n",
      "Current loss: 58574.377431272376\n",
      "Epoch 56 : 70.0 % done\n",
      "Current loss: 67988.78816764246\n",
      "Epoch 56 : 80.0 % done\n",
      "Current loss: 77672.79767042259\n",
      "Epoch 56 : 90.0 % done\n",
      "Current loss: 87426.20421507781\n",
      "Epoch 56 : 100.0 % done\n",
      "Current loss: 97105.866199554\n",
      "\n",
      "Epoch 56 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(2.5877, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(7.9315, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0761, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-6.7434, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.7023, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(94.0314, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(12.7180, device='cuda:0')\n",
      "Epoch 57 : 0.0 % done\n",
      "Current loss: 6.496211051940918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57 : 10.0 % done\n",
      "Current loss: 9733.493026733473\n",
      "Epoch 57 : 20.0 % done\n",
      "Current loss: 19485.258110865718\n",
      "Epoch 57 : 30.0 % done\n",
      "Current loss: 29149.544626443356\n",
      "Epoch 57 : 40.0 % done\n",
      "Current loss: 38853.21945679324\n",
      "Epoch 57 : 50.0 % done\n",
      "Current loss: 48515.557830558\n",
      "Epoch 57 : 60.0 % done\n",
      "Current loss: 58101.68755138644\n",
      "Epoch 57 : 70.0 % done\n",
      "Current loss: 67774.24351367801\n",
      "Epoch 57 : 80.0 % done\n",
      "Current loss: 77504.4135747647\n",
      "Epoch 57 : 90.0 % done\n",
      "Current loss: 87331.2054278973\n",
      "Epoch 57 : 100.0 % done\n",
      "Current loss: 97005.21350526497\n",
      "\n",
      "Epoch 57 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(2.6400, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(7.9909, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0758, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-6.8506, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.7212, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(94.8169, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(12.8764, device='cuda:0')\n",
      "Epoch 58 : 0.0 % done\n",
      "Current loss: 2.306362364027235\n",
      "Epoch 58 : 10.0 % done\n",
      "Current loss: 9668.558711795367\n",
      "Epoch 58 : 20.0 % done\n",
      "Current loss: 19379.199220866012\n",
      "Epoch 58 : 30.0 % done\n",
      "Current loss: 28868.263695258785\n",
      "Epoch 58 : 40.0 % done\n",
      "Current loss: 38477.948293969865\n",
      "Epoch 58 : 50.0 % done\n",
      "Current loss: 48205.504152044894\n",
      "Epoch 58 : 60.0 % done\n",
      "Current loss: 57999.67470647342\n",
      "Epoch 58 : 70.0 % done\n",
      "Current loss: 67674.04350015396\n",
      "Epoch 58 : 80.0 % done\n",
      "Current loss: 77419.15269416146\n",
      "Epoch 58 : 90.0 % done\n",
      "Current loss: 87144.5744407786\n",
      "Epoch 58 : 100.0 % done\n",
      "Current loss: 96898.43788235627\n",
      "\n",
      "Epoch 58 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(2.6894, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(8.0472, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0755, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-6.9528, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.7394, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(95.5968, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(13.0336, device='cuda:0')\n",
      "Epoch 59 : 0.0 % done\n",
      "Current loss: 2.2233670552571616\n",
      "Epoch 59 : 10.0 % done\n",
      "Current loss: 9494.563926127283\n",
      "Epoch 59 : 20.0 % done\n",
      "Current loss: 19183.987866885192\n",
      "Epoch 59 : 30.0 % done\n",
      "Current loss: 28885.798207543532\n",
      "Epoch 59 : 40.0 % done\n",
      "Current loss: 38665.39311535935\n",
      "Epoch 59 : 50.0 % done\n",
      "Current loss: 48357.71588463703\n",
      "Epoch 59 : 60.0 % done\n",
      "Current loss: 58109.541407149925\n",
      "Epoch 59 : 70.0 % done\n",
      "Current loss: 67722.24975532648\n",
      "Epoch 59 : 80.0 % done\n",
      "Current loss: 77384.77969606685\n",
      "Epoch 59 : 90.0 % done\n",
      "Current loss: 87190.49003754267\n",
      "Epoch 59 : 100.0 % done\n",
      "Current loss: 96795.42738417903\n",
      "\n",
      "Epoch 59 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(2.7353, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(8.1015, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0752, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-7.0520, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.7570, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(96.3721, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(13.1900, device='cuda:0')\n",
      "Epoch 60 : 0.0 % done\n",
      "Current loss: 3.963823699951172\n",
      "Epoch 60 : 10.0 % done\n",
      "Current loss: 9684.419365690736\n",
      "Epoch 60 : 20.0 % done\n",
      "Current loss: 19251.33254557662\n",
      "Epoch 60 : 30.0 % done\n",
      "Current loss: 29054.14077890164\n",
      "Epoch 60 : 40.0 % done\n",
      "Current loss: 38685.54153864965\n",
      "Epoch 60 : 50.0 % done\n",
      "Current loss: 48247.67321009333\n",
      "Epoch 60 : 60.0 % done\n",
      "Current loss: 58013.41197541176\n",
      "Epoch 60 : 70.0 % done\n",
      "Current loss: 67855.97690063147\n",
      "Epoch 60 : 80.0 % done\n",
      "Current loss: 77523.20812167722\n",
      "Epoch 60 : 90.0 % done\n",
      "Current loss: 87006.19798913237\n",
      "Epoch 60 : 100.0 % done\n",
      "Current loss: 96700.09729555596\n",
      "\n",
      "Epoch 60 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(2.7784, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(8.1545, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0749, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-7.1494, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.7742, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(97.1438, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(13.3456, device='cuda:0')\n",
      "Epoch 61 : 0.0 % done\n",
      "Current loss: 3.055803680419922\n",
      "Epoch 61 : 10.0 % done\n",
      "Current loss: 9766.86466561758\n",
      "Epoch 61 : 20.0 % done\n",
      "Current loss: 19511.02854230535\n",
      "Epoch 61 : 30.0 % done\n",
      "Current loss: 29162.058580446595\n",
      "Epoch 61 : 40.0 % done\n",
      "Current loss: 38786.113672939624\n",
      "Epoch 61 : 50.0 % done\n",
      "Current loss: 48409.26138305285\n",
      "Epoch 61 : 60.0 % done\n",
      "Current loss: 58037.40423188481\n",
      "Epoch 61 : 70.0 % done\n",
      "Current loss: 67577.94111352281\n",
      "Epoch 61 : 80.0 % done\n",
      "Current loss: 77160.06648787207\n",
      "Epoch 61 : 90.0 % done\n",
      "Current loss: 86859.8697214277\n",
      "Epoch 61 : 100.0 % done\n",
      "Current loss: 96595.90070296837\n",
      "\n",
      "Epoch 61 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(2.8180, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(8.2063, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0746, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-7.2437, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.7909, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(97.9119, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(13.5002, device='cuda:0')\n",
      "Epoch 62 : 0.0 % done\n",
      "Current loss: 3.6476049423217773\n",
      "Epoch 62 : 10.0 % done\n",
      "Current loss: 9461.375023968876\n",
      "Epoch 62 : 20.0 % done\n",
      "Current loss: 19159.882441804304\n",
      "Epoch 62 : 30.0 % done\n",
      "Current loss: 28805.52170575685\n",
      "Epoch 62 : 40.0 % done\n",
      "Current loss: 38501.59425387476\n",
      "Epoch 62 : 50.0 % done\n",
      "Current loss: 48223.773256848755\n",
      "Epoch 62 : 60.0 % done\n",
      "Current loss: 57879.18135018774\n",
      "Epoch 62 : 70.0 % done\n",
      "Current loss: 67427.15685495628\n",
      "Epoch 62 : 80.0 % done\n",
      "Current loss: 77081.55506402263\n",
      "Epoch 62 : 90.0 % done\n",
      "Current loss: 86824.51711112897\n",
      "Epoch 62 : 100.0 % done\n",
      "Current loss: 96498.88030408429\n",
      "\n",
      "Epoch 62 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(2.8548, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(8.2567, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0742, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-7.3359, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.8070, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(98.6758, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(13.6540, device='cuda:0')\n",
      "Epoch 63 : 0.0 % done\n",
      "Current loss: 1.8406847318013508\n",
      "Epoch 63 : 10.0 % done\n",
      "Current loss: 9498.364899795064\n",
      "Epoch 63 : 20.0 % done\n",
      "Current loss: 19050.925395440576\n",
      "Epoch 63 : 30.0 % done\n",
      "Current loss: 28709.953346754206\n",
      "Epoch 63 : 40.0 % done\n",
      "Current loss: 38397.98470855206\n",
      "Epoch 63 : 50.0 % done\n",
      "Current loss: 48118.16726819536\n",
      "Epoch 63 : 60.0 % done\n",
      "Current loss: 57736.57035220105\n",
      "Epoch 63 : 70.0 % done\n",
      "Current loss: 67361.98807609796\n",
      "Epoch 63 : 80.0 % done\n",
      "Current loss: 77045.64009032933\n",
      "Epoch 63 : 90.0 % done\n",
      "Current loss: 86560.80593962513\n",
      "Epoch 63 : 100.0 % done\n",
      "Current loss: 96396.98924411838\n",
      "\n",
      "Epoch 63 \n",
      "\n",
      "Encoder obj lstm wts sum= tensor(2.8881, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Encoder obj lstm bias sum= tensor(8.3070, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Decoder obj emb wts sum= tensor(140.0739, device='cuda:0')\n",
      "Decoder obj lstm wts sum= tensor(-7.4274, device='cuda:0')\n",
      "Decoder obj lstm bias sum= tensor(3.8229, device='cuda:0')\n",
      "Decoder obj linear wts sum= tensor(99.4372, device='cuda:0')\n",
      "Decoder obj linear bias sum= tensor(13.8072, device='cuda:0')\n",
      "Epoch 64 : 0.0 % done\n",
      "Current loss: 6.117825984954834\n",
      "Epoch 64 : 10.0 % done\n",
      "Current loss: 9708.93720465715\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-1a528bbe464b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mENG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAttnDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_vocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-93911ef4db38>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(sources, targets, enc_obj, dec_obj)\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mtarget_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_per_pair\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdec_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_optimiser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdec_optimiser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-60e1180f50e7>\u001b[0m in \u001b[0;36mloss_per_pair\u001b[1;34m(source_sentence, target_sentence, enc_obj, dec_obj, enc_optimiser, dec_optimiser, loss_fn)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mloss_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menc_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdec_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vasistha singhal\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[1;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mtotal_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1e-6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mclip_coef\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vasistha singhal\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mtotal_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1e-6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mclip_coef\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vasistha singhal\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\functional.py\u001b[0m in \u001b[0;36mnorm\u001b[1;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[0;32m    893\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrobenius_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;31m# TODO: when https://github.com/pytorch/pytorch/issues/33782 is fixed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embedding_size = 100\n",
    "hidden_size = 150\n",
    "encoder = Encoder(embedding_size, hidden_size, ENG).to(device)\n",
    "decoder = AttnDecoder(embedding_size, hidden_size, target_vocab_size).to(device)\n",
    "train_model(X_train, Y_train, encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DVhhgiRSTfx8"
   },
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), 'q4_encoder_weights.pt')\n",
    "torch.save(decoder.state_dict(), 'q4_decoder_weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6XRVOwPgTfx-"
   },
   "outputs": [],
   "source": [
    "def test_loss(sources, targets, enc_obj, dec_obj):\n",
    "    \n",
    "    loss_fn = nn.NLLLoss()\n",
    "    num_sentences = len(sources)\n",
    "    total_loss = 0\n",
    "        \n",
    "    for i in range(num_sentences):\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            source_sentence = torch.cuda.LongTensor(sources[i])\n",
    "            target_sentence = torch.cuda.LongTensor(targets[i])\n",
    "            Ts = source_sentence.size(-1)\n",
    "            Tt = target_sentence.size(-1)\n",
    "            enc_hidden = (torch.zeros(1, 1, enc_obj.hidden_size, device=device), torch.zeros(1, 1, enc_obj.hidden_size, device=device))\n",
    "            \n",
    "            loss_val = 0\n",
    "            \n",
    "            enc_output, enc_hidden = enc_obj(source_sentence[i], enc_hidden)\n",
    "\n",
    "            dec_input = torch.tensor([[target_vocab_size-1]], device=device)  #SOS token\n",
    "\n",
    "            # first hidden state of decoder is made the final hidden state of the encoder\n",
    "            dec_hidden = enc_hidden\n",
    "\n",
    "            for i in range(Tt):\n",
    "\n",
    "                dec_output, dec_hidden = dec_obj(dec_input, dec_hidden, enc_output.squeeze(1))\n",
    "                _ , index = dec_output.topk(1)\n",
    "                dec_input = index.squeeze().detach()\n",
    "                if index.item() == 0:\n",
    "                    break\n",
    "\n",
    "                target_word = torch.cuda.LongTensor([target_sentence[i].item()])\n",
    "\n",
    "                loss_val += loss_fn(dec_output, target_word)\n",
    "            \n",
    "            total_loss += loss_val.item()/Tt\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YwM9Ziy2TfyA"
   },
   "outputs": [],
   "source": [
    "# this test loss is for 500 sentences\n",
    "# test_loss(X_test, Y_test, encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kS21_J4NTfyD"
   },
   "outputs": [],
   "source": [
    "def eval_bleu(enc_obj, dec_obj, source_sentence, target, target_vocab_dict):\n",
    "    \n",
    "    # function to return the BLEU score for a single sentence \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        Ts = source_sentence.size(-1)\n",
    "        enc_hidden = (torch.zeros(1, 1, enc_obj.hidden_size, device=device), torch.zeros(1, 1, enc_obj.hidden_size, device=device))\n",
    "\n",
    "        enc_output, enc_hidden = enc_obj(source_sentence, enc_hidden)\n",
    "\n",
    "        dec_input = torch.tensor([[target_vocab_size-1]], device=device)  # SOS\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "        \n",
    "        predicted = []\n",
    "\n",
    "        for i in range(60):\n",
    "            dec_output, dec_hidden = dec_obj(dec_input, dec_hidden, enc_output.squeeze(1))\n",
    "            _ , index = dec_output.data.topk(1)\n",
    "            if index.item() == 0:\n",
    "                #decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                predicted.append(target_vocab_dict[index.item()])\n",
    "\n",
    "            dec_input = index.squeeze().detach()\n",
    "\n",
    "#     print(predicted)\n",
    "#     print(target)\n",
    "    return bleu_score([predicted], [[target]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ChbGsYo4TfyF"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(enc_obj, dec_obj, source_test, target_test, target_vocab_dict):\n",
    "    \n",
    "    # returns the average bleu score for the model with the given test data\n",
    "    \n",
    "    total_bleu = 0\n",
    "    for i in range(len(source_test)):\n",
    "        source_sentence = torch.cuda.LongTensor(source_test[i])\n",
    "        target = [target_vocab_dict[x] for x in target_test[i][:-1]]\n",
    "        bleu = eval_bleu(enc_obj, dec_obj, source_sentence, target, target_vocab_dict)\n",
    "        total_bleu += bleu\n",
    "    \n",
    "    return total_bleu/len(source_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q6FRpBhpTfyJ",
    "outputId": "eea243c5-fcc4-40c7-c42a-55851827abdb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 72,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(encoder, decoder, X_test, Y_test, TAM.index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hnMDWAZ3TfyN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Q4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
