{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run onluy the first time!\n",
    "# from pyunpack import Archive\n",
    "# Archive('Images.zip').extractall('Assignment4_Images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from io import StringIO\n",
    "from PIL import Image\n",
    "from torchtext.vocab import GloVe\n",
    "from torchtext.data import Field\n",
    "from torchtext.data.metrics import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetClass(Dataset):\n",
    "    \n",
    "    def __init__(self, folder, image_list, captions):\n",
    "        \n",
    "        self.folder = folder\n",
    "        self.captions = captions\n",
    "        self.size = 5*len(image_list)\n",
    "        self.image_list = []\n",
    "        for image in image_list:\n",
    "            all_images = [image + '#' + str(i) for i in range(5)]\n",
    "            self.image_list.extend(all_images)\n",
    "        \n",
    "    def __getitem__(self, idx):     \n",
    "        \n",
    "        image_name = self.image_list[idx]\n",
    "        caption = self.captions.loc[image_name, 'Caption']\n",
    "        img = Image.open(self.folder + image_name[:-2]).resize((227, 227))\n",
    "        trans = transforms.ToTensor()\n",
    "        return trans(img), caption\n",
    "      \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_loader(directory, image_list, captions, train_fraction=0.8, num_workers=0, batch_size=32):\n",
    "\n",
    "    dataset = DatasetClass(directory, image_list, captions)\n",
    "    \n",
    "    N = dataset.size\n",
    "    train_size = int(N*train_fraction)\n",
    "    test_size = N - train_size\n",
    "\n",
    "    train_data, test_data = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    trainloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    testloader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    \n",
    "    return trainloader, testloader, train_size, test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('captions.txt') as f:\n",
    "    captions = pd.read_csv(StringIO(f.read()), sep='\\t', header=None, names=['Image', 'Caption']) #.set_index('Image', drop=True)\n",
    "    \n",
    "with open('image_names.txt') as f:\n",
    "    names = list(map(lambda x: x.rstrip(), f.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, testloader, train_size, test_size = train_test_loader(directory='Assignment4_Data/Images/', image_list=names, captions=captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GloVe Representation\n",
    "\n",
    "Available GloVe Representations: \n",
    "\n",
    "1. glove.42B.300d \n",
    "2. glove.840B.300d \n",
    "3. glove.twitter.27B.25d \n",
    "4. glove.twitter.27B.50d \n",
    "5. glove.twitter.27B.100d \n",
    "6. glove.twitter.27B.200d \n",
    "7. glove.6B.50d \n",
    "8. glove.6B.100d \n",
    "9. glove.6B.200d \n",
    "10. glove.6B.300d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_glove = GloVe(name='6B', dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field = Field(tokenize='basic_english', lower=True, eos_token='<EOS>', init_token='<SOS>')\n",
    "preprocessed_text = captions['Caption'].apply(lambda x: text_field.preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field.build_vocab(preprocessed_text, vectors=embedding_glove)\n",
    "embedding_trained = nn.Embedding.from_pretrained(text_field.vocab.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_tokens = np.array(text_field.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions['Image'] = captions['Image'].apply(lambda x: x[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetVLAD(nn.Module):\n",
    "    \n",
    "    def __init__(self, k):\n",
    "        super(NetVLAD, self).__init__()\n",
    "        \n",
    "        # CNN\n",
    "        self.c1 = nn.Conv2d(3, 96, 11, stride=4)\n",
    "        self.mp1 = nn.MaxPool2d(3, stride=2)\n",
    "        self.c2 = nn.Conv2d(96, 256, 5)\n",
    "        self.mp2 = nn.MaxPool2d(3, stride=2)\n",
    "        self.c3 = nn.Conv2d(256, 384, 3)\n",
    "        self.c4 = nn.Conv2d(384, 384, 3)\n",
    "        self.c5 = nn.Conv2d(384, 256, 3, stride=3)\n",
    "\n",
    "        # NetVLAD\n",
    "        self.K = k\n",
    "        self.nv_conv = nn.Conv2d(256, k, 1)\n",
    "        self.nv_soft_ass = nn.Softmax2d()\n",
    "\n",
    "        # NetVLAD Parameter\n",
    "        self.c = nn.Parameter(torch.Tensor(self.K, 256))\n",
    "        \n",
    "        # Flatten to get h\n",
    "        self.flat = nn.Flatten(1, -1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # CNN\n",
    "        x = self.mp1(F.relu(self.c1(x)))\n",
    "        x = self.mp2(F.relu(self.c2(x)))\n",
    "        x = self.relu(self.c5(F.relu(self.c4(F.relu(self.c3(x))))))\n",
    "\n",
    "        # NetVLAD Step 1\n",
    "        a = self.nv_soft_ass(self.nv_conv(x))\n",
    "\n",
    "        # NetVLAD Step 2\n",
    "        for k in range(self.K):\n",
    "            a_k = a[:, k, :, :]\n",
    "            c_k = self.c[k, :]\n",
    "            temp = (x - c_k.reshape(1, -1, 1, 1))*a_k.unsqueeze(1)\n",
    "            z_k = torch.sum(temp, axis=(2, 3))\n",
    "            if k==0:\n",
    "                Z = z_k.unsqueeze(1)\n",
    "            else:\n",
    "                Z = torch.cat((Z, z_k.unsqueeze(1)), 1)\n",
    "        \n",
    "        # Flatten\n",
    "        Z = self.flatten(Z)\n",
    "\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, hidden_size, output_size, embedding_pre_trained):\n",
    "        \n",
    "        super(RNNDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Pre-trained Word Embedding of all the words is used\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_pre_trained)\n",
    "        \n",
    "        # Input to the RNN is word embeddings of a word\n",
    "        self.RNN = nn.RNN(input_size=embed_size, hidden_size=hidden_size) \n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)  \n",
    "        \n",
    "    def forward(self, input_vec, hidden_vec):\n",
    "        '''\n",
    "        Parameters:\n",
    "        ------------\n",
    "        input_vec  - tensor of index of word/token. Example: torch.LongTensor([[0]]) for sos_token\n",
    "        hidden_vec - train_image or output from previous RNN cell\n",
    "        '''\n",
    "        embedded_input_vec = self.embedding(input_vec)\n",
    "        output_vec, hidden_vec = self.RNN(embedded_input_vec, hidden_vec)\n",
    "        output_vec = self.softmax(self.out(output_vec[0]))\n",
    "        return output_vec, hidden_vec      \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, hidden_size, output_size, embedding_pre_trained):\n",
    "        \n",
    "        super(RNNDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Pre-trained Word Embedding of all the words is used\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_pre_trained)\n",
    "        \n",
    "        # Input to the RNN is word embeddings of a word\n",
    "        self.RNN = nn.LSTM(input_size=embed_size, hidden_size=hidden_size) \n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)  \n",
    "        \n",
    "    def forward(self, input_vec, hidden_vec):\n",
    "        '''\n",
    "        Parameters:\n",
    "        ------------\n",
    "        input_vec  - tensor of index of word/token. Example: torch.LongTensor([[0]]) for sos_token\n",
    "        hidden_vec - train_image or output from previous RNN cell\n",
    "        '''\n",
    "        embedded_input_vec = self.embedding(input_vec)\n",
    "        output_vec, hidden_vec = self.RNN(embedded_input_vec, hidden_vec)\n",
    "        output_vec = self.softmax(self.out(output_vec[0]))\n",
    "        return output_vec, hidden_vec      \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_image(train_image, image_caption, encoder_obj, decoder_obj, decoder_hidden_size, encoder_optim, decoder_optim, loss_func):\n",
    "    '''\n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_image     - Images stored in batches using DataLoader\n",
    "    image_caption   - Caption of the image (in the same word embedding representation used in )\n",
    "\n",
    "    \n",
    "    '''\n",
    "    # Start of sentence and End of Sentence token\n",
    "    eos_token = 1\n",
    "    sos_token = 0\n",
    "    \n",
    "    \n",
    "    # Length of the image caption\n",
    "    caption_length = image_caption.size(0)\n",
    "    \n",
    "    # Setting gradients from previous backpropagation to zero\n",
    "    encoder_optim.zero_grad()\n",
    "    decoder_optim.zero_grad()\n",
    "    \n",
    "    # TO BE CHECKED!!!!!!!!!!!!!!\n",
    "    encoder_output = encoder_obj(train_image).view(1, 1, decoder_hidden_size)    \n",
    "    decoder_input = torch.tensor([[sos_token]], device=device)\n",
    "    decoder_hidden = encoder_output\n",
    "    \n",
    "    loss = 0\n",
    "    for i in range(caption_length):\n",
    "        decoder_output, decoder_hidden = decoder_obj(decoder_input, decoder_hidden) \n",
    "        max_val, max_ind = decoder_output.topk(1)  # Choosing the word with maximum probability \n",
    "        decoder_input = max_ind \n",
    "        loss += loss_func(decoder_output, image_caption[i])\n",
    "        if decoder_input.item() == eos_token:\n",
    "            break\n",
    "    loss.backward()\n",
    "    encoder_optim.step()\n",
    "    decoder_optim.step()\n",
    "    \n",
    "    return loss.item()/caption_length\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(lr, trainloader_images, encoder_obj, decoder_obj, preprocessed_text, captions):\n",
    "    \n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    \n",
    "    encoder_optim = optim.SGD(encoder_obj.parameters(), lr=lr)\n",
    "    decoder_optim = optim.SGD(decoder_obj.parameters(), lr=lr)\n",
    "    old_loss = np.inf\n",
    "    epochs = 0\n",
    "    losses = []\n",
    "    while True:\n",
    "        new_loss = 0\n",
    "        epoch += 1\n",
    "        \n",
    "        ## CHANGE THIS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        for x in trainloader_images:\n",
    "            image_name = ___\n",
    "            train_image = ____\n",
    "            image_captions = np.append(preprocessed_text[captions['Image']==image_name].values, '<EOS>')\n",
    "            \n",
    "            for image_caption in image_captions:\n",
    "                new_loss += train_one_image(train_image, image_caption, encoder_obj, decoder_obj, decoder_hidden_size, encoder_optim, decoder_optim, loss_func)\n",
    "            \n",
    "            print('Epoch {0}: Loss = {1}'.format(epoch, new_loss))\n",
    "            losses.append(new_loss)\n",
    "            \n",
    "            if abs(new_loss-old_loss)/new_loss < 1e-5:\n",
    "                print('Converged')\n",
    "                return losses, epoch\n",
    "            \n",
    "            old_loss = new_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder_obj, decoder_obj, image, image_caption, decoder_hidden_size, vocab_tokens):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Start of sentence and End of Sentence token\n",
    "        sos_token = np.argwhere(vocab_tokens=='<SOS>')\n",
    "        eos_token = np.argwhere(vocab_tokens=='<EOS>')\n",
    "\n",
    "        # Length of the image caption\n",
    "        caption_length = image_caption.size(0)\n",
    "\n",
    "\n",
    "        # TO BE CHECKED!!!!!!!!!!!!!!\n",
    "        encoder_output = encoder_obj(image).view(1, 1, decoder_hidden_size)    \n",
    "        decoder_input = torch.tensor([[sos_token]], device=device)\n",
    "        decoder_hidden = encoder_output\n",
    "\n",
    "        output_caption = []\n",
    "        for i in range(caption_length):\n",
    "            decoder_output, decoder_hidden = decoder_obj(decoder_input, decoder_hidden) \n",
    "            max_val, max_ind = decoder_output.topk(1)  # Choosing the word with maximum probability \n",
    "            decoder_input = max_ind \n",
    "            loss += loss_func(decoder_output, image_caption[i])\n",
    "            if decoder_input.item() == eos_token:\n",
    "                output_caption.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                output_caption.append(vocab_tokens[decoder_input.item()])\n",
    "\n",
    "        return bleu_score(image_caption, output_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "decoder_hidden_size =50\n",
    "\n",
    "encoder_obj = NetVLAD(k).to(device)\n",
    "\n",
    "embed_size = embedding_trained.weight.shape[1]\n",
    "decoder_output_size = embedding_trained.weight.shape[0]\n",
    "decoder_obj = RNN(embed_size, decoder_hidden_size, decoder_output_size, embedding_trained).to(device)\n",
    "\n",
    "train(lr=0.01, trainloader, encoder_obj, decoder_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "decoder_hidden_size =50\n",
    "\n",
    "encoder_obj = NetVLAD(k).to(device)\n",
    "\n",
    "embed_size = embedding_trained.weight.shape[1]\n",
    "decoder_output_size = embedding_trained.weight.shape[0]\n",
    "decoder_obj = LSTM(embed_size, decoder_hidden_size, decoder_output_size, embedding_trained).to(device)\n",
    "\n",
    "train(lr=0.01, trainloader, encoder_obj, decoder_obj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
