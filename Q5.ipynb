{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer, Transformer\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torchtext.vocab import GloVe\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from torchtext.data.metrics import bleu_score\n",
    "import spacy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading files\n",
    "\n",
    "with open('trainen.txt', encoding='utf8') as f:\n",
    "    eng_train = list(map(lambda x: x.rstrip(), f.readlines()))\n",
    "    \n",
    "with open('trainta.txt', encoding='utf8') as f:\n",
    "    tamil_train = list(map(lambda x: x.rstrip(), f.readlines()))\n",
    "    \n",
    "with open('deven.txt', encoding='utf8') as f:\n",
    "    eng_test = list(map(lambda x: x.rstrip(), f.readlines()))\n",
    "    \n",
    "with open('devta.txt', encoding='utf8') as f:\n",
    "    tamil_test = list(map(lambda x: x.rstrip(), f.readlines()))\n",
    "    \n",
    "embedding_glove = GloVe(name='6B', dim=100)\n",
    "\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "stop_words = [',','.','?','!',')','(',':',']','[','$','#','&','%','--']\n",
    "ENG = Field(tokenize = tokenize_en, init_token='sos', eos_token = 'eos', lower=True, stop_words=stop_words)\n",
    "processed_eng_train = list(map(lambda x: ENG.preprocess(x), eng_train))\n",
    "processed_eng_test = list(map(lambda x: ENG.preprocess(x), eng_test))\n",
    "\n",
    "ENG.build_vocab(processed_eng_train, vectors=embedding_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(processed_eng):\n",
    "    \n",
    "    #function to return the numericalized version of the tokenized sentences\n",
    "    X = []\n",
    "    for tokenized_sentence in processed_eng:\n",
    "        int_sequence = [2]  #first element is the SOS token \n",
    "        for token in tokenized_sentence:\n",
    "            int_sequence.append(ENG.vocab.stoi[token])\n",
    "        int_sequence.append(3) #last element is the EOS token\n",
    "        X.append(int_sequence)\n",
    "    \n",
    "    return X\n",
    "\n",
    "# X_train and X_test are lists of lists with the integer sequences for a given sentence\n",
    "X_train = preprocess(processed_eng_train)\n",
    "X_test = preprocess(processed_eng_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Same thing for Tamil sentences\n",
    "TAM = Tokenizer()\n",
    "TAM.fit_on_texts(tamil_train)\n",
    "Y_train = TAM.texts_to_sequences(tamil_train)\n",
    "Y_test = TAM.texts_to_sequences(tamil_test)\n",
    "\n",
    "#adding EOS token\n",
    "EOS = len(TAM.word_index)+2\n",
    "SOS = len(TAM.word_index)+1\n",
    "_ = [y.append(EOS) for y in Y_train]\n",
    "_ = [y.append(EOS) for y in Y_test]\n",
    "\n",
    "Y_train = pad_sequences(Y_train, padding='post', value=0, maxlen=30)\n",
    "Y_test = pad_sequences(Y_test, padding='post', value=0, maxlen=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-padding the inputs with value 1, from the dictionary ENG.vocab.stoi\n",
    "X_train = pad_sequences(X_train, padding='pre', value=1, maxlen=55)\n",
    "X_test = pad_sequences(X_test, padding='pre', value=1, maxlen=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9723\n",
      "18671\n"
     ]
    }
   ],
   "source": [
    "source_vocab_size = len(ENG.vocab)\n",
    "target_vocab_size = len(TAM.word_index)+3\n",
    "print(source_vocab_size)\n",
    "print(target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\n",
    "\n",
    "class DatasetClass(Dataset):\n",
    "    \n",
    "    def __init__(self, source, target):\n",
    "        \n",
    "        self.source = source\n",
    "        self.target = target\n",
    "      \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.source)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.source[idx], self.target[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_loader(source_train, target_train, source_test, target_test, num_workers=0, batch_size=32):\n",
    "\n",
    "    train_data = DatasetClass(source_train, target_train)\n",
    "    test_data = DatasetClass(source_test, target_test)\n",
    "\n",
    "    trainloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    testloader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    \n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=55):\n",
    "        super(PositionEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pos_enc = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        # even columns use sin\n",
    "        pos_enc[:, 0::2] = torch.sin(position * div_term)\n",
    "        # odd columns use cos\n",
    "        pos_enc[:, 1::2] = torch.cos(position * div_term)\n",
    "        pos_enc = pos_enc.unsqueeze(0).transpose(0, 1)    # 55 x 1 x 100\n",
    "        print('pos_enc.shape', pos_enc.shape)\n",
    "        \n",
    "        # to prevent the optimiser from changing the position encodings\n",
    "        self.register_buffer('pos_enc', pos_enc)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pos_enc[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trans(nn.Module):\n",
    "    \n",
    "    def __init__(self, target_vocab_size, embed_size, num_heads, num_layers, dropout, ENG):\n",
    "        \n",
    "        super(Trans, self).__init__()\n",
    "        self.pos_encoder = PositionEncoding(embed_size, dropout)\n",
    "        self.encoder = nn.Embedding.from_pretrained(ENG.vocab.vectors)\n",
    "        self.transformer = nn.Transformer(embed_size, num_heads, num_layers, num_layers, dropout=dropout)\n",
    "        self.decoder = nn.Embedding(target_vocab_size, embed_size)\n",
    "        self.fc = nn.Linear(embed_size, target_vocab_size)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        # src and trg will be 1-D tensors with size Ts and Tt\n",
    "        src = self.encoder(src)     # batch_size x Ts x 100\n",
    "        src = self.pos_encoder(src.transpose(0,1)) # Ts x batch_size x 100\n",
    "        trg = self.decoder(trg)     # batch_size x Tt x 100\n",
    "        trg = self.pos_encoder(trg.transpose(0,1)) # Tt x batch_size x 100\n",
    "        src_mask = (torch.tril(torch.ones(src.shape[0], src.shape[0])) == 0).to(device)\n",
    "        trg_mask = (torch.tril(torch.ones(trg.shape[0], trg.shape[0])) == 0).to(device)\n",
    "        output = self.transformer(src, trg, src_mask, trg_mask) \n",
    "        output = self.fc(output)  # output will now be Tt x batch_size x target_vocab_size\n",
    "        output = self.logsoftmax(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_enc.shape torch.Size([55, 1, 100])\n"
     ]
    }
   ],
   "source": [
    "embed_size = 100\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "trans_model = Trans(target_vocab_size, embed_size, num_heads, num_layers, dropout, ENG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trans(\n",
       "  (pos_encoder): PositionEncoding(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (encoder): Embedding(9723, 100)\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=100, out_features=100, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=100, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=100, bias=True)\n",
       "          (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=100, out_features=100, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=100, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=100, bias=True)\n",
       "          (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=100, out_features=100, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=100, out_features=100, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=100, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=100, bias=True)\n",
       "          (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "          (dropout3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (1): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=100, out_features=100, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=100, out_features=100, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=100, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=100, bias=True)\n",
       "          (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "          (dropout3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Embedding(18671, 100)\n",
       "  (fc): Linear(in_features=100, out_features=18671, bias=True)\n",
       "  (logsoftmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(trainloader, trans, lr=0.001, mom=0.9):\n",
    "    \n",
    "    loss_fn = nn.NLLLoss()\n",
    "    trans_optimiser = optim.SGD(trans.parameters(), lr=lr, momentum=mom)\n",
    "    \n",
    "    max_epochs = 100\n",
    "    old_loss = np.inf\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        \n",
    "        running_loss = 0.0\n",
    "    \n",
    "        for i in trainloader:\n",
    "            \n",
    "            loss_val = 0\n",
    "            i[0] = i[0].type(torch.LongTensor)   # batch_size x Ts\n",
    "            i[1] = i[1].type(torch.LongTensor)   # batch_size x Tt\n",
    "            source_batch = i[0].to(device)\n",
    "            target_batch = i[1].to(device)\n",
    "\n",
    "            output = trans(source_batch, target_batch)  # Tt x batch_size x target_vocab_size\n",
    "\n",
    "            # calculating loss for the batch\n",
    "            for j in range(output.shape[1]):\n",
    "                logprobs = output[:, j, :].squeeze(1)\n",
    "                target_classes = target_batch[j].squeeze(0)\n",
    "                # inputs to NLL Loss should be of sizes Tt x n_classes and Tt\n",
    "                loss_val += loss_fn(logprobs, target_classes)\n",
    "\n",
    "            loss_val.backward()\n",
    "\n",
    "            nn.utils.clip_grad_norm_(trans.parameters(), 0.5)\n",
    "            trans_optimiser.step()\n",
    "            \n",
    "            running_loss += loss_val.item()\n",
    "        \n",
    "        print('Epoch', epoch+1, ': Loss =', running_loss)\n",
    "        \n",
    "        if abs(running_loss-old_loss)/running_loss < 1e-3:\n",
    "            print('Converged')\n",
    "            break\n",
    "            \n",
    "        old_loss = running_loss\n",
    "    \n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, testloader = train_test_loader(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-8e3b99c66b7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrans_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-63-cfa52643b637>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(trainloader, trans, lr, mom)\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[0mloss_val\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogprobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0mloss_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vasistha singhal\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vasistha singhal\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(trainloader, trans_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
